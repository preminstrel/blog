<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog de Preminstrel</title>
    <link>https://preminstrel.github.io/blog/</link>
    <description>Recent content on Blog de Preminstrel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>preminstrel@gmail.com (Hanshi Sun)</managingEditor>
    <webMaster>preminstrel@gmail.com (Hanshi Sun)</webMaster>
    <lastBuildDate>Fri, 23 Jun 2023 14:03:23 +0800</lastBuildDate><atom:link href="https://preminstrel.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SEU Review | 东南大学回忆录</title>
      <link>https://preminstrel.github.io/blog/post/2023/06/23/seu/</link>
      <pubDate>Fri, 23 Jun 2023 14:03:23 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2023/06/23/seu/</guid>
      
      <description>&lt;blockquote&gt;
&lt;p&gt;Do the right thing, whatever it takes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;六月二十一号，拿到双证，注销学生证，办完离校手续，驶出九龙湖校区的东门，看着手里的学生卡变成了校友卡，&lt;strong&gt;这时候才意识到，我已经回不去了&lt;/strong&gt;。尽管在学校的时候经常也骂SEU的一些迷惑操作，一直盼着毕业逃离这里，&lt;strong&gt;但是真正到走的时候真的好不舍&lt;/strong&gt;。算下来大学四年，我在南京九龙湖校区就待了一年半，无锡国际新校区一年，加拿大三个多月，深圳八个月左右。&lt;/p&gt;
&lt;p&gt;我感觉SEU是一个典型的工科学校，评奖评优时对于GPA等学术成绩相对崇拜，文娱生活相对较少，人文关怀做的也不够到位，整体生活并不够多元化（也有可能是我没有去主动接触）；感觉国内的非工科高校在校园生活这一方面做的会好很多。多些烟火气，多一些有意思的活动，才是大学生活，而不是鼓励成天在图书馆卷绩点（这一点可能是因为我大学大部分时间被COVID-19所覆盖，所以活动也不好举办而产生这样的bias）。SEU对于很多OOD(Out-of-Distribution)的举动的态度都是，不会积极support你，但是也不会去恶意地限制你（一些陆本有这种操作）。&lt;/p&gt;
&lt;h2 id=&#34;高中的延续&#34;&gt;高中的延续&lt;/h2&gt;
&lt;p&gt;大学四年，感觉思想转变的也是特别快的。大一入学前，我还是想保研国内的高校；大一上学期萌生出国的念头，由于特别喜欢日本动漫，暑假又去了日本旅游，所以刚开始想去日本的东京大学(&lt;del&gt;也叫东大，而且日本半导体还算可以&lt;/del&gt;)，甚至还买了日本研究院入试的过去问来看，还考了日语四级。那个时候真的还挺想做电路的，新生研讨课论文还信誓旦旦地说“要为中国芯贡献自己的一份力”，之后才发现自己不太适合做硬件，开始转码之路。&lt;/p&gt;
&lt;p&gt;教学方面，我觉得大一大二的数学学院的老师特别负责，教的也超级好，学工科数分/线性代数/概率论/复变函数的日子真的很开心。&lt;strong&gt;尤其是我的数分老师马红铝，真的可以感受到她对于教学的热爱&lt;/strong&gt;。就算是疫情线上上课，也会很用心地用latex beamer做slides，课也都会录屏方便我们回看。相比来说，大三大四专业课的老师对于教学的态度是更加参差不齐的，是有非常认真负责的老师（比如通信原理老师姜明），但是很多的老师是非常划水的，甚至大部分课程都让研究生代劳，自己当甩手掌柜的。&lt;/p&gt;
&lt;p&gt;关于SEU教学用的教材，质量方差极大，大部分都挺垃圾的。比如C++，线性代数（很薄，一点用都没有），模拟电路（4系的模电好像是外国的教材质量很好，我们是国内的一本上年代的书）等。SEU的课我感觉基本就是在对课本知识有一定了解后对往年题进行疯狂拟合，就可以得到一个很好的分数，GPA只能代表做题能力强弱，意义不是很大。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/TG9gvQIW3Cs4ilF.jpg&#34; alt=&#34;IMG_2201.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;SEU的大一大二体育课无疑是折磨的，因为有着跑操制度。每天需要六点多起来，在七点二十前进行打卡。大一甚至还要求集体方阵跑操和晚上一起上晚自习，高数和物理都有月考和期中考试，感觉&lt;strong&gt;有点像高四&lt;/strong&gt;。SEU有暑期学校的制度，暑期学校的课只能说是一点用都没有，不如多放点暑假，给大家自由的时间去支配去做自己热爱的事情（&lt;del&gt;哪怕就是玩也比这个有意义&lt;/del&gt;）。&lt;/p&gt;
&lt;p&gt;本科时间内我没有参加什么社团，在百团大战的时候本来想要加入漫研社，但是去看得时候才发现自己本质上就是个喜欢看动漫的而已。最后加入了学院学生会，主要就是组织一些交流会活动，做一做微信推送什么的，干了一年之后感觉归属感不是特别强烈，也不是自己真正想要干的事情，就退出了。&lt;/p&gt;
&lt;h2 id=&#34;covid-19的开始&#34;&gt;COVID-19的开始&lt;/h2&gt;
&lt;p&gt;大一的寒假，迎来了COVID-19，下学期基本上都是在家度过的。那段时间开始就超级摸鱼，天天玩Switch，Xenoblade2玩了两三周目，打通了无数游戏，动森玩了250+小时……最后幸亏是暑假之后开始才期末，不然GPA就死掉了。大一下有一门课叫电路基础，我暑假自学的时候其实也学得挺认真的，但是这门课还是我本科前三年专业课中分数最低的。也许，从这个时候开始，我对硬件这部分的兴趣就在逐渐衰弱了吧。大一暑假还和高中同学一起去阜宁进行了社会实践，实际上一直在旅游。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/VdK8Ts6CcQS9Ng7.jpg&#34; alt=&#34;IMG_3161.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;SEU的选课的自由度也是非常小的，除了不计算绩点的任选课（SEU的任选课还是挺有意思的，我挺后悔的一点是没有选一些更有意思的任选课，而是有些功利地询问哪些任选课不会点名亦或是给分高），大部分专业课都是规定好的，能跨系选修的课少之又少。有一些专业课是限选课，比如六选四，但是实际上这些课有一些是有明显继承关系的。可以说，选课层面上毫无自由可言。顺带提一句，SEU的选课系统的UI做的还是可以的，虽然选课高峰期时会经常崩溃。我记得大一的时候经常是要抢着选好老师的，比如高数有十几个老师可以让我们选，我们需要先和学长学姐了解好这些老师的情况再进行抢课。&lt;/p&gt;
&lt;p&gt;大一结束有一个转专业考试，现在看来也还挺可惜的，没有去参加转到CS。当时比较矛盾，因为CS实际上是我高考的第一志愿，最后被调剂到了电子信息。但是在电子信息学的成绩也不错，如果转到CS意味着放弃一些高GPA的课，还要补着修一些CS的课，去熟悉新的环境。现在看起来其实很挺幼稚的，真的不要太关注沉没成本，永远要做对的事情，否则会失去更多的东西。&lt;/p&gt;
&lt;h2 id=&#34;九龙湖的最后时光&#34;&gt;九龙湖的最后时光&lt;/h2&gt;
&lt;p&gt;我大二的时候想要选一门数据结构与算法，但是6系当时没有开这门课。所以我还特地去了9系去选了这门课，苦苦齐集了两院教务和任课老师的签字最后成功选上了，还在大四的时候替换了一门自己不想选的课程。上数据结构的日子真的很开心，整个大学的培养计划课程只有大一的C++和这个是可以写代码的课，其他几乎都是物理课和实验课。因为我是6系电子，所以自然逃不了模电和数电实验的折磨，记得那阵子为了验收整天捧着面包板来回宿舍和实验室，回来还要针对实验写很多页的实验报告去卷到一个比较高的实验分数。我特别记得物理实验的实验报告是要求手写的，而且必须还要额外提交一份小论文才可以有资格评优。&lt;del&gt;大物实验因为我的疏忽MOOC没有按时做所以最后是89，老师甚至帮我改成了90&lt;/del&gt;。相比这些，大三的微机实验就水了很多，几乎都可以在课内完成。&lt;/p&gt;
&lt;p&gt;值得一提的是，SEU的政治课程我感觉算比较友善的，思修、近代史、军理都是开卷考试的，我的近代史老师甚至还会提前泄题一丢丢。给分倒真的很看老师的心情，比如我的毛概（或者是马原？记不清了）老师给分就特别低，而且很会卡人。一般这些课我们都会在课上玩手机或者刷专业课的题目，但是他就完全不允许。而且听说后面要加大力度，增加二十四史之类的政治课，学分比重变得更大了，就会更加浪费时间了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/TSLavqzrA8i1jxJ.jpg&#34; alt=&#34;IMG_5051.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;九龙湖的大草坪在疫情的时候修的差不多了，有机会和舍友一起在上面放了一次风筝，特别有意思（长这么大风筝第一次飞那么远）。我离开九龙湖校区的时候，游泳馆还没修好，今年再去的时候，已经是红色钢肠的样子了。在SEU前两年也打过一些竞赛，但是结果都不是很好，因为我自己太摆烂了，还有就是竞赛的回报比较低，意义也不是很大，所以就并没有去用心做。&lt;/p&gt;
&lt;h2 id=&#34;抵达无锡校区&#34;&gt;抵达、无锡校区&lt;/h2&gt;
&lt;p&gt;大三的专业课我上的非常折磨，这个时候&lt;strong&gt;我对硬件知识的厌恶开始进一步加剧&lt;/strong&gt;。由于大一大二的课程专业课占比不是特别多，而且数学性多一些，所以还可以handle，但是到了大三这种分裂感越来越大，同时由于没有什么一起转码的伙伴所以学习上有一种孤独感。大三很多课都是物理课，包括电磁场，现代光学，半导体物理等。很多次我都是上课摆烂玩手机，然后为了GPA在考前几周进行自学，把崭新的课本从第一页学起来，考试周疯狂拟合一下做题套路获得一个相对满意的分数，纯粹属于是&lt;strong&gt;冲激函数&lt;/strong&gt;式学习法。虽然我可以获得高的GPA，但是&lt;strong&gt;我真的感觉在浪费自己的时间，做自己不喜欢的事情（即使我可以做好）是很痛苦的一件事&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/1ei2HTuV5D7Ns4j.jpg&#34; alt=&#34;IMG_5951.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;无锡校区的食堂很不好吃，基本就只有一个大伙，因此我甚至还自己做了一段时间的饭，宿舍里小火慢炖猪蹄汤什么的。但是无锡校区的宿舍条件还是很不错的，是二人间，还有马桶、浴霸什么的。也并不是上床下桌，而是桌子和床都在地上。无锡那段时间，我在research上跟了AI学院的一个老师remote做了一段时间，最后毕设也跟他做的，还是很开心的一段经历，老师人也很不错。&lt;/p&gt;
&lt;p&gt;大三的时候我报名了去加拿大公派暑研Mitacs，这无疑是一段非常宝贵的经历，对我的影响也非常大，详细见&lt;a href=&#34;https://preminstrel.com/blog/post/2023/03/21/ms_app/&#34;&gt;我申请MS的blog&lt;/a&gt;和&lt;a href=&#34;https://preminstrel.com/blog/post/2022/04/15/mitacs/&#34;&gt;申请Mitacs的blog&lt;/a&gt;。不仅让我遇到了很多志同道合的朋友，更让我体验到了国外的科研氛围和生活感受（&lt;del&gt;主要是生活感受，暑研的时候我一直在摸鱼&lt;/del&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/xsUrQRKy6F3PS8q.jpg&#34; alt=&#34;IMG_0958.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;这里值得一提的是，无锡的教务和新辅导员人都非常不错，很多事情都可以通情达理地帮学生解决，属于是东南行政为数不多让我感觉到开心的地方了；无论是我大三暑假的research intern、大四的Apple intern还是最后毕业论文的线上答辩，他们都爽快地批了，这在陆本都是难得的。&lt;/p&gt;
&lt;h2 id=&#34;脱离学校的大四&#34;&gt;脱离学校的大四&lt;/h2&gt;
&lt;p&gt;2022年暑假十月回国后在上海隔离酒店住了一周，随后回家摆烂休息。很快收到Apple intern的offer，本来想下学期去英国爱丁堡交流玩耍的，这下子变成果子的打工人了。十一月就去了深圳火速入职，然后打工一直到六月底才结束。Apple的culture挺好的，是我很喜欢的公司，虽然时薪不算高，但是有很多隐形的补贴和各种福利。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/cHCgmM1rnFVQuWv.jpg&#34; alt=&#34;IMG_3048.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;Apple的intern对我的影响也是挺大的，让我看了一眼工业界的样子。我在Apple主要做的software，刚进去的时候实际上对于web前后端都不是很熟悉，只会写一下plain HTML，数据库什么基本都是现学的。实际上，Apple还对我的一些Workflow、时间管理也有了很大的提升。和同事们也交流了很多关于工业界的事情，以及US的Apple的情况，也建立了一些connection。Mentor和manager人也特别nice，同事人也都很好。在Apple工作也是十分wlb的，很多情况下，我都是早上十点起床，过去吃个午饭，晚上吃完晚饭回来。&lt;del&gt;更多的不太能说，有违Apple的BC&lt;/del&gt;。&lt;/p&gt;
&lt;p&gt;在2022年11-12月的时候超级忙，首先是Apple的一些bring up，还要兼顾Master的申请，SEU的毕业设计等事情。大四的成绩因为我的彻底摆烂（很多课都skip/remote了），直接从3.98下滑到了3.96。整个大四都不在学校，但是认识了很多有意思的人，甚至比前三年加起来都要多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/m4iL21ScYxEq3Ih.jpg&#34; alt=&#34;1d063a8cb7521b352c30deb2ca520243.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;春节后和高中同学一起去云南旅游，超级开心（&lt;del&gt;vlog拖到现在都没剪，我是懒狗&lt;/del&gt;）。五月份六月份还分别去了一趟HK和武汉旅游，毕业后的七月已经schedule了三个旅游计划了。现在的感受是，真的要注重生活，活在当下。不可以总是妥协，下载的游戏不可以说忙完这阵子以后再玩，想去的地方就应该马上就去；我还记得五月份Zelda王泪发售的那天晚上直接玩到早上五点那种激动感（&lt;del&gt;打完之后睡一会去上班&lt;/del&gt;），&lt;strong&gt;我们并不在水里生活，不需要总是说等上岸再休息&lt;/strong&gt;。实际上，等到有了工作，有了家庭，只会更忙更累。虽然Apple相对wlb，但是我还是感觉每天工作就是在坐牢，工作和自由的学校生活的感觉是完全不一样的。&lt;/p&gt;
&lt;h2 id=&#34;盛大的毕业&#34;&gt;盛大的毕业&lt;/h2&gt;
&lt;p&gt;六月中下旬特地飞回SEU参加毕业典礼，今年的毕业典礼是疫情后第一次的毕业典礼，也在是SEU修好的大草坪上的第一次毕业典礼，我感觉做的超级好，&lt;del&gt;为这个毕业典礼我可以当一天SEU吹&lt;/del&gt;。确实只有在真的离开了SEU之后，才感受到SEU的好和对SEU的不舍。&lt;/p&gt;
&lt;p&gt;时常我会幻想，如果我填志愿的时候没有把SEU放在第一志愿，也许我会去北航然后10043无缘SEU，也可能去华科学CS。虽然本文指出了SEU很多不好的地方，但是我可以说，&lt;strong&gt;“在SEU的四年我很开心，这句话绝对不是谎言”&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/07/02/2lWdm5tGJVUrijE.jpg&#34; alt=&#34;IMG_6145.jpeg&#34;&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>HK</title>
      <link>https://preminstrel.github.io/blog/post/2023/05/26/hk/</link>
      <pubDate>Fri, 26 May 2023 19:46:47 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2023/05/26/hk/</guid>
      
      <description>&lt;p&gt;在深圳实习的时候，早就办好了港澳通行证，终于趁着五一节从深圳来到了香港找DR玩。&lt;/p&gt;
&lt;h2 id=&#34;checklist&#34;&gt;Checklist&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 有效的港澳通行证&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; iPhone Wallet 添加 Octopus&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 支付宝购买境外流量包，到HK会自动激活，需要打开数据漫游&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;交通&#34;&gt;交通&lt;/h2&gt;
&lt;p&gt;深圳北站-香港西九龙站高铁不到半小时，然后用八达通坐地铁/巴士即可；另外的路线是从深圳湾走，或者从福田口岸走，转巴士/地铁。但是还是高铁方便点，西九龙的位置也不错。关于八达通，iPhone的Wallet里面是可以直接添加的，要下载一个八达通游客版App，然后就可以直接用手机NFC了（开卡押金要50港币，好贵）。西九龙站高铁连着Austin Station（中文名是柯士甸，&lt;del&gt;HK很多地方的翻译都有点奇怪，比如把Kennedy翻译成坚迪&lt;/del&gt;），可以直接换乘。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/05/26/zPtuNYlZmgrjpVq.jpg&#34; alt=&#34;01BD6CE3-E1E6-435A-8250-D7E99BE16CF9_1_105_c.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;关于过关：到了西九龙火车站后开始过关，手续很简单，基本就是放卡、刷脸、扫描指纹这样的，虽然人很多，但是速度也很快。过关结束后会给一张白色的小票，上面记录着来港时间和在留时间限制。&lt;strong&gt;需要好好保存好，可能会随机抽查，第二天我在地铁站被拦下来抽查了这个小票。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/05/26/K8PxDbUFXjJgYfN.jpg&#34; alt=&#34;D825FCF0-C81C-41CD-8E25-DB8F7BFFDB3E_1_105_c.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;关于地铁和巴士：无论地铁还是巴士，价格都挺贵的。有的巴士还是根据你上车的站数离终点站的站数进行计费的，我记得我一次坐公交直接刷了$15吓到我了。地铁要确认好线和站名，还有听朋友说在HK最好用Google/高德地图不要用百度地图，会有一些地方不准确。&lt;/p&gt;
&lt;h2 id=&#34;消费&#34;&gt;消费&lt;/h2&gt;
&lt;p&gt;在香港除了一些店是cash only（进店前一定要看清楚了）之外其他基本都可以用支付宝/八达通支付，我还带了信用卡结果一点都没用得上。HK感觉消费挺贵的，吃了一小碗朴实无华的馄饨面都很贵的价格，大概物价是大陆的4~5倍。但是港大的食堂我感觉价格还可以，因为有一些补贴，菠萝油挺好吃的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/05/26/TCmw6jq3rZftyhG.jpg&#34; alt=&#34;179186D6-C79B-43C5-8974-31F41A870CF6_1_105_c.jpeg&#34;&gt;&lt;/p&gt;
&lt;p&gt;香港的住宿超级贵，我去之前看了眼宾馆，单人间基本都四位数一晚上，好在朋友收留我住他港大宿舍，不然我就要当天往返了。&lt;/p&gt;
&lt;h2 id=&#34;景点&#34;&gt;景点&lt;/h2&gt;
&lt;p&gt;先去了港大找同学玩，港大不大，但是红墙还挺好看的。叮叮车、天星小轮、星光大道等也都迅速打卡了一下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/05/26/ANrtW4OhQkTxi6P.jpg&#34; alt=&#34;EDBE3DD6-0F34-4733-953B-4331D1B8EA35_1_105_c.jpeg&#34;&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>US Visa</title>
      <link>https://preminstrel.github.io/blog/post/2023/04/18/us_visa/</link>
      <pubDate>Tue, 18 Apr 2023 17:15:47 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2023/04/18/us_visa/</guid>
      
      <description>&lt;p&gt;敏感专业ECE，在广州21号窗口面签。我刚开始已经怀着肯定被check的心态去面了，结果很神奇地直接水过了。护照不是白本，有日本旅游签证和加拿大的十年工签。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/04/18/uyOWN4PzsKIZYQf.jpg&#34; alt=&#34;736BBE85-3FA0-426C-97A8-F8CC095AF2E1_1_102_o.jpeg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;checklist&#34;&gt;Checklist&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Passport&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Appointment Letter with time&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; DS-160&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; SEVIS I-901 Payment Confirmation&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; I-20 (signed)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; resume (rewrite with new template)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study Plan&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Advisor resume (如果是course-based，可以自己选一个research interest相近的)&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Photo 51x51&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Admission Letter&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Enrollment Letter&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Transcript&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Publication&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Certificate of Deposit&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 户口本&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Course List, academic requirement&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; TOEFL&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ID&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Some travel photos&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 父母收入证明&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;online-application&#34;&gt;Online Application&lt;/h2&gt;
&lt;p&gt;首先是要早点和学校申请I-20，CMU ECE发的还挺快的，一周之内就下来的，但是有的department或者学校会发的比较慢，可以发邮件催一下。一般来说如果check了，大概6周左右。申请US Visa需要用到以下几个网站：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DS-160在线填写(&lt;a href=&#34;https://ceac.state.gov/GenNIV/default.aspx&#34;&gt;https://ceac.state.gov/GenNIV/default.aspx&lt;/a&gt;)，这个填写很难受，每填一会就会timeout，所以要及时保存&lt;/li&gt;
&lt;li&gt;约签网站(&lt;a href=&#34;https://atlas.my.salesforce-sites.com&#34;&gt;https://atlas.my.salesforce-sites.com&lt;/a&gt;)， 用来预约面签&lt;/li&gt;
&lt;li&gt;350美金 SEVIS(&lt;a href=&#34;https://www.fmjfee.com/i901fee/index.html&#34;&gt;https://www.fmjfee.com/i901fee/index.html&lt;/a&gt;)，SEVIS ID和school code要按照I-20填写&lt;/li&gt;
&lt;li&gt;签证状态查询(&lt;a href=&#34;https://ceac.state.gov/CEACStatTracker/Status.aspx&#34;&gt;https://ceac.state.gov/CEACStatTracker/Status.aspx&lt;/a&gt;)，用来track面签后签证的进度&lt;/li&gt;
&lt;li&gt;护照状态查询(&lt;a href=&#34;https://www.ustraveldocs.com/cn_zh/cn-niv-passporttrack.asp&#34;&gt;https://www.ustraveldocs.com/cn_zh/cn-niv-passporttrack.asp&lt;/a&gt;)，用于track签证issue之后的护照位置状态&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preparation&#34;&gt;Preparation&lt;/h2&gt;
&lt;p&gt;在准备checklist里面的材料时，有这几个材料是比较需要时间准备的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;resume，一级重要，STEM基本必看。我是用的官网的模板，相当于把申请季的resume重写了一遍，内容压缩在了一页。要把一些machine learning什么的敏感词都去掉，改成signal processing/data analysis/optimization/digital image processing；和申请季策略相反，要把经历写的水水的，一次RA经历可以就写一个bullet point。Publication最好不要隐瞒，我的publication里面有AI字眼也硬着头皮交了，但是VO好像没太注意&lt;/li&gt;
&lt;li&gt;study plan，官网也有模板，但是官网的模板有点偏research向了。我的plan study分了这几个section：Summary，Personal Background，Program Information，Potential Advisor，Course Selection Plan，Funding和Plans After Graduation；注意毕业后的打算务必是return back to China directly&lt;/li&gt;
&lt;li&gt;照片也要带着，我前面一个人因为DS-160的照片有点问题，所以要了实体照片去扫描&lt;/li&gt;
&lt;li&gt;I-20要记得签字&lt;/li&gt;
&lt;li&gt;实际上从track签证的网站上可以看到，我的case是在面签的前一天就被创建了，所以面签前他们已经对你的情况和DS-160的填写情况有了一定的了解&lt;/li&gt;
&lt;li&gt;没问到的不要主要答，没要的不要主动递，不表露移民倾向&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;准备面签常见的问题，练习口述，我参考了这个&lt;a href=&#34;https://www.1point3acres.com/bbs/thread-765076-1-1.html&#34;&gt;page&lt;/a&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;个人背景&lt;/li&gt;
&lt;li&gt;Why this program/Why US/Why major&lt;/li&gt;
&lt;li&gt;research interest (be specific)，包括具体例子和实际应用&lt;/li&gt;
&lt;li&gt;Funding/父母工作&lt;/li&gt;
&lt;li&gt;毕业打算&lt;/li&gt;
&lt;li&gt;之前去美国的经历（如果之前有签证），包括旅游、交换、暑研等&lt;/li&gt;
&lt;li&gt;准备上什么课/之前发表的论文的介绍&lt;/li&gt;
&lt;li&gt;某段实习经历或论文涉及类似Machine Learning的敏感词，也可以提前想好如何解释，为自己洗清嫌疑&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;before-interview&#34;&gt;Before Interview&lt;/h2&gt;
&lt;p&gt;我约的是9:15，但是实际上我是八点多进去开始排队的，提前一些比较好。值得一提的是，由于电子产品和水什么的都不可以带进去，所以要找个地方存一下。门口有一些报亭什么的可以存，但是我觉得不太安全，于是在附近的大厦旁边找了一个丰巢寄存，取件只需要手机号+取件码，手机存进去完全没有任何的问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;进去之后首先是给护照贴条形码标签，这一步需要Appointment Letter，证明你有预约。&lt;/li&gt;
&lt;li&gt;排队进入一个窗口，给看了I-20和passport，工作人员扫了一下条形码，确认照片。&lt;/li&gt;
&lt;li&gt;排队录指纹&lt;/li&gt;
&lt;li&gt;排很长很长的队进行面签，面签的窗口无法自己选择的，由工作人员进行assign，哪里空了叫补上&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;interview&#34;&gt;Interview&lt;/h2&gt;
&lt;p&gt;我被assign到了21号窗口排队，VO是一个白男。我前面有两个人，第一个人拿了白单子走了(被拒签)，第二个人带孩子一起探望在美国的老人，轻松过签，然后就到了我。&lt;/p&gt;
&lt;p&gt;我：Good morning. (递passport和I-20)&lt;/p&gt;
&lt;p&gt;VO(看了一会)：What kind of work do your parents do?&lt;/p&gt;
&lt;p&gt;我：My father is a &amp;hellip;.&lt;/p&gt;
&lt;p&gt;VO：What do you specialize in Electrical Engineering?&lt;/p&gt;
&lt;p&gt;我：Generally speaking, I &amp;hellip;.&lt;/p&gt;
&lt;p&gt;VO(直接打断我)：No, be specific.&lt;/p&gt;
&lt;p&gt;我：I mainly focus on signal processing and data analysis.&lt;/p&gt;
&lt;p&gt;VO：I want your resume.&lt;/p&gt;
&lt;p&gt;我：Okay. (递resume)&lt;/p&gt;
&lt;p&gt;VO看resume的方式很特别，边看边用手指丈量resume上的每个section。然后不说话一直在打字，打了大概一分钟。&lt;/p&gt;
&lt;p&gt;VO(拿起旁边的红单子和黄单子交给我)：Your visa is approved.&lt;/p&gt;
&lt;p&gt;我：Thank you!&lt;/p&gt;
&lt;p&gt;VO：Bye-bye.&lt;/p&gt;
&lt;h2 id=&#34;after-interview&#34;&gt;After Interview&lt;/h2&gt;
&lt;p&gt;面签后上网查看状态，已经是Approved了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/04/18/xwHu5QfpAN9Prsq.png&#34; alt=&#34;Screenshot 2023-04-18 at 5.11.39 PM.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;第二天看得时候，从Approved变成了Issued。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/04/19/ZLBRxT3MqY8QhJl.png&#34; alt=&#34;Screenshot 2023-04-19 at 8.14.44 PM.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;第三天(Apr 20)在护照跟踪平台上查到&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Passport Status:&lt;/strong&gt; 护照已从领事馆那边收回，目前正在安排运送&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当天下午收到EMS的提醒，已经从广州发出。在 Apr 22收到护照，有效期五年，Expiration date一直到 Apr 2028！可以五年随时回国了哈哈哈，所以应该是只要不check基本上都是五年F1签证了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2023/04/22/MozxYDjVghn64OG.jpg&#34; alt=&#34;9BAEB2F4-C139-494F-8632-5605457FC58C.jpeg&#34;&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>M.S.申请回忆录</title>
      <link>https://preminstrel.github.io/blog/post/2023/03/21/ms_app/</link>
      <pubDate>Tue, 21 Mar 2023 19:46:54 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2023/03/21/ms_app/</guid>
      
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“ My Heart is in the Work.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Andrew Carnegie, Founder, November 15, 1900&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;之前听过一句话，&lt;strong&gt;大学的生活珍贵到无论如何度过，都觉得是虚度光阴&lt;/strong&gt;。2022可以说是我大学中的一个transition了，这一年发生了太多太多，从大三结束，到加拿大暑研，到Apple实习，到23Fall的申请季，再到疫情的放开。考完了最后的试，见了南长街，参了南禅寺，最后一顿二楼聚餐，120周年校庆的纪念版校园卡，最近还申请了SEU的校友卡……在SEU的生活，差不多落下了一个句号。考完的那一天，看着夕阳，我失去了辨别天与海的能力。我知道，在这个校园内的生活屈指可数了。&lt;/p&gt;
&lt;p&gt;我本科是EE的，但是research基本做的都是ML/CV的东西，所以申请ECE方向带DS/ML的track，有一些转专业的意思，所以放弃了一些硬件EE项目。关于MS申请，前期也做了一些工作，和同学也讨论了很多次。&lt;a href=&#34;https://trinkle23897.github.io/posts/application&#34;&gt;n+e的这篇blog&lt;/a&gt; 给了我很多有用的信息，我准备这次写总结也差不多按照这位大佬的格式来。我申请所用到的材料大概包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;成绩单和在读证明 (中英文都要，&lt;del&gt;UMich得用scholaro计算GPA，我直接掉了0.14&lt;/del&gt;)，排名证明 (Optional，这个找教务开，但是学校的档案馆不给翻译，就在公证的地方找了一个英文翻译服务)，高中毕业证公证 (ETHz要求)&lt;/li&gt;
&lt;li&gt;TOEFL&amp;amp;GRE&lt;/li&gt;
&lt;li&gt;文书 (CV, SoP, PS/PHS, Diversity Statement)&lt;/li&gt;
&lt;li&gt;三封推荐信&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gpa&#34;&gt;GPA&lt;/h2&gt;
&lt;p&gt;首先是出身，本科是东南大学，感觉US对于985什么的都不太认识（ETHz友好），除了头部的那几个，但是对于上海科技大学、南方科技大学等学校都很认可。&lt;/p&gt;
&lt;p&gt;我的GPA还是不错的，得益于SEU比较好的GPA评分机制，申请的时候3.98/4.0，均分94.04。我感觉SEU的GPA不是很难拿，专业课基本上把往年题刷一遍，课后题弄清楚了都可以90分左右。但是需要注意的是&lt;strong&gt;GPA的边际效益很不好，对于申请来说可能3.98和3.9，甚至和3.85的区别不是特别巨大的，然而，从3.9提升到3.98所花费的时间成本却远远高于所得到的好处，能做的事情还很多，多做一些自己爱做的&lt;/strong&gt;。有条件最好还是多分一些时间给软背景，我申请就是吃了这个亏，科研很弱，软背景不够强，Top项目很难申请。&lt;/p&gt;
&lt;p&gt;有条件的建议大三去交换，既可以轻松拿高绩点，而且可以白嫖学校的出国补助 (up to 120k)，更是可以拿到推荐信，建立海外connection，再不济也可以公费去旅游一趟，当度假体验一下也是很舒服的。&lt;/p&gt;
&lt;p&gt;PS：SEU的同学可以看看这个repo：&lt;a href=&#34;https://github.com/preminstrel/awesome-seu&#34;&gt;preminstrel/awesome-seu&lt;/a&gt;，欢迎提交PR和建议&lt;/p&gt;
&lt;h2 id=&#34;toeflgre&#34;&gt;TOEFL&amp;amp;GRE&lt;/h2&gt;
&lt;p&gt;对于TG，&lt;strong&gt;一定要早考，杜绝拖延症&lt;/strong&gt;。我就是因为之前一直没考上，拖到大三暑假在加拿大暑研的时候才考出TOEFL成绩，而且GRE也没时间考了，所幸今年的MS项目大多还是optional/not required，这个一定要提前check好各个项目的要求，同时注意best score用处不是很大。
TOEFL感觉最重要最难的是听力，最容易提升的是写作。还有一点是，多约几次多考几次，有的时候真的只是运气问题，我托福最后拿了115分 (30 | 30 | 27 | 28)，我感觉纯粹是运气使然，口语有的题目都没来得及说完。在国外考托福有一个福利就是很便宜，价格大概是国内的一半。
但是TOEFL其实不用过多看重，有时候中介和一些机构就是喜欢让你把语言和标化考的高起来 (e.g., 110+/105+)。实际上TOEFL也就是一个filter，过了那个线就无所谓了。对于大部分学校，100够用了；对于个别学校，要105 (UIUC)；对于极其奇葩的项目，要很奇怪的分数，比如CMU一个项目要Speaking 28+。&lt;/p&gt;
&lt;h2 id=&#34;科研rl&#34;&gt;科研&amp;amp;RL&lt;/h2&gt;
&lt;p&gt;这一部分，我的失败，彻彻底底。我&lt;strong&gt;大学最失败的一点就是没有用卷GPA一半的时间出来做research&lt;/strong&gt;。所以我科研做的工作都比较水，对于研究领域也没有很强的insight；建议有Ph.D.理想的少发EI水会，真的用处不是很大，而且有时候会起反作用，让人觉得你水水的。当然，如果目标只是MS的话，其实还可以在简历上装点一下门面的，虽然作用也仅仅是装点门面了&amp;hellip;&lt;/p&gt;
&lt;p&gt;我的科研主要分为三段。第一段是校内的SRTP，一个省级项目，大二暑假写了一篇一作EI投了&lt;del&gt;现在看来我当时写的就是厕纸，做的work也很差&lt;/del&gt;。第二段是校内计算机实验室跟的一个老师，做的计算机视觉在Medical Imaging上的一些应用。虽然没有什么产出，但是感觉学的东西比第一段要多，申请的时候有一篇SCI四作，用处不是很大。第三段是Mitacs暑研去加拿大Alberta做的Unsupervised Learning for Anomaly Detection的项目，申请的时候有一篇二作在投。三篇推荐信也出自这三段科研中，其中加拿大暑研的老师是自己写自己交的推荐信，其他两位都是我自己的写了让学校老师签名的。&lt;/p&gt;
&lt;p&gt;加拿大supervisor人真的很nice，不仅自己写推荐信还帮我交的贼快，几乎我这里系统里刚一发invite；我申请了22个项目她都挨个交了，真的是人特别好特别好，很幸运能遇到这么好一个老师。当然，东大的老师人也非常nice，回复都很及时，遇到大家真的很幸运。学校里两个老师的推荐信我都让中介帮我拟稿了，有一点要注意的就是最好用不同的letter format和不同的口吻。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/BcsLpI7my9E2Ubx&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/21/BcsLpI7my9E2Ubx.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关于Mitacs暑研，我感觉是一次很好的机会，因为它是带funding的，算是SEU平台难得的暑研（公费旅游）机会了。在克服了签证，机票，Covid PCR等一系列困难后，带着快一百斤的行李，坐了快一天一夜的飞机来到了Edmonton。Edmonton的夏天不冷不热，穿一件刚刚好。在UofA的校园里闲逛，一切都仿佛那么新奇。劝退的餐饮费让我自学了煎牛排，意大利面，通心粉，煮水饺，下鸡蛋面……认识了许多志同道合的小伙伴，同时也一定程度上锻炼了自己的口语（还一直是菜狗🐶）。Banff的Lake Louise很美，美的不那么真实，水质很像是电影中渲染的特效。在河边，我获得了全网统一的头像（笑。Banff之后，我便染上了Covid，拖着咳嗽剧痛的嗓子参加了托福首考，最后94分 (Speaking 20)。忘不了麦当劳的Poutine，忘不了Timms的平价咖啡，忘不了Botanic Garden，忘不了Southgate的Apple Store，忘不了Green Golden 的Alberta。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/puUPXQjnFI3qvLb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/21/puUPXQjnFI3qvLb.jpg&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;实习&#34;&gt;实习&lt;/h2&gt;
&lt;p&gt;我在加拿大暑研的时候还顺便投了Apple R&amp;amp;D的intern岗位，具体经历在&lt;a href=&#34;https://zhuanlan.zhihu.com/p/603277820&#34;&gt;知乎上有过一篇文章&lt;/a&gt;，主要是因为感觉大四的时候应该比较清闲，而且简历上也没有professional experience这一section，所以想要找个internship做一下，顺便也想看看工业界和学术界的区别。由于抱着随便投投的心态，所以只投了比较喜欢的Apple（资深果粉XD）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/6UDe7WnAhGR3zi5&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/21/6UDe7WnAhGR3zi5.jpg&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2022.8 在Apple中国官网海投简历，附件为transcript，resume和一些link（LinkedIn，GitHub等）。&lt;/p&gt;
&lt;p&gt;2022.9 收到HR的电话（那时候我彻底把Apple这事情忘了，刚开始我还以为是骚扰电话，挂了很多次没接），HR简单问了一下我的基本情况和时间安排之后说尽快帮我安排面试。过了几天我收到了面试的邀请，用webex参加了面试。&lt;/p&gt;
&lt;p&gt;一面是team的manager，人很友善。首先让我自我介绍，并详细了解了我的那些research经历。面试本应是英文的，但是因为我当时人在加拿大做research intern，所以他说就不用考察我这部分能力了。基本情况了解后，便随便chat了一段时间，没有考察任何的题目和基础知识掌握程度，也没有code review。整个面试时间大概半个小时，聊的很开心。&lt;/p&gt;
&lt;p&gt;2022.10 过了一个月，这个月我又把这事情给忘了，这次接到面试的邀请的时候我刚从加拿大回来，在上海的隔离酒店。这次面试我的是一个未来会一起工作的同事，他首先讲了一下team做的事情和他正在做的project，让我有所了解。随后，他问了我的research experience中写到的一个anomaly detection算法如何应用于XXX，我简单讲了一下，然后挑了一篇论文的可视化heat map给他看。又chat了一会后，他问我愿意不愿意来帮他做这个project，我说挺好的，也挺感兴趣。整个面试时间大概也是半小时左右。&lt;/p&gt;
&lt;p&gt;2022.11 在家摸鱼的时候，接到了HR的电话说我被录了，可以开始走入职流程了，背景调查了两周之后我顺利拿到了offer。在这之后还收到了爱丁堡交流项学期交流项目的offer（因为本来准备去英国玩一阵子，没想着Apple能录），忍痛放弃了去旅游的机会。当月就入职了Apple，开始打工，合同签到了6.30，也就是我毕业的时候。&lt;/p&gt;
&lt;p&gt;有趣的是，在9月的时候，我还面了一个EPM岗（HR瞎给的面试，我一点相关经历没有，面试前十分钟才去查了一下EPM是啥…）。面试是全英文，刚聊五分钟我就想退出会议了，之后痛苦地尬聊了半小时，也没有后续了。&lt;/p&gt;
&lt;p&gt;关于薪资/津贴/房补和其他福利等就不讲了，对于实习来说感觉很满意。最近感觉Apple R&amp;amp;D在国内的坑位变多了，感兴趣的小伙伴可以投一下。感觉bar其实也还好，主要是看对眼。入职检查的时候查出来眼底有问题，眼压有点高，让我更加意识到了健康的重要性，sigh。&lt;/p&gt;
&lt;p&gt;在Apple干了也有几个月了，感觉还可以，同事都很nice，老板还想留我干到八月底。&lt;del&gt;虽然也经常摸鱼哈哈哈。&lt;/del&gt;&lt;/p&gt;
&lt;h2 id=&#34;选校&#34;&gt;选校&lt;/h2&gt;
&lt;p&gt;由于我科研比较弱，Paper都是水文章，所以我只能申请MS，Ph.D.只抽了一个UCSD (因为UCSD的MS要GRE&amp;hellip;)。选校我只分了三个level：Reach，Match和Safety (实际上Match里面还要细分一下，bar还是有很大区别的，有的分的也不是很合理)。主要申请美国，欧洲只申请了双E，因为有Mitacs的Graduate Fellowship所以加拿大随手申了两个。排名的话CS专业的话可以参考&lt;a href=&#34;https://csrankings.org/#/fromyear/2012/toyear/2023/index?all&amp;amp;us&#34;&gt;CS Ranking&lt;/a&gt;，但是要注意的是，faculty越多，分数也会越高，所以这个ranking也是有bias的。公认的四大一般是MIT、Stanford、Berkeley和CMU。&lt;del&gt;其中CMU经常被开除Top4&lt;/del&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;彩票&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford EE，这个今年招了很多人耶，但是女生居多，是男生的好几倍，这种顶级项目某种程度上也很看运气&lt;/li&gt;
&lt;li&gt;Caltech EE，class size很小，某种程度上比Stanford还要难，光是名字说出来就很厉害的感觉&lt;/li&gt;
&lt;li&gt;UCSD ECE PhD，纯纯彩票，并且没有陶瓷，MS要GRE申不了&lt;/li&gt;
&lt;li&gt;UIUC CS MS，因为ECE MS要GRE所以我只能申请CS送钱&lt;/li&gt;
&lt;li&gt;UMich CSE，这个bar对于我们这些本科EE的人来说还是很高的，所以放在了彩票，但是价值不一定能与前面四个媲美&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;主申&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CMU ECE/MSIN，这俩项目都是CMU工程学院的，所以也不用再交一次TOEFL；相比ECE，MSIN更加偏就业导向，甚至没有什么faculty。虽然CMU是CS的四大，但是综合排名比较一般。&lt;del&gt;计算机技校&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;UT Austin ECE，这个项目的class size很小，DICE track基本不收人，我申请的是bioECE，好像也不咋收人，Austin的好处就是更好转PhD，学费便宜，可以RA/TA，基本可以cover很多花销，&lt;del&gt;偷偷说一句，他们家的申请系统做的是真的烂啊，和UW的申请系统有的一比，填申请非常折磨&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;GaTech ECE，因为在黑人区所以治安有一些concern，招的人也不是特别多，价格相对便宜，三大理工学院，性价比很高&lt;/li&gt;
&lt;li&gt;ETHz EEIT/EPFL EE，瑞士双E，国际排名很牛，今年ETHz要GRE，我没交。我们学校在ETHz是每年有一些quota的，电子信息专业的均分大于91的感觉都有希望。花费少，签证好过，风景好，排名牛，但是缺点就是读PhD要卷硕士GPA，research时间会比较少，而且和US的connection的不是那么多，如果想读US PhD有点曲线救国的意思，但是ETHz好像thesis可以选择去US学校交流访问半年&lt;/li&gt;
&lt;li&gt;UCLA ECE，地域较好，处于加州，学费也便宜，但是他家ECE偏Hardware一些，我申请的signal方向，并不太喜欢这个track（但是其他track我更不match），还是很喜欢加州的天气的&lt;/li&gt;
&lt;li&gt;Duke ECE，我拿了offer之后才知道Duke ECE很贵，Duke的综排还可以US Top10，专排不行，这个项目也可以选成全软课&lt;/li&gt;
&lt;li&gt;NWU CS MS，今年NWU CS陆本拒麻了，这个学校和Duke一样，综排还可以，专排不行&lt;/li&gt;
&lt;li&gt;UofT CS MScAc，一个就业导向项目，8 months coursework+8 months applied research internship，多大的title在加拿大也挺好用的。这个项目interview是硬性要求，shortlist之后会慢慢发面试。面试是和项目的Administration聊大概45min，我是三月中旬收到interview invitation的，晚上十点聊的，感觉Claire人还是很好的，开始的时候就说会尽快结束，我说渴了可以随时喝点东西哈哈哈，今年发interview发的很多，往年是如果发了就基本上是admit了&lt;/li&gt;
&lt;li&gt;UBC ECE MSc，科研项目，没套瓷的话基本属于送钱，我套瓷套晚了，没收到答复&lt;/li&gt;
&lt;li&gt;Columbia CS MS，申请单纯是为了CS title，NYC+Columbia太贵了，&lt;del&gt;哥大EE的class size太大了，俗称点击就送，今年看见有人EE没交申请费没完成申请都被发了offer&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;Berkeley EECS MEng，Berkeley的EECS很强，但是这个项目是MEng，项目的优势是title和加州的地理位置吧&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;保底&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cornell ECE MEng，这个项目其实难度不应该放在保底，应该是主申-保底的过渡，在我这里排名比较低是因为我有一些MEng的bias；康奈尔国内名气很不错&lt;/li&gt;
&lt;li&gt;USC CS 37，加州私立，不看推荐信，不看文书，只看三维，保底专用&lt;/li&gt;
&lt;li&gt;UPenn ESE，UPenn的bar也很低&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program&lt;/th&gt;
&lt;th&gt;Level&lt;/th&gt;
&lt;th&gt;Decision&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Stanford EE MS&lt;/td&gt;
&lt;td&gt;Reach&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Feb 22, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Caltech EE MS&lt;/td&gt;
&lt;td&gt;Reach&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Mar 27, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UCSD ECE PhD&lt;/td&gt;
&lt;td&gt;Reach&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;May 9, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UIUC CS MS&lt;/td&gt;
&lt;td&gt;Reach&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Mar 16, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UMich CSE MS&lt;/td&gt;
&lt;td&gt;Reach&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Apr 11, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CMU ECE MS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Match&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Admitted (Accepted)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Feb 17, 2023&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CMU MSIN MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Waitlist =&amp;gt; Rejected&lt;/td&gt;
&lt;td&gt;Mar 10, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UT Austin ECE MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Apr 25, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GaTech ECE MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Mar 27, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ETHz EEIT MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Apr 4, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EPFL EE MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Mar 16, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UCLA ECE MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Apr 18, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Duke ECE MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Feb 7, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NWU CS MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Feb 10, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UofT CS MScAc&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Apr 18, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UBC ECE MSc&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Apr 15, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Columbia CS MS&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Withdrawn&lt;/td&gt;
&lt;td&gt;May 13, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Berkeley EECS MEng&lt;/td&gt;
&lt;td&gt;Match&lt;/td&gt;
&lt;td&gt;Rejected&lt;/td&gt;
&lt;td&gt;Apr 14, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cornell ECE MEng&lt;/td&gt;
&lt;td&gt;Safety&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Mar 9, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;USC CS MS&lt;/td&gt;
&lt;td&gt;Safety&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Mar 14, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UPenn ESE MSE&lt;/td&gt;
&lt;td&gt;Safety&lt;/td&gt;
&lt;td&gt;Admitted&lt;/td&gt;
&lt;td&gt;Mar 24, 2023&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;文书&#34;&gt;文书&lt;/h2&gt;
&lt;p&gt;文书的重要性无法确定，但是我感觉凭&lt;strong&gt;先尽人事而以待天命&lt;/strong&gt;的态度，我也得把文书好好写了，不仅是为了申请，也是为了给自己大学三年一个交代。在反复修改PS的时候，也对未来的career plan有了一些清楚的认知。&lt;/p&gt;
&lt;h3 id=&#34;cv&#34;&gt;CV&lt;/h3&gt;
&lt;p&gt;我在大三申请Mitacs暑研的时候就写过了CV，申请季用了新的LaTeX template重构了一下，模版在&lt;a href=&#34;https://github.com/preminstrel/MSApplication/tree/master/CV&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;PDF：&lt;a href=&#34;https://preminstrel.com/MSApplication/CV/resume.pdf&#34;&gt;https://preminstrel.com/MSApplication/CV/resume.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;html：&lt;a href=&#34;https://preminstrel.com&#34;&gt;https://preminstrel.com&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;页边距可以适当调整，但是不要太离谱&lt;/li&gt;
&lt;li&gt;注意一些格式，比如bullet point，时间格式等&lt;/li&gt;
&lt;li&gt;不要超过两页，除非特别牛，Pub很多那种；如果没有价值的东西太多，建议一页，否则审材料的人一看就感觉你水水的&lt;/li&gt;
&lt;li&gt;检查embedded link是否有效&lt;/li&gt;
&lt;li&gt;用Grammarly或GPT检查一下语法拼写&lt;/li&gt;
&lt;li&gt;不用写上了哪些课，除非真的没东西写了/大幅度转专业&lt;/li&gt;
&lt;li&gt;建议写的compact一些&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sop&#34;&gt;SoP&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://trinkle23897.github.io/posts/application&#34;&gt;n+e的这篇blog&lt;/a&gt; 讲的很好，我刚开始SoP也是CV的单纯流水账，什么都想塞进去，导致写得很烂还很长，SoP一般要求是两页，about 1k words。之后我痛改前非，几乎重构了第一版的SoP。到最后，大概修订了五个版本的样子。我还找了fiverr帮我改SoP，真的好贵。有几个点是我想说的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最好删除已经在简历中出现过的GPA，成绩，奖项，申请；SoP要展示的是你的motivation和experience，而不是重复成绩单上的东西&lt;/li&gt;
&lt;li&gt;Motivation，有意思的开头有深度的引子，讲出为什么喜欢这个专业&lt;/li&gt;
&lt;li&gt;Preparation/Qualification，体现了主观的motivation转向实践（详细），描写最重要的两个项目（精华），对专业的认识加深，体现了自己的客观capability，进一步激发了兴趣，并加强了你的信念，即很适合该课程的学习&lt;/li&gt;
&lt;li&gt;强调过程，不要设置一个问题并跳到解决方案。展示达成该解决方案的过程。给出方法和推理的细节。举一个之前看到的例子，&amp;ldquo;As a result of the in-depth exposure to networks I gained in the dormitory project, I was well prepared for the challenges that awaited me as the manager of Information Services at the Transportation Center at Northwestern University.&amp;quot;，只有在最后一句，才开始扩展到更广泛的教训&lt;/li&gt;
&lt;li&gt;提出了一个明确的、有意义的观点，与该段中提出的证据紧密相连。还必须谈到一些更广泛的意义，但是很多人很容易被冲昏头脑，写得太笼统，或者采取简单的方法用一些肤浅的东西来作为result。即套话是没有用的，这一点有点像托福写作，Do not be hollow。&lt;/li&gt;
&lt;li&gt;Career goals，写出你的3-5年的future plan和ultimate goal&lt;/li&gt;
&lt;li&gt;Why School，research部分可以提到到心仪faculty的名字，并表明对他们的工作有一些了解。这个需要去对每个学校你感兴趣的faculty进行调研，看看他们最近发的paper是干嘛的；课程部分也可以写，可以体现你的interest&lt;/li&gt;
&lt;li&gt;最好有整个文章的大目的，要有递进式，段与段之间要有比较自然的transition&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* Avoid the &amp;#34;what I did with my life&amp;#34; approach.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* Avoid the &amp;#34;I&amp;#39;ve always wanted to be a &amp;#34; approach.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* Avoid a catalog of achievements. This is only a list of what you have done, and tells nothing about you as a person. Normally, the statement is far more than a resume.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* Avoid lecturing the reader. For example, you should not write a statement such as &amp;#34;Communication skills are important in this field.&amp;#34; Any graduate admissions committee member knows that and is not trying to learn about the field from the applicant. Some statements do ask applicants about their understanding of the field.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* It should be objective, yet self-revelatory. Write directly and in a straightforward manner that tells about your experience and what it means to you. Do not use &amp;#34;academese.&amp;#34; This is not a research paper for a professor.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* It should form conclusions that explain the value and meaning of your experience, such as what you learned about yourself and your field, your future goals, and your career plans. Draw your conclusions from the evidence your life provides.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* It should be specific. Document your conclusions with specific instances, or draw your conclusions as the result of individual experience. See below a list of general words and phrases to avoid using without explanation.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* It should be an example of careful persuasive writing. Career Center Counselors can help you determine if this is so by reviewing your draft statement.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* It should get to the point early on and catch the attention of the reader.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;* It often should be limited in length, no more than two pages or less. In some instances it may be longer, depending on the school&amp;#39;s instructions.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;psphsdiversity-statement&#34;&gt;PS/PHS/Diversity Statement&lt;/h3&gt;
&lt;p&gt;我是亚洲男性工科生，感觉没有任何的diversity，只能说是first generation这样的。我写了在加拿大暑研的经历，感受到了加拿大推行diversity，想要以后为这个field contribute什么的，感觉没一点用。这里挺推荐各位女同学抽一抽顶级项目的，感觉顶级项目还是很看diversity的，比如大S的EE，国内今年录取的女生是数倍于男生的。这里又得夸一夸&lt;a href=&#34;https://trinkle23897.github.io/posts/application&#34;&gt;n+e的这篇blog&lt;/a&gt; 里面的PHS了，他写的非常好，他在GitHub上有很多开源的贡献，可以从这个点出发。&lt;/p&gt;
&lt;h2 id=&#34;网申&#34;&gt;网申&lt;/h2&gt;
&lt;p&gt;用的outlook邮箱+gmail邮箱搞的，用国内邮箱怕offer/reject直接进trash。送分只送了托福，下面统计一下花费。
总共$2711.88，不到两万人民币：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TOEFL送分 $21x20，其中UCLA送了两次，EPFL没送&lt;/li&gt;
&lt;li&gt;Stanford $125&lt;/li&gt;
&lt;li&gt;Caltech $100&lt;/li&gt;
&lt;li&gt;UCSD $155&lt;/li&gt;
&lt;li&gt;UCLA $155&lt;/li&gt;
&lt;li&gt;UIUC $90&lt;/li&gt;
&lt;li&gt;CMU $75x2&lt;/li&gt;
&lt;li&gt;GaTech $85&lt;/li&gt;
&lt;li&gt;Duke $95&lt;/li&gt;
&lt;li&gt;UofT $91.31&lt;/li&gt;
&lt;li&gt;Cornell $105&lt;/li&gt;
&lt;li&gt;NWU $95&lt;/li&gt;
&lt;li&gt;USC $90&lt;/li&gt;
&lt;li&gt;UPenn $90&lt;/li&gt;
&lt;li&gt;UMich $90&lt;/li&gt;
&lt;li&gt;Columbia  $85&lt;/li&gt;
&lt;li&gt;UBC $122.91&lt;/li&gt;
&lt;li&gt;Berkeley $155&lt;/li&gt;
&lt;li&gt;ETHz $161.33&lt;/li&gt;
&lt;li&gt;EPFL $161.33&lt;/li&gt;
&lt;li&gt;UT Austin $90&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个小插曲，刚开始我还申了UW，但是UW的推荐信系统坏掉了推荐信没发出去，加上网申系统很烂，失去好感，后续没申请；ETHz刚开始一直收不到Xingyu的推荐信，之后联系了小蜜解决了。&lt;/p&gt;
&lt;h2 id=&#34;等拒信&#34;&gt;等拒信&lt;/h2&gt;
&lt;p&gt;这里主要是timeline，偷个懒，把知乎的回答粘贴过来。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;学校：中游工科 985&lt;/p&gt;
&lt;p&gt;专业：EE&lt;/p&gt;
&lt;p&gt;申请：ECE/CS&lt;/p&gt;
&lt;p&gt;GPA：3.98/4.0, 均分94+, 小专业Rank1, TOEFL 115 (S27)，GRE没空学了就没考（菜狗不想努力了&lt;/p&gt;
&lt;p&gt;科研：校内三段，mitacs加拿大暑研一段，水论文几篇&lt;/p&gt;
&lt;p&gt;实习：Apple R&amp;amp;D一段 (Nov-present, base深圳)&lt;/p&gt;
&lt;p&gt;推荐信：两封本校+一封海外暑研&lt;/p&gt;
&lt;p&gt;其他：国奖x2, 其他小奖加起来不到2w (太碎了，很多都没想写到CV)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Dec 23: 交完了所有的申请，自此邮箱陷入长期沉默状态&lt;/p&gt;
&lt;p&gt;Feb 7: 1st AD ECE@Duke&lt;/p&gt;
&lt;p&gt;Feb 10: 1st REJ CS@NWU，NWU CS今年貌似不咋收陆本&lt;/p&gt;
&lt;p&gt;Feb 18：2nd AD ECE@CMU，有点想去&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://smms.app/image/Y7xthfnimgvRMp4&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/21/Y7xthfnimgvRMp4.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Feb 23：2nd REJ EE@Stanford，摸奖失败，今年好像扩招了，比往年收了更多人，女生居多&lt;/p&gt;
&lt;p&gt;Mar 9：3rd AD ECE MEng@Cornell，应该不会去，算集邮了&lt;/p&gt;
&lt;p&gt;Mar 10：1st WL MSIN@CMU，听说往年5月份Waitlist才可能转正，其实相当于REJ了&lt;/p&gt;
&lt;p&gt;Mar 14：4th AD CS37@USC，3rd REJ CS28@USC，还收到了UofT的interview邀请&lt;/p&gt;
&lt;p&gt;Mar 16：4th REJ CS@UIUC，5th REJ EE@EPFL&lt;/p&gt;
&lt;p&gt;Mar 18：晚上和UofT的老师interview，聊了45min&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://smms.app/image/qEAJX3wO4sKu5DN&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/22/qEAJX3wO4sKu5DN.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mar 24：5th AD EE@UPenn&lt;/p&gt;
&lt;p&gt;Mar 27：6th AD ECE@Gatech，听说这个项目很好转PhD；6th REJ EE@Caltech；感觉到彩票要全聚德了…&lt;/p&gt;
&lt;p&gt;Apr 4：7th AD EEIT@ETHz，曾经的欧陆梦，没交GRE也录了，感觉明面上的录取requirements说的GRE required就是扯淡（&lt;/p&gt;
&lt;p&gt;Apr 11：7th REJ CSE@UMich&lt;/p&gt;
&lt;p&gt;Apr 14：8th REJ EECS MEng@Berkeley&lt;/p&gt;
&lt;p&gt;Apr 15：9th REJ ECE@UBC&lt;/p&gt;
&lt;p&gt;Apr 18：8th AD ECE@UCLA，10th REJ MScAC@UofT&lt;/p&gt;
&lt;p&gt;Apr 25：11th REJ ECE@UT Austin&lt;/p&gt;
&lt;p&gt;Apr 26：12th REJ MSIN@CMU，Waitlist之后也没发love letter 理所应当的rej（逃&lt;/p&gt;
&lt;p&gt;May 9：13th REJ ECE PhD@UCSD，彩票正式全聚德！&lt;/p&gt;
&lt;h2 id=&#34;miscfun-facts&#34;&gt;Misc/Fun Facts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ETHz有一个ESOP option，我还写了一个master thesis pre-proposal，实际上就是把本科毕业设计的任务书改了改加了点文献翻译一下交了过去，还装模作样地拟了一个timeline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://smms.app/image/orPIGVeyFKL82u7&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/21/orPIGVeyFKL82u7.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择更加适合的项目而不是bar更高的项目，适合的才是最好的，不要学生思维，只看ranking和reputation选学校&lt;/li&gt;
&lt;li&gt;connection is all you need&lt;/li&gt;
&lt;li&gt;不要焦虑地等结果，找个事情让自己忙起来&lt;/li&gt;
&lt;li&gt;注意身体健康，合适地摆烂很重要&lt;/li&gt;
&lt;li&gt;申请也不过是人生中一个小小的stage&lt;/li&gt;
&lt;li&gt;我的一些申请资料：&lt;a href=&#34;https://github.com/preminstrel/MSApplication&#34;&gt;preminstrel/MSApplication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fin&#34;&gt;Fin&lt;/h2&gt;
&lt;p&gt;“其实并不是打从一开始就知道自己要前进的方向，只是回过神时，自己便置身于此地了。仅此而已。”&lt;/p&gt;
&lt;p&gt;特别感谢一直支持我的父母，老师和朋友。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;孙寒石&lt;/p&gt;
&lt;p&gt;于2023年立夏&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Canada Visa</title>
      <link>https://preminstrel.github.io/blog/post/2022/05/09/canada-visa/</link>
      <pubDate>Mon, 09 May 2022 16:15:25 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/05/09/canada-visa/</guid>
      
      <description>&lt;h2 id=&#34;application---march&#34;&gt;Application - March&lt;/h2&gt;
&lt;p&gt;参与 Mitacs 项目需要申请加拿大签证，这里进行一个申请的记录。这个项目让我们申请的是 TRV (Temporary Resident Visa)，通过 GSS (Global Skill Strategy) 在入境时申请 Work Exemption，也就是说，我申请的是 Visitor Visa V-1。但是实际上，目前很多人下的签都是 WX-1，是工签，有人是 MULTIPLE，也有人是 ONE。由于我还没贴签，所以不太清楚自己下的是什么。&lt;/p&gt;
&lt;p&gt;申请的时候，有两个通道，分别是 GCKey 和 IRCC Portal，前者是老通道，后者是疫情之后才开放的新通道。老通道是填表，同时要上传很多材料；新通道是类似于做问卷一样，填入一些信息，eye-friendly。我选择了 Portal 进行申报，在三月一号的时候完成了签证的申请。在缴费 $185 的时候，我发现用 Mastercard 付款失败了，但是用银联的信用卡反而成功了，群里也有人反应说借记卡也可以付款成功。指纹采集信当天就下来了，我预约了三月四号中午去上海的签证中心录指纹。&lt;/p&gt;
&lt;h2 id=&#34;biometrics---march&#34;&gt;Biometrics - March&lt;/h2&gt;
&lt;p&gt;三月四日，我坐动车去上海的加拿大签证中心进行生物信息采集 (Biometrics)，本想去一趟斌斌舅舅家玩，但是由于他有事还是算了。录指纹的时候要携带：护照、护照复印件、同意书、指纹采集信；在采集完之后会给你一个热敏打印一样的小贴纸，这个不能扔，要好好保管住了，在贴签的时候以及出入境都要查看。指纹有效期是十年，所以十年之内再申请加签是不用再录指纹的。录完指纹后，我在南京路逛了逛，中午吃了西餐，下午去了一趟外文书店看了看。随后改签了动车，早早就回无锡了。&lt;/p&gt;
&lt;h2 id=&#34;webform---april&#34;&gt;Webform - April&lt;/h2&gt;
&lt;p&gt;签证迟迟不下，我便利用 Webform 进行了催签，上海和北京和加拿大的都催了一下，收到了北京签证中心的回复，但是回复令人心寒，通篇的基调是 negative 的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Email from &lt;code&gt;beijing-immigration@international.gc.ca&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;Please be advised that your application is currently undergoing standard background checks and the processing time will be extended. Unfortunately, we are unable to advice at this time when a final decision might be made.&lt;/p&gt;
&lt;p&gt;Please note that all applications are considered on their own merits and there is no guarantee that a visa will be issued.  You will be advised if any further documentation or information is required.&lt;/p&gt;
&lt;p&gt;Should you wish to withdraw your application, please send signed a written request advising this office that you wish to withdraw your application to &lt;a href=&#34;mailto:beijing-immigration@international.gc.ca&#34;&gt;beijing-immigration@international.gc.ca&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;加拿大签证中心给的回复稍微中性一些，但是都是套话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Good day Hanshi Sun,&lt;/p&gt;
&lt;p&gt;Thank you for contacting Immigration, Refugees and Citizenship Canada (IRCC).&lt;/p&gt;
&lt;p&gt;We verified the information you provided and can confirm that your application is still in process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;re-application---april&#34;&gt;Re-application - April&lt;/h2&gt;
&lt;p&gt;由于害怕自己被拒签，所以又在 GCKey 上申请了一次，多花了 $100，还申请了调档，花了 ￥100。GCKey申请后指纹自动同步了，这次没过几天就开始了 review。过了仅仅五个工作日，在 2022-04-29 的凌晨 02:00，我的邮箱先后收到了 IRCC Portal 的 Withdraw 和 GCKey 的 Original Passport Request，重新递交的效果显著！&lt;/p&gt;
&lt;p&gt;GCKey Timeline:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Subject&lt;/th&gt;
&lt;th&gt;Date sent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Original Passport Request&lt;/td&gt;
&lt;td&gt;April 28, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Correspondence Letter&lt;/td&gt;
&lt;td&gt;April 28, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Submission Confirmation&lt;/td&gt;
&lt;td&gt;April 21, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Confirmation of Online Application Transmission&lt;/td&gt;
&lt;td&gt;April 21, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;申请材料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application for Visitor Visa (Temporary Resident Visa) Made Outside of Canada (IMM5257): &lt;code&gt;imm5257e.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Offer of Employment: &lt;code&gt;invitation-letter + award-letter&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proof of Work Permit Exemption: &lt;code&gt;award-letter.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Family Information Form (IMM5707): &lt;code&gt;imm5707e.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Travel History: &lt;code&gt;travel.pdf&lt;/code&gt; (概述去过的境外国家经历+目的+签证)&lt;/li&gt;
&lt;li&gt;Passport: &lt;code&gt;passport.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;General Education and Employment Form: &lt;code&gt;imm0104e.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proof of Means of Financial Support: &lt;code&gt;invitation-letter + award-letter + 芝麻信用英文报告&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Digital photo: &lt;code&gt;photo.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Purpose of Travel - Other: 描述本次去的目的和行程，写一个文档，转成 PDF 上传&lt;/li&gt;
&lt;li&gt;Proof that you Meet the Requirements of the Job Being Offered： 成绩单 + invitation-letter + award-letter&lt;/li&gt;
&lt;li&gt;Schedule 1 - Application for a Temporary Resident Visa Made Outside Canada (IMM 5257): &lt;code&gt;imm5257b_1.pdf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当天将护照和相关材料用顺丰寄出，寄到北京加拿大签证申请中心，静候贴签的 tracking number。&lt;/p&gt;
&lt;h2 id=&#34;vfs---may&#34;&gt;VFS - May&lt;/h2&gt;
&lt;p&gt;五月四号，五一节之后的第一天，收到 VFS 邮件，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - Your application has been dispatched from the Canada Visa Application Centre to the IRCC Office on Wed May 04 2022 for processing.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第二封：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - Your application has been received and is under process at the IRCC Office on Thu May 05 2022.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第三封（五月六日）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - The decision envelope for your application has been dispatched from the IRCC Office to the Canada Visa Application Centre.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第四封（五月六日）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - The decision envelope for your application has been received at the Canada Visa Application Centre on Fri May 06 2022 from the IRCC Office, and is ready for collection or further delivery by courier as per your option upon the submission of the application. For details of the VAC locations, please refer to our website at: &lt;a href=&#34;https://visa.vfsglobal.com/chn/en/can/attend-centre;&#34;&gt;https://visa.vfsglobal.com/chn/en/can/attend-centre;&lt;/a&gt; for the business hours of operation under the COVID impact, please refer to our website at: &lt;a href=&#34;https://visa.vfsglobal.com/chn/en/can/news/covid-update&#34;&gt;https://visa.vfsglobal.com/chn/en/can/news/covid-update&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第五封（五月六日）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The decision envelope for your application, tracking ID No. 20220504CNBJPKT72344 has been couriered from the Canada Visa Application Centre, Canada Visa Application Centre, Beijing on Fri May 06 2022 via courier partner. Please use Tracking id. or the AWB number provided to track the shipment on the courier partners website after 24hrs.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;五月九日，收到了北京发来的 EMS，贴签结束。查看签证后，发现是 WX-1，有效期到 2029-07-08，还算满意了。但是申请签证的过程，堪称折磨。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Mitacs Globalink Research Intern</title>
      <link>https://preminstrel.github.io/blog/post/2022/04/15/mitacs/</link>
      <pubDate>Fri, 15 Apr 2022 13:47:07 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/04/15/mitacs/</guid>
      
      <description>&lt;p&gt;去年八月份的时候，学校教务处官网发了申报 Mitacs 项目的相关申报流程。简单来说，这个项目是加拿大 Mitasc 和 CSC 的合作项目，每年选派 200 名本科生在大三暑假去加拿大各高校去进行 Research。这个项目和自己套的暑研比，优缺点也是比较明显的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: 每个月国家提供经费 $1800，国家承担来回机票，报销签证费用；Mitacs 免费帮买保险，也有理由可以去进行课内的暑期短学期的学分替换，签证申请比较方便等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cons&lt;/strong&gt;: 套到的项目和学校都不如自己套的好，项目匹配机制比较迷，周期很长，需要填很多材料，申请流程比较繁琐。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我最后是录到了 University of Alberta 的 ECE department 的一个华人老师 &lt;a href=&#34;https://www.ece.ualberta.ca/~xingyu/index.html&#34;&gt;Xingyu Li&lt;/a&gt; 的 Weak/self supervision for abnormal detection in medical image analysis 项目，下面我来简单谈谈申请流程。&lt;/p&gt;
&lt;h2 id=&#34;personal-background&#34;&gt;Personal Background&lt;/h2&gt;
&lt;p&gt;首先提供我申请时的背景。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本科 SEU，CGPA: 3.98/4, 93.8/100&lt;/li&gt;
&lt;li&gt;无语言成绩，无竞赛，有国奖等零碎的奖学金&lt;/li&gt;
&lt;li&gt;有一段相关科研和毫不相关的科研经历，有一篇 EI 水会并未在 CV 提及&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;2021-09-04&lt;/em&gt;   完成 Mitacs 网申，收到自动回复的邮件说 Application Complete&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-09-26&lt;/em&gt;   收到邮件说申请 portal 已关闭：2022 Globalink Research Internship applications now closed&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-07&lt;/em&gt;   陆续有人收到拒信&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-10&lt;/em&gt;   官网更新消息，CUC (Candidate under consideration)，也就是通过初审，进入项目匹配阶段。这一阶段大家可能会收到一些老师的面试和相关信息&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-18&lt;/em&gt;   收到第一个面试邮件，是我 Rank 7 UBC 的一位女老师。提出了以下面试细节：
&lt;ul&gt;
&lt;li&gt;5-10 min interview&lt;/li&gt;
&lt;li&gt;Zoom is required, with video if possible&lt;/li&gt;
&lt;li&gt;time restricted, on Friday Nov 19th, noon PDT (Vancouver time)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-20&lt;/em&gt;   凌晨四点，进入 Zoom 会议，面试的人有七个，我排在倒数第二个，我以为大概五点多到我，但是四点半就到我了。因为这个老师面试极快，先 30s 介绍了自己的项目，然后就问了我一个问题——介绍你以前的project，我讲之前，她问我你能不能在两分钟之内结束。我打开自己做的 Slides 开始讲，成功在两分钟之内讲完了。说好的 5-10 min 呢？我很快讲完之后，她直接说：OK，have a good day, you can leave the zoom room. 面完就感觉她挺急的，这个 rank 7 的项目估计是寄了，不过也不咋心疼&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-12-10&lt;/em&gt;   Portal 出 offer 了 是 Alberta 无面试录了。下午一点收到了 offer 邮件，说匹配成功，Congratulations&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-12-11&lt;/em&gt;   可以在 Portal 上 accept 了，接受之后会有一封感谢信发到邮箱&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-02-17&lt;/em&gt;   收到下一步的邮件，填写一些 agreement 和个人信息。关于疫情是否 on site，Mitacs 觉得是大概率 on site 的。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-03-01&lt;/em&gt;   在 IRCC Portal 申请了签证&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-03-04&lt;/em&gt;   去上海签证中心录生物信息，进入漫长的 Visa 审核流程&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-03-22&lt;/em&gt;   CSC 系统更新，发贺信，进行签约和一些银行卡开卡手续&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-04-15&lt;/em&gt;   最近上海爆发疫情，CSC 银行卡的寄送受到了影响，我们也封校了，签证还在审理中&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;application-detail&#34;&gt;Application Detail&lt;/h2&gt;
&lt;p&gt;这里说一说我认为的对成功申请和匹配的一些建议和细节。首先，匹配分两个阶段。第一阶段初审过后，会分为三类：CUC、Waitlist、Rej。CUC 代表经过初审，可以去和教授进行面试和 rerank 来匹配。匹配结束后，CUC 有可能没匹配上，进入 Waitlist。然后是第二轮匹配，这个时候第一轮的 Waitlist 有概率会被录取，但是可能性比较小。&lt;/p&gt;
&lt;h3 id=&#34;初审-rightarrow-cuc&#34;&gt;初审 $\rightarrow$ CUC&lt;/h3&gt;
&lt;p&gt;想要通过初审，那么 GPA 和 Resume 是最重要的。也可以看到，我没有任何竞赛和语言成绩，依旧通过了初审。所以 transcript 和 Resume 至关重要。GPA 这个也不好准备，大家都知道，基本上低于 85 分就没啥希望了。主要努力方向还是 Resume。Mitacs 官网会提供 Resume Template，但是我不建议用。个人写自己的 Resume 是要去突出自己的优势，稍微掩盖自己的劣势的。同时，Mitacs 给的模板比较粗糙，不好看。所以我建议用 LaTeX 进行写作，Overleaf 上也可以找到一些好看的模板，不会装 LaTeX 的可以用 Overleaf 在线编译下载 PDF（最好还是学一下 LaTeX，这个不会就去做 Research 也挺离谱的）。&lt;/p&gt;
&lt;h3 id=&#34;interview-rightarrow-offer&#34;&gt;Interview $\rightarrow$ Offer&lt;/h3&gt;
&lt;p&gt;这个阶段基本是最难熬的，可能会很焦虑。尤其是当你发现，自己一封邮件没收到，群里小伙伴有的都面完五六个老师的时候，心里会很不是滋味。这里我想说的是，无面试录取的可能性很大，尤其是 University of Alberta，好多都是没面试直接拿 Offer 的。如果你有幸被面试，那么一定要提前做好展示的 Slides，然后配置好网络环境（科学上网），最后面试完给老师发一封感谢信。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;这个项目总体来说，由于 Mitacs 的谜一般的匹配机制，所以也是比较看运气的。所以，如果没录不用觉得自己咋样，自己套往往是更好的。祝大家都能拿到 offer。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Transformer</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/26/transformer/</link>
      <pubDate>Wed, 26 Jan 2022 17:47:40 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/26/transformer/</guid>
      
      <description>&lt;p&gt;《Attention Is All You Need》是 Google 团队在 2017 年提出的一篇论文。该论文以“attention”为核心，提出了 Transformer 模型。Transformer 基于 Encoder-Decoder，摒弃了 CNNs，完全由 Attention mechanism 实现。&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;p&gt;传统 seq2seq 最大的问题在于将 Encoder 端的所有信息&lt;strong&gt;压缩到一个固定长度的向量&lt;/strong&gt;中，并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，而且这样一股脑的把该固定向量送入 Decoder 端，Decoder 端不能够关注到其想要关注的信息。并且模型计算不可并行，计算隐层状态 $h_t$ 依赖于 $h_{t-1}$ 以及状态 $t$ 时刻的输入，因此需要耗费大量时间。&lt;/p&gt;
&lt;p&gt;Transformer 完全依赖于 Attention Mechanism，解决了输入输出的长期依赖问题，并且拥有并行计算的能力，大大减少了计算资源的消耗。Self-Attention模块，让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力。Muti-Head Attention 模块使得 Encoder 端拥有并行计算的能力。&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;
&lt;p&gt;Transformer 采用 Encoder-Decoder 架构，如下图所示。Encoder 层和 Decoder 层分别由 6 个相同的 Encoder 和decoder堆叠而成，模型架构更加复杂。其中，Encoder 层引入了 &lt;em&gt;&lt;strong&gt;Multi-Head&lt;/strong&gt;&lt;/em&gt; 机制，可以并行计算，Decoder 层仍旧需要串行计算。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/Ml7Wiqra8TdAZv2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/Ml7Wiqra8TdAZv2.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Encoder 层和 Decoder 层内部结构如下图所示。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/rCmxoUspEFbhfSd&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/rCmxoUspEFbhfSd.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Encoder 具有两层结构，&lt;strong&gt;Self-Attention 和前馈神经网络&lt;/strong&gt;。Self-Attention 计算句子中的每个词都和其他词的关联，从而帮助模型更好地理解上下文语义，引入 Muti-Head Attention 后，每个头关注句子的不同位置，增强了Attention 机制关注句子内部单词之间作用的表达能力。前馈神经网络为 Encoder 引入非线性变换，增强了模型的拟合能力。&lt;/li&gt;
&lt;li&gt;Decoder 接受 output 输入的&lt;strong&gt;同时接受 Encoder 的输入&lt;/strong&gt;，帮助当前节点获取到需要重点关注的内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-head-attention&#34;&gt;Multi-Head Attention&lt;/h3&gt;
&lt;p&gt;Multi-Head Attention 计算过程如下图，在讲解Multi-Head Attention 之前，我们需要了解Self-Attention。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/vuW2BzLpKig3lrV&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/vuW2BzLpKig3lrV.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Query 与 Key 作用得到 Attention 的权值，之后这个权值作用在 Value 上得到 Attention值。&lt;/strong&gt; 这种通过 Query 和 Key 的相似性程度来确定 value 的权重分布的方法被称为 &lt;em&gt;&lt;strong&gt;scaled dot-product attention&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{D_k}})V$$&lt;/p&gt;
&lt;p&gt;这里给出我在知乎上看到的一个很不错的帖子里面的图片解释 scaled dot-product attention：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/1VaBDNAm4S2Yex9&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/1VaBDNAm4S2Yex9.jpg&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;但是，在 Transformer 模型中，作者使用了 Muti-Head 机制代替了 single self-attention。&lt;/p&gt;
&lt;p&gt;$$
\text{MultiHead}(Q,K,V) =\text{Concat}\left(\text {head}_1, \ldots, \text{head}_h \right) W^{O}
$$&lt;/p&gt;
&lt;p&gt;$$
\text{where head}_{\mathrm{i}} =\operatorname{Attention}\left(QW_i^Q, KW_i^K, VW_i^V \right)
$$&lt;/p&gt;
&lt;p&gt;Where the projections are parameter matrices $W_{i}^{Q} \in \mathbb{R}^{d_{model} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{model} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{model} \times d_{v}}$ and $W^{O} \in \mathbb{R}^{h d_{v} \times d_{model}}$.&lt;/p&gt;
&lt;p&gt;论文中采用 8 个头，$h=8,d_{k}=d_{v}=d_{model} / h=64$。通过权重矩阵 $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ 将 $Q,K,V$ 分割，每个头分别计算 single self-attention，因为权重矩阵 $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ 不相同，$QW_i^Q,KW_i^K,VW_i^V$ 的结果各不相同，因此我们说每个头的关注点各有侧重。最后，将每个头计算出的 single self-attention 进行 concat，通过总的权重矩阵 $W^O$ 决定对每个头的关注程度，从而能够做到在不同语境下对相同句子进行不同理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention 是将 Query 和 Key 映射到同一高维空间中去计算相似度，而对应的 Multi-head Attention 把 Query 和 Key 映射到高维空间 $\alpha$ 的不同子空间 $(\alpha_1,\alpha_2,\dots, \alpha_h)$ 中去计算相似度。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;position-wise-feed-forward&#34;&gt;Position-wise Feed Forward&lt;/h3&gt;
&lt;p&gt;$$\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2$$&lt;/p&gt;
&lt;p&gt;每一层经过 Attention 之后，还会有一个 FFN，这个 FFN 的作用就是&lt;strong&gt;空间变换&lt;/strong&gt;。FFN 包含了 2 层 Linear Transformation 层，中间的激活函数是 ReLU。&lt;/p&gt;
&lt;p&gt;Attention 层的 output 最后会和 $W^O$ 相乘，为什么这里又要增加一个 2 层的 FFN 网络？其实，FFN 的加入&lt;strong&gt;引入了非线性(ReLu激活函数)，变换了 Attention Output 的空间, 从而增加了模型的表现能力&lt;/strong&gt;。把 FFN 去掉模型也是可以用的，但是效果差了很多。&lt;/p&gt;
&lt;h3 id=&#34;layer-normalization&#34;&gt;Layer Normalization&lt;/h3&gt;
&lt;p&gt;在每个 block 中，最后出现的是 Layer Normalization，其作用是规范优化空间，加速收敛。&lt;/p&gt;
&lt;p&gt;$$\text{LN}(x_i)=\alpha\frac{x_i-\mu_i}{\sqrt{\sigma^2+\xi}}+\beta$$&lt;/p&gt;
&lt;p&gt;当我们使用梯度下降算法做优化时，我们可能会对输入数据进行归一化，但是经过网络层作用后，我们的数据已经不是归一化的了。随着网络层数的增加，数据分布不断发生变化，偏差越来越大，导致我们不得不使用&lt;strong&gt;更小的学习率&lt;/strong&gt;来稳定梯度。Layer Normalization 的作用就是&lt;strong&gt;保证数据特征分布的稳定性&lt;/strong&gt;，将数据标准化到 ReLU 激活函数的作用区域，可以使得激活函数更好的发挥作用&lt;/p&gt;
&lt;h3 id=&#34;positional-encoding&#34;&gt;Positional Encoding&lt;/h3&gt;
&lt;p&gt;位置信息编码位于 Encoder 和 Decoder 的 Embedding 之后，每个 block 之前。它非常重要，没有这部分模型就无法运行。Positional Encoding 是 Transformer 的特有机制，弥补了 Attention 机制无法捕捉 sequence 中 token 位置信息的缺点。&lt;/p&gt;
&lt;p&gt;$$
PE_{(pos, 2i)}=\sin\left(pos/10000^{2i/d_{\text{model}}}\right)
$$&lt;/p&gt;
&lt;p&gt;$$
PE_{(pos,2i+1)}=\cos\left(pos/10000^{2i/d_{\text{model}}}\right)
$$&lt;/p&gt;
&lt;p&gt;Positional Embedding 的成分直接叠加于 Embedding 之上，使得每个 token 的&lt;strong&gt;位置信息&lt;/strong&gt;和它的&lt;strong&gt;语义信息&lt;/strong&gt;(embedding)充分融合，并被传递到后续所有经过复杂变换的序列表达中去。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transformer 中，模型输入 Encoder 的每个 token 向量由两部分加和而成：Position Encoding + Input Embedding。Transformer 的特性使得输入 Encoder 的向量之间完全平等（不存在 RNN 的 recurrent 结构），token 的实际位置于位置信息编码唯一绑定。Positional Encoding 的引入使得模型能够充分利用 token 在 sequence 中的位置信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;论文中使用的 Positional Encoding(PE) 是正余弦函数，位置(pos)越小，波长越长，每一个位置对应的 PE 都是唯一的。同时作者也提到，之所以选用正余弦函数作为 PE，是因为这可以使得模型学习到 token 之间的相对位置关系：因为对于任意的偏移量 $k$，$PE_{pos+k}$ 可以由 $PE_{pos}$ 的线性表示，也就是 $PE_{pos}$ 乘上某个线性变换矩阵就得到了 $PE_{pos+k}$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
P E_{(p o s+k, 2 i)}=\sin \left((p o s+k) / 10000^{2 i / d_{\text {model }}}\right)
$$$$
P E_{(p o s+k, 2 i+1)}=\cos \left((p o s+k) / 10000^{2 i / d_{\text {model }}}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;mask&#34;&gt;Mask&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Mask&lt;/strong&gt;&lt;/em&gt; 表示掩码，它&lt;strong&gt;对某些值进行掩盖，使其在参数更新时不产生效果&lt;/strong&gt;。Transformer 模型里面涉及两种 Mask，分别是 Padding Mask 和 Sequence Mask。其中，Padding Mask 在所有的 scaled dot-product attention 里面都需要用到，而 Sequence Mask 只有在 Decoder 的 Self-Attention 里面用到。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Padding Mask&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;什么是 Padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的 Attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。&lt;/p&gt;
&lt;p&gt;具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！&lt;/p&gt;
&lt;p&gt;而我们的 Padding mask 实际上是一个张量，每个值都是一个Boolean，值为 False 的地方就是我们要进行处理的地方。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Sequence mask&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Sequence Mask 是为了使得 Decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。&lt;/p&gt;
&lt;p&gt;具体办法是：&lt;strong&gt;产生一个上三角矩阵，上三角的值全为 0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于 Decoder 的 Self-Attention，里面使用到的 scaled dot-product attention，同时需要 Padding Mask 和 Sequence mask 作为 attn_mask，具体实现就是两个 Mask 相加作为 attn_mask。其他情况，attn_mask 一律等于 Padding mask。&lt;/p&gt;
&lt;h3 id=&#34;linear--softmax&#34;&gt;Linear &amp;amp; Softmax&lt;/h3&gt;
&lt;p&gt;Decoder 最后是一个线性变换和 Softmax 层。解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是 Softmax 层。&lt;/p&gt;
&lt;p&gt;线性变换层是一个简单的全连接神经网络，它可以&lt;strong&gt;把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里&lt;/strong&gt;。不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数（&lt;strong&gt;相当于做 vocaburary_size 大小的分类&lt;/strong&gt;）。接下来的 Softmax 层便会把那些分数变成概率（都为正数、上限 1.0）。&lt;strong&gt;概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;整体运行效果图如下：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/7wBRdlvJnzeVUL9&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/7wBRdlvJnzeVUL9.gif&#34; &gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/311156298&#34;&gt;Transformer - Attention is all you need&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Self-Attention</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/26/self-attention/</link>
      <pubDate>Wed, 26 Jan 2022 16:36:51 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/26/self-attention/</guid>
      
      <description>&lt;blockquote&gt;
&lt;p&gt;Attention is all you need.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog，本次来进行一次总结。首先便是 Self-Attention 的公式&lt;/p&gt;
&lt;p&gt;$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{D_k}})V$$&lt;/p&gt;
&lt;h2 id=&#34;terminology&#34;&gt;Terminology&lt;/h2&gt;
&lt;p&gt;公式种出现的 $Q,K,V$ 分别是 Query、Key、Value的缩写，我们的表达式如下：&lt;/p&gt;
&lt;p&gt;$$X W^Q=Q$$
$$XW^K=K$$
$$XW^V=V$$&lt;/p&gt;
&lt;p&gt;文章中所谓的 $Q,K,V$ 矩阵来源于 $X$ 与矩阵的乘积，本质上是 $X$ 的一系列的线性变换。做线性变换是为了提升模型的拟合能力，矩阵 $W$ 都是可以训练的，起到一个缓冲的效果。&lt;/p&gt;
&lt;p&gt;我们假设 $Q,K$ 种元素的均值为 0，方差为 1，$A^T=Q^TK$ 的均值为 0，方程为 $D$。当 $D$ 变得很大时，$A$ 中的元素的方差也会变得很大，如果 $A$ 中的元素方差很大，那么 $A$ 的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。我们可以将分布“陡峭”程度与 $D$ 解耦，从而使得训练过程中梯度值保持稳定。&lt;/p&gt;
&lt;p&gt;$$A\leftarrow \dfrac{A}{\sqrt{D_k}}$$&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;公式中的 $QK^T$ 表示的是 $Q,K$ 的内积，也可以说是一方在另一方的&lt;strong&gt;投影&lt;/strong&gt;，其大小也可以表示其&lt;strong&gt;相关性&lt;/strong&gt;。Softmax 是为了将一系列的值&lt;strong&gt;归一化&lt;/strong&gt;而存在的。&lt;/p&gt;
&lt;p&gt;$$\text{softmax}(z_k)=\frac{e^{z_k}}{\sum_{i=1}^Ie^{z_i}}$$&lt;/p&gt;
&lt;p&gt;而随后与 $V$ 的乘积，代表的是&lt;strong&gt;向量经过注意力机制加权求和之后的结果&lt;/strong&gt;。也就是说，softmax 管的是一个相关度权值大小，与后面的 $V$ 相乘，得到的是通过相关度权值标准而重新计算得到的量。&lt;/p&gt;
&lt;p&gt;对 Self-Attention 来说，它跟每一个输入的向量都做 Attention，所以没有考虑到输入的顺序。更通俗来讲，大家可以发现我们前文的计算每一个词向量都与其他词向量计算内积，得到的结果丢失了我们原来文本的顺序信息。对比来说，LSTM 是对于文本顺序信息的解释是输出词向量的先后顺序，而我们上文的计算对 sequence 的顺序这一部分则完全没有提及，你打乱词向量的顺序，得到的结果仍然是相同的，此处便可以引出 Transformer 的位置编码部分。&lt;strong&gt;Query 与 Key 作用得到 Attention 的权值，之后这个权值作用在 Value 上得到 Attention值。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Attention 机制的实现&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;math&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; sqrt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;torch.nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;Self_Attention&lt;/span&gt;(nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Module):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# input : batch_size * seq_len * input_dim&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# q : batch_size * input_dim * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# k : batch_size * input_dim * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# v : batch_size * input_dim * dim_v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;def&lt;/span&gt; __init__(self,input_dim,dim_k,dim_v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a2f&#34;&gt;super&lt;/span&gt;(Self_Attention,self)&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;__init__()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;q &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Linear(input_dim,dim_k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;k &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Linear(input_dim,dim_k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;v &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Linear(input_dim,dim_v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;_norm_fact &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; sqrt(dim_k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;forward&lt;/span&gt;(self,x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Q &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;q(x) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Q: batch_size * seq_len * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        K &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;k(x) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# K: batch_size * seq_len * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        V &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;v(x) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# V: batch_size * seq_len * dim_v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        atten &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Softmax(dim&lt;span style=&#34;color:#666&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;)(torch&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;bmm(Q,K&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;))) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;_norm_fact &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Q * K.T() # batch_size * seq_len * seq_len&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;bmm(atten,V) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Q * K.T() * V # batch_size * seq_len * dim_v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; output
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/410776234&#34;&gt;超详细图解Self-Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Arrhythmia Classifier Using CNN With ALQ</title>
      <link>https://preminstrel.github.io/blog/post/2021/10/17/arrhythmia-classifier-using-cnn-with-alq/</link>
      <pubDate>Sun, 17 Oct 2021 13:37:23 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2021/10/17/arrhythmia-classifier-using-cnn-with-alq/</guid>
      
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;在许多医疗保健方案中，患者被诊断出患有各种各样的疾病，包括心血管疾病（CVDs），这是一种普遍的致命疾病。 心电图描述了人的心电活动，对准确诊断有重要意义。 但在早期，有些心律失常症状不明显，持续时间短，难以识别，导致严重后果。 因此，部署在低功耗设备上的实时心率检测成为人们关注的焦点。&lt;/p&gt;
&lt;p&gt;神经网络通过模拟人脑的层次结构实现数据的层次特征表达，具有强大的信息处理能力，促进了心电分类方法算法和模型的发展。 虽然神经网络模型的检测和分类精度看起来相当可观，但其庞大的可训练网络参数消耗大量内存，需要更多的时间进行复杂的计算，难以部署在低功耗的硬件平台上。 为了解决这个问题，我们考虑了网络结构的设计和自适应性的量化压缩方法。采用自适应位宽量化方法对模型误差进行优化，可以降低典型量化方法的精度下降，甚至提高模型误差的精度。 简单来说，我们的贡献有三个方面:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出了一种自适应损失感知量化算法（ALQ），以降低一维卷积神经网络的内存和功耗，同时保持甚至提高分类精度。&lt;/li&gt;
&lt;li&gt;基于我们的压缩方法，进一步提出了一种基于长时程心电片段分析的17层卷积神经网络（CNN）结构用于心律失常（17类）检测，实现了心律失常检测的总体准确率为93.5%。&lt;/li&gt;
&lt;li&gt;最后，我们实现了量化方法，在存储压缩率达到了 23.4 倍的情况下，分类的准确率达到了95.84%，说明了所提出的量化方法相对于以往方法的优越性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;参考论文：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Zhongnan Qu, Zimu Zhou, Yun Cheng, Lothar Thiele. Adaptive Loss-Aware Quantization for Multi-Bit Networks. In &lt;em&gt;the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt; (CVPR), 2020, pp. 7988-7997&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我根据Qu的论文提出的&lt;strong&gt;自适应损失感知量化  (Adaptive Loss-aware Quantization,ALQ)&lt;/strong&gt; 算法对ECGNet进行了量化压缩。论文中实现了Conv2d的情况，我修改模型后得到Conv1d的模型。在经过ALQ压缩后，能够实现大部分Layers低于 2 位，少部分Layers低于1位的平均位宽（average bitwidth）， 而不会降低模型的精度。 甚至，在量化之后，模型的精度有可能得到提高，达到95%的准确率。（一般来说会降低，这里我也不清楚为什么会提高，猜测有可能是量化压缩提高了正则化）&lt;/p&gt;
&lt;p&gt;量化解决方案通过最小化误差以重建全精度权重来训练量化器不同， ALQ 直接最小化损失函数上的量化引起的误差， 既不涉及梯度近似（gradient approximation）也不涉及全精度维护（full precision maintenance）。 ALQ 还利用了包括自适应位宽、 平滑位宽减少、 和迭代训练的量化，以允许更小的网络规模而不会损失准确性。&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;在本节中，我们首先介绍分类器的架构概述，并描述我们的一维 CNN 架构的细节。在本节的最后，讨论了ALQ策略和一些ALQ参数的选择。整个提出的框架可以分为两部分，如图所示。&lt;/p&gt;
&lt;p&gt;第一部分是心律失常分类神经网络体系结构，该体系结构基于基本块设计，确定神经网络的深度。对模型进行训练后，得到的ECGNet的精度达到93.5%。模型参数应被保存，以便进行量化压缩。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/FCWKTZGctHsUDM5&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/FCWKTZGctHsUDM5.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;第二部分是ALQ策略（Adaptive Loss-aware quantification）。网络中每一层对量化的敏感度是不同的。因此，假设我们给出的总比特数不变，对量化较敏感的层位数获得的位宽应该较大，而对量化较不敏感的层位获得的位宽应该较小，以达到更好的精度。该方法通过对 $\alpha$ 域的最小有效坐标进行剪枝来降低平均位宽，并在正确选择 $n$ 等参数的基础上优化二值化的基底 $B_k$ 和坐标 $\alpha_k$ 。该部分实现了对神经网络有效的压缩，不同于现有方法，成功避免了精度下降可，以满足较低的资源需求。&lt;/p&gt;
&lt;h3 id=&#34;ecgnet-model&#34;&gt;ECGNet Model&lt;/h3&gt;
&lt;p&gt;我们原创的心律失常分类卷积神经网络如图所示。该网络由若干基本块和两个线性层组成。基本块层包括一维卷积层和最大池化层，它们之间的激活为 ReLU。基本块用于特征提取，线性层用于分类。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/L9ym6FfDrJVEoYH&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/L9ym6FfDrJVEoYH.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;输入是原始的长时间心电信号，由3600个采样点组成，持续时间为10秒。该网络无需对原始信号进行人工特征提取、特征分割和数据处理，即可实现端到端检测并推断出分类输出。在设计网络结构时，我们在网络规模和精度之间进行了权衡。最后，我们决定基本块的数量应该是7，因为这样的深度可以产生相当多的输出。同时，它保留了很小的网络参数。因此，我们最后得到的卷积神经网络是17层的。具体网络设计结构如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Layer&lt;/th&gt;
&lt;th&gt;Layer Name&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Other Layer Params&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Kernel $\times$ Unit&lt;/th&gt;
&lt;th&gt;Params&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=2, Padding=7&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$16\times 8$&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stride=4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$8$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=2, Padding=5&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$12\times 12$&lt;/td&gt;
&lt;td&gt;1,164&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stride=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$4$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=1, Padding=4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$9\times 32$&lt;/td&gt;
&lt;td&gt;3,488&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stride=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$5$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=1, Padding=3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$7\times 64$&lt;/td&gt;
&lt;td&gt;14,400&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stride=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$4$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=1, Padding=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$5\times 64$&lt;/td&gt;
&lt;td&gt;20,544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stride=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=1, Padding=1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$3\times 64$&lt;/td&gt;
&lt;td&gt;12,352&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;12&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Strides=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13&lt;/td&gt;
&lt;td&gt;Conv1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Strides=1, Padding=1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$3\times 72$&lt;/td&gt;
&lt;td&gt;13,896&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;14&lt;/td&gt;
&lt;td&gt;MaxPooling1D&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Strides=2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2$&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;15&lt;/td&gt;
&lt;td&gt;Flatten&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;16&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU, Dropout Rate=0.1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$1\times 216$&lt;/td&gt;
&lt;td&gt;13,888&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;17&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Activation = ReLU&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$1\times 17$&lt;/td&gt;
&lt;td&gt;1,105&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Total params&lt;/strong&gt;: 80,973&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Params size&lt;/strong&gt;: 316.3KB&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;:  93.5%&lt;/p&gt;
&lt;h3 id=&#34;alq-algorithm&#34;&gt;ALQ Algorithm&lt;/h3&gt;
&lt;p&gt;尽管所提出的网络架构深度极小，但由于权值的位宽较大，心律失常检测网络仍然存在内存和功耗问题。由于不同层的重要性不同，采用自适应位宽对不同层进行量化是合理的，可以显著降低网络的平均位宽。我们采用自适应一维损耗多比特网络（MBNs）量化方法来帮助我们实现对原来提出的网络的压缩。&lt;/p&gt;
&lt;p&gt;在模型压缩中，通常通过   pruning ,  quantization,  distillation 等方法来压缩预训练模型。在 ECGNet 的压缩中，我们利用了量化压缩。我们通过将深度神经网络的 weights 和 activations 量化为 multi-bit networks (MBNs) 来实现模型的压缩。&lt;/p&gt;
&lt;p&gt;与将误差最小化的量化方法重建全精度权值不同，一维ALQ使损失函数的量化误差最小化。在此过程中，并不涉及梯度近似。在我们训练全精度ECGNet后，量化过程就可以开始了。为了提高压缩速度，引入了并行计算。对于向量化的权值 $w∈R^{N×1}$，将 $w$ 划分为 $m$ 个不相交的组。每组权重用 $w_k∈R^{n×1}$ 表示，其中 $N = n × m$，在二进制基础上可以表示量化的权重。&lt;/p&gt;
&lt;p&gt;$$\omega_k=\sum_{i=1}^{I_k}\alpha_i \beta_i=B_k\alpha_k,\quad \beta_i\in{-1,1}^{n\times 1}$$&lt;/p&gt;
&lt;p&gt;我们用 $I_k$ 来表示第 k 组的位宽，于是我们可以定义平均位宽为：&lt;/p&gt;
&lt;p&gt;$$\bar{I}=\frac{1}{m}\sum_{k=1}^m I_k$$&lt;/p&gt;
&lt;p&gt;我们的优化方案如下：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/NY4k58hUmXreZdf&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/NY4k58hUmXreZdf.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Step 1 Full precision ECGNet&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这一步就是在本文第一部分中，对ECGNet进行全精度的训练。得到的数据需要进行保存，会用在后面两步之中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2 Pruning in $\alpha$ Domain&lt;/strong&gt;
在这一步中， 我们通过修剪最不重要的（w.r.t the loss） $\alpha$ 逐步减少Layer的平均位宽 $\bar{I}$  。   在这一步中， 我们只设置了一些元素 $\alpha$  为零。这一步会导致模型准确率的下降，但是我们会在Step 2中重新提高回来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3: Optimizing Binary Bases $B_g$ and Coordinates $\alpha _g$&lt;/strong&gt;
在这一步中， 我们重新训练剩余的   binary bases 和 coordinates 来恢复因为Step 1中减少位宽引起的准确率的下降。在这个步骤中，我们首先固定 coordinates 来对搜索binary bases 进行最优搜索，然后再固定 binary bases 对coordinates进行搜索。经过优化之后，准确率重新提高了。&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;ALQ量化压缩后七个卷积层和两个全连接层的平均位宽结果如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Layer&lt;/th&gt;
&lt;th&gt;Average Bitwidth&lt;/th&gt;
&lt;th&gt;Params&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_1&lt;/td&gt;
&lt;td&gt;1.2500&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;td&gt;170 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_2&lt;/td&gt;
&lt;td&gt;1.9896&lt;/td&gt;
&lt;td&gt;1,164&lt;/td&gt;
&lt;td&gt;2,316 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_3&lt;/td&gt;
&lt;td&gt;1.7005&lt;/td&gt;
&lt;td&gt;3,488&lt;/td&gt;
&lt;td&gt;5,921 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_4&lt;/td&gt;
&lt;td&gt;1.7095&lt;/td&gt;
&lt;td&gt;14,400&lt;/td&gt;
&lt;td&gt;24,617 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_5&lt;/td&gt;
&lt;td&gt;1.4133&lt;/td&gt;
&lt;td&gt;20,544&lt;/td&gt;
&lt;td&gt;29,035 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_6&lt;/td&gt;
&lt;td&gt;0.8545&lt;/td&gt;
&lt;td&gt;12,352&lt;/td&gt;
&lt;td&gt;10,555 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conv1d_7&lt;/td&gt;
&lt;td&gt;0.8550&lt;/td&gt;
&lt;td&gt;13,896&lt;/td&gt;
&lt;td&gt;11,881 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Linear_1&lt;/td&gt;
&lt;td&gt;1.7422&lt;/td&gt;
&lt;td&gt;13,888&lt;/td&gt;
&lt;td&gt;24,196 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Linear_2&lt;/td&gt;
&lt;td&gt;2.0000&lt;/td&gt;
&lt;td&gt;1,105&lt;/td&gt;
&lt;td&gt;2210 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ALL&lt;/td&gt;
&lt;td&gt;1.3696&lt;/td&gt;
&lt;td&gt;80973&lt;/td&gt;
&lt;td&gt;110901 bit= 13.538 KB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可以看到，316.3KB的模型经过压缩后变成了13.538KB，平均位宽为1.3696。压缩率为23.36x。&lt;/p&gt;
&lt;p&gt;准确率上，甚至还上升了，达到了95%，上升了1.5%。&lt;/p&gt;
&lt;p&gt;归一化后的混淆矩阵如下图：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/vUbhQkWmIeusolR&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/vUbhQkWmIeusolR.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
</description>
      
    </item>
    
    <item>
      <title>Quantization Compression</title>
      <link>https://preminstrel.github.io/blog/post/2021/10/17/quantization-compression/</link>
      <pubDate>Sun, 17 Oct 2021 10:48:26 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2021/10/17/quantization-compression/</guid>
      
      <description>&lt;p&gt;本文参考&lt;a href=&#34;https://jackwish.net/2019/neural-network-quantization-introduction-chn.html&#34;&gt;此博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;量化有若干相似的术语。&lt;strong&gt;低精度&lt;/strong&gt;（Low precision）可能是最通用的概念。常规精度一般使用 FP32（32位浮点，单精度）存储模型权重；低精度则表示 FP16（半精度浮点），INT8（8位的定点整数）等等数值格式。不过目前低精度往往指代 INT8。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;量化&lt;/strong&gt;一般指 INT8 。不过，根据存储一个权重元素所需的位数，还可以包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.02830&#34;&gt;二值神经网络&lt;/a&gt;：在运行时权重和激活只取两种值（例如 +1，-1）的神经网络，以及在训练时计算参数的梯度。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1605.04711&#34;&gt;三元权重网络&lt;/a&gt;：权重约束为+1,0和-1的神经网络。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.05279&#34;&gt;XNOR网络&lt;/a&gt;：过滤器和卷积层的输入是二进制的。 XNOR 网络主要使用二进制运算来近似卷积。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;工业界最终选择了 INT8 量化—— FP32 在推理（inference）期间被 INT8 取代，而训练（training）仍然是 FP32。&lt;a href=&#34;http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf&#34;&gt;TensorRT&lt;/a&gt;，&lt;a href=&#34;https://www.tensorflow.org/lite/performance/post_training_quantization&#34;&gt;TensorFlow&lt;/a&gt;，&lt;a href=&#34;https://github.com/pytorch/glow/blob/master/docs/Quantization.md&#34;&gt;PyTorch&lt;/a&gt;，&lt;a href=&#34;https://github.com/apache/incubator%20-mxnet/blob/master/python/mxnet/contrib/quantization.py&#34;&gt;MxNet&lt;/a&gt; 和许多其他深度学习软件都已启用（或正在启用）量化。&lt;/p&gt;
&lt;p&gt;通常，可以根据 FP32 和 INT8 的转换机制对解决方案进行分类。一些框架简单地引入了 &lt;code&gt;Quantize&lt;/code&gt; 和 &lt;code&gt;Dequantize&lt;/code&gt; 层，当从卷积或全链接层送入或取出时，它将 FP32 转换为 INT8 或相反。在这种情况下，如图的上半部分所示，模型本身和输入/输出采用 FP32 格式。深度学习框架加载模型，重写网络以插入&lt;code&gt;Quantize&lt;/code&gt; 和 &lt;code&gt;Dequantize&lt;/code&gt; 层，并将权重转换为 INT8 格式。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/Mfj8QXZxsqUSRna&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/Mfj8QXZxsqUSRna.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其他一些框架将网络整体转换为 INT8 格式，因此在推理期间没有格式转换，如图的下半部分。该方法要求算子（Operator）都支持量化，因为运算符之间的数据流是INT8。对于尚未支持的那些，它可能会回落到 Quantize/Dequantize 方案。下文的讨论都基于这种方式。&lt;/p&gt;
&lt;p&gt;由于 INT8 使用的比特数只有 FP32 的 25% ，在 INT8 和 FP32 之间转换数值的方法非常重要，因为它会显着影响预测精度。&lt;/p&gt;
&lt;h2 id=&#34;量化的算术&#34;&gt;量化的算术&lt;/h2&gt;
&lt;p&gt;量化过程可以分为两部分：&lt;strong&gt;将模型从 FP32 转换为 INT8，以及使用 INT8 进行推理&lt;/strong&gt;。本节说明这两部分背后的&lt;strong&gt;算术原理&lt;/strong&gt;。如果不了解基础算术原理，在考虑量化细节时通常会感到困惑。&lt;/p&gt;
&lt;h3 id=&#34;定点和浮点&#34;&gt;定点和浮点&lt;/h3&gt;
&lt;p&gt;从事计算机科学的人很少了解算术运算的执行方式。由于量化桥接了固定点（fixed point）和浮点（floating point），在接触相关研究和解决方案之前，有必要先了解它们的基础知识。&lt;/p&gt;
&lt;p&gt;定点和浮点都是数值的表示（representation），它们区别在于，将整数（integer）部分和小数（fractional）部分分开的点，点在哪里。 &lt;strong&gt;定点保留特定位数整数和小数，而浮点保留特定位数的有效数字（significand）和指数（exponent）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/Xli9bQO3GjZmzwI&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/Xli9bQO3GjZmzwI.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上图给出了定点和浮点表示的格式和示例。对于定点，II 表示整数，FF 表示 IIIII.FFFFF中的分数。对于浮点数，base分别为二进制、十进制和十六进制格式的 2、10 和 16 。定点和浮点的数值示例在图中是一一对应的。&lt;/p&gt;
&lt;p&gt;在指令集（Instruction Set Architecture）的内置数据类型中，&lt;strong&gt;定点是整数，浮点是二进制格式&lt;/strong&gt;。一般来说，指令集层面的定点是连续的，因为它是整数，且两个邻近的可表示数字的间隙是 1 。另一方面，浮点代表实数，其数值间隙由指数确定，因而具有非常宽的值域（32 位数值最大整数是 $2^{31}−1$，而浮点值域为 $(2−2^{-23})×2^{127}$），值越接近零就越准确。一个观察结果是，在给定指数时，浮点在不同范围内拥有数值数量相同数量，如下图。例如，$[1,2)$中浮点值的数量与 $[0.5,1)、[2,4]、[4,8]$等相同。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/8CBADo39zfO5FPR&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/8CBADo39zfO5FPR.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;浮点运算可以由整数运算组成，在计算机发展的早期，浮点计算都是用软件在定点硬件上模拟的。下面的等式展示了如何将浮点乘法用定点乘法和加法表示。加法的表示方法要复杂得多，这里不做进一步讨论，有需求的可以参考计算机体系结构相关资料。&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
z &amp;amp;=x \times y \\
z_{\text {significand }} \times b a s e^{z_{\text {exponent }}} &amp;amp;=\left(x_{\text {significand }} \times \text { base }^{x_{exponent }}\right) \times\left(y_{\text {significand }} \times \text { base }^{y_{{exponent }}}\right) \\
&amp;amp;=\left(x_{\text {significand }} \times y_{\text {significand }}\right) \times\left(\text { base }^{x_{exponent }} \times \text { base }^{y_{exponent}}\right) \\
&amp;amp;=\left(x_{\text {significand }} \times y_{\text {significand }}\right) \times \text { base }^{x_{exponent}+y_{exponent }}
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;实际上，在上面有效数字的整数乘法之后，当乘法结果相对于表示范围太大时，通常需要&lt;strong&gt;重新缩放&lt;/strong&gt;，如下图。&lt;strong&gt;重新缩放移动将有效数字结果的一部分转移到指数&lt;/strong&gt;，并以最近舍入方法舍入剩余的有效数字。图的右半部分是一个例子。&lt;strong&gt;由于部分数字被舍弃，浮点乘法会丢失一些信息&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;量化浮点&#34;&gt;量化浮点&lt;/h3&gt;
&lt;p&gt;神经网络由浮点运算构成。如&lt;a href=&#34;https://jackwish.net/2019/%EF%BC%83fixed-point-and-floating-point&#34;&gt;定点与浮点&lt;/a&gt;所述，FP32 和 INT8 的值域是$[(2−2^{-23})×2^{127},(2^{23}-2)×2^{127}]$ 和 $[−128,127]$，而取值数量大约分别为 $2^{32}$ 和 $2^8$ 。因此，&lt;strong&gt;将网络从 FP32 转换为 INT8 并不像数据类型转换截断那样简单&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;幸运的是，&lt;strong&gt;神经网络权重的值分布范围很窄，非常接近零&lt;/strong&gt;。下图给出了 MobileNetV1 中十层（拥有最多值的层）的权重分布。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/ECAOkrn1HR7TBLa&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/ECAOkrn1HR7TBLa.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当值落在 $(−1,1)$ 中，量化浮点使用类似$x_{float}=x_{scale}\times x_{quantized}$ 的方法将 FP32 映射到 INT8，其中 $x_{float}$ 表示 FP32 权重，$x_{quantized}$ 表示量化的 INT8 权重，$x_{scale}$ 是映射因子（缩放因子）。有时我们不希望将 FP32 零映射到 INT8 零，即&lt;a href=&#34;https://en.wikipedia.org/wiki/Digital_signal_processing&#34;&gt;数字信号处理&lt;/a&gt;中的&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantization_(signal_processing)&#34;&gt;均一量化&lt;/a&gt;和如下等式 。&lt;/p&gt;
&lt;p&gt;$$x_{float}=x_{scale}\times (x_{quantized}−x_{zero\_point})$$&lt;/p&gt;
&lt;p&gt;大多数情况下量化选用无符号整数，那么 INT8 值域为 $[0,255]$ 。zero_point 在这种情况下更有意义。具体而言，如下面的 等式所示，量化浮点值可以分为两个步骤&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过在权重张量（Tensor）中找到 &lt;em&gt;min&lt;/em&gt; 和 &lt;em&gt;max&lt;/em&gt; 值从而确定 $x_{scale}$ 和$x_{zero_point}$。&lt;/li&gt;
&lt;li&gt;将权重张量的每个值从 FP32 转换为 INT8 。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
x_{\text {float }}  \in\left[x_{\text {float }}^{\min }, x_{\text {float }}^{\max }\right]
$$
$$
x_{\text {scale }} =\frac{x_{\text {float }}^{\max }-x_{\text {float }}^{\text {min }}}{x_{\text {quantized }}^{\max }-x_{\text {quantized }}^{\min }}
$$&lt;/p&gt;
&lt;p&gt;$$
x_{\text {zeropoint }} =x_{\text {quantized }}^{\max }-x_{\text {float }}^{\max } \div x_{\text {scale }}
$$&lt;/p&gt;
&lt;p&gt;$$
x_{\text {quantized }} =x_{\text {float }} \div x_{\text {scale }}+x_{\text {zeropoint }}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意，当浮点运算结果不等于整数时，需要额外的舍入步骤&lt;/strong&gt;。例如将 FP32 值域 $[−1,1]$ 映射到 INT8 值域 $[0,255]$，有 $x_{scale}=\frac{2}{255}$，而$x_{zero_point}=255−\frac{255}{2}\approx 127$。&lt;/p&gt;
&lt;p&gt;量化过程中存在误差是不可避免的，就像数字信号处理中量化一样。下图显示了数字信号处理的&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantization_(signal_processing)#Quantization_error_models&#34;&gt;量化和误差&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sm.ms/image/8CBADo39zfO5FPR&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/8CBADo39zfO5FPR.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;量化并不是什么新知识，我们在对图像做预处理时就用到了量化。回想一下，我们通常会将一张 uint8 类型、数值范围在 0~255 的图片归一成 float32 类型、数值范围在 0.0~1.0 的张量，这个过程就是反量化。类似地，我们经常将网络输出的范围在 0.0~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是量化。所以量化本质上只是对数值范围的重新调整，可以「粗略」理解为是一种线性映射。(之所以加「粗略」二字，是因为有些论文会用非线性量化，但目前在工业界落地的还都是线性量化，所以本文只讨论线性量化的方案)。&lt;/p&gt;
&lt;p&gt;不过，可以明显看出，反量化一般没有信息损失，而量化一般都会有精度损失。这也非常好理解，float32 能保存的数值范围本身就比 uint8 多，因此必定有大量数值无法用 uint8 表示，只能四舍五入成 uint8 型的数值。量化模型和全精度模型的误差也来自四舍五入的 clip 操作。&lt;/p&gt;
&lt;p&gt;我们用 $r$  表示&lt;strong&gt;浮点实数&lt;/strong&gt;，$q$  表示&lt;strong&gt;量化后的定点整数&lt;/strong&gt;。&lt;strong&gt;浮点和整型之间的换算公式&lt;/strong&gt;为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$r=S(q-Z)$$
$$q=round(\frac{r}{S}+Z)$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中，$S$ 是 &lt;em&gt;scale&lt;/em&gt;，表示&lt;strong&gt;实数和整数之间的比例关系&lt;/strong&gt;，$Z$  是 &lt;em&gt;zero point&lt;/em&gt;，表示&lt;strong&gt;实数中的 0 经过量化后对应的整数&lt;/strong&gt;，它们的计算方法为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$$S=\frac{r_{max}-r_{min}}{q_{max}-q_{min}}$$
$$Z = round(q_{max}-\frac{r_{max}}{S})$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$r_{max}, r_{min}$ 分别是 $r$ 的最大值和最小值；$q_{min}, q_{max}$ 同理。需要强调的一点是，定点整数的 zero point 就代表浮点实数的 0，二者之间的换算不存在精度损失。 这么做的目的是为了在 padding 时保证浮点数值的 0 和定点整数的 zero point 完全等价，保证定点和浮点之间的表征能够一致。&lt;/p&gt;
&lt;h3 id=&#34;矩阵运算的量化&#34;&gt;矩阵运算的量化&lt;/h3&gt;
&lt;p&gt;由于卷积网络中的卷积层和全连接层本质上都是一堆矩阵乘法，因此我们先看如何将&lt;strong&gt;浮点运算上的矩阵转换为定点运算&lt;/strong&gt;。
假设 $r_1,r_2$ 是浮点实数上的两个 $N\times N$ 的矩阵，$r_3$ 是 $r_1,r_2$ 相乘后的矩阵
$$r_3^{i,k}=\sum_{j=1}^N r_1^{i,j}r_2^{j,k}$$
假设  $S_1,Z_1$ 是 $r_1$ 矩阵对应的 scale 和 zero point，$S_2,Z_2,S_3,Z_3$ 同理，那么可以推出：
$$S_3(q_3^{i,k}-Z_3)=\sum_{j=1}^NS_1(q_1^{i,k}-Z_1)S_2(q_2^{i,k}-Z_2)$$
整理得到 $$q_3^{i,k}=\frac{S_1S_2}{S_3}\sum_{j=1}^N (q^{i,j}_1-Z_1)(q^{i,j}_2-Z_2)+Z_3$$&lt;/p&gt;
&lt;p&gt;除了 $\frac{S_1S_2}{S_3}$ ，其他都是定点整数运算。那如何把  $\frac{S_1S_2}{S_3}$ 也变成定点运算呢？这里要用到一个 &lt;em&gt;trick&lt;/em&gt;。假设 $M=\frac{S_1S_2}{S_3}$  ，由于 $M$ 通常都是 $(0, 1)$之间的实数 (这是通过大量实验统计出来的)，因此可以表示成 $M=2^{-n}M_0$，其中 $M_0$  是一个定点实数。注意，定点数并不一定是整数，所谓定点，指的是小数点的位置是固定的，即小数位数是固定的。因此，如果存在 $M=2^{-n}M_0$，那我们就可以通过 $M_0$ 的 bit 位移操作实现 $M=2^{-n}M_0$ ，这样整个过程就都在定点上计算了。&lt;/p&gt;
&lt;p&gt;很多刚接触量化的同学对这一点比较疑惑，下面我就用一个简单的示例说明这一点。我们把 $M=\frac{S_1S_2}{S_3}$  代入可以得到：$$q_3^{i,j}=M\sum_{j=1}^N (q_1^{i,j}-Z_1)(q_2^{i,j}-Z_2)+Z_3=MP+Z_3$$&lt;/p&gt;
&lt;p&gt;这里面 $P$  是一个在&lt;strong&gt;定点域上计算好的整数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设 $P = 7091,M = 0.0072474273418460$  ($M$ 可以通过 $S$ 事先计算得到)，那下面我们就是要找到一个 $M_0$ 和 $n$ ，使得 $M P =2^{-n}M_0 P$ 成立。我们可以用一段代码来找到这两个数：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;M &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0.0072474273418460&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;P &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;7091&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;multiply&lt;/span&gt;(n, M, P):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    result &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; M &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; P
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Mo &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;int&lt;/span&gt;(&lt;span style=&#34;color:#a2f&#34;&gt;round&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;**&lt;/span&gt; n &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; M)) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# 这里不一定要四舍五入截断，因为python定点数不好表示才这样处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    approx_result &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; (Mo &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; P) &lt;span style=&#34;color:#666&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; n
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a2f&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;n=&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;, Mo=&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;, approx=&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;%f&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;, error=&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;%f&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;%&lt;/span&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          (n, Mo, approx_result, result&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;approx_result))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;for&lt;/span&gt; n &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;range&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;16&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    multiply(n, M, P)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到，在 $n=11、M_0=15$ 的时候，误差就已经在 1 以内了。因此，只要 $M_0P$ 的数值范围在 21(32-11) 个 bit 内，就可以通过对 $M_0P$ 右移 $n$ 个 bit 来近似 $MP$ 了，而这个误差本身在可以接受的范围内。这样一来，就可以完全通过定点运算来计算，即我们实现了&lt;strong&gt;浮点矩阵乘法的量化&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;卷积网络的量化&#34;&gt;卷积网络的量化&lt;/h3&gt;
&lt;p&gt;有了上面矩阵乘法的量化，我们就可以进一步尝试对卷积网络的量化。假设一个这样的网络：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://smms.app/image/JBluaykQ8vRwc6x&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2023/03/23/JBluaykQ8vRwc6x.png&#34; &gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个网络只有三个模块，现在需要把 &lt;em&gt;conv、fc、relu&lt;/em&gt; 量化。&lt;/p&gt;
&lt;p&gt;假设输入为 $x$，可以事先统计样本的最大值和最小值，然后计算出 $S_x$ (scale) 和 $Z_x$ (zero point)。
同样地，假设 &lt;em&gt;conv、fc&lt;/em&gt; 的参数为 $w_1, w_2$，以及 scale 和 zero point 为 $S_{w1},Z_{w1},S_{w2},Z_{w2}$ 。中间层的 feature map 为 $a_1,a_2$ ，事先统计出它们的 scale 和 zero point 为 $S_{a1},Z_{a1},S_{s2},Z_{a2}$。&lt;/p&gt;
&lt;p&gt;卷积运算和全连接层的本质都是矩阵运算，因此我们可以把卷积运算表示成 (这里先忽略加 bias 的操作，这一步同样可以量化，不过中间有一些 trick，我们在之后的文章再仔细研究)：
$$a_1^{i,k}=\sum_{j=1}^N x^{i,j}w_1^{i,j}$$
$$q_{a1}^{i,k}=M\sum_{j=1}^N (q_x^{i,j}-Z_x)(q_{w1}^{i,j}-Z_{w1})+Z_{a1}$$
其中 $M=\dfrac{S_{w1}S_{x}}{S_{a1}}$。得到 conv 的输出后，不用反量化回  $a_1$ ，直接用  $q_{a1}$ 继续后面的计算即可。&lt;/p&gt;
&lt;p&gt;对于量化的 RuLU 来说，计算公式不再是 $q_{a2}=\max(q_{a1},0)$，而是 $q_{a2}=\max(q_{a1},Z_{a1})$，并且 $S_{a1}=S_{a2},Z_{a1}=Z_{a2}$。在部署的时候，ReLU 或者 BN 通常是会整合到 conv 中一起计算的。&lt;/p&gt;
&lt;p&gt;得到 $q_{a2}$ 之后，我们可以计算 fc 层。假设网络输出为 $y$ ，对应的 scale 和zero_point 为 $S_y,Z_y$ ，则量化之后的  fc 层可以用如下公式来计算：
$$
q_{y}^{i, k}=M \sum_{j=1}^{N}\left(q_{a 2}^{i, j}-Z_{a 2}\right)\left(q_{w 2}^{j, k}-Z_{w 2}\right)+Z_{y}
$$&lt;/p&gt;
&lt;p&gt;然后通过公式 $y=S_y(q_y-Z_y)$ 把结果反量化回去，就可以得到近似原来全精度模型的输出了。&lt;/p&gt;
&lt;p&gt;可以看到，上面整个流程都是用定点运算实现的。我们在得到全精度的模型后，可以事先统计出 weight 以及中间各个 feature map 的 min、max，并以此计算出 scale 和 zero point，然后把 weight 量化成 int8/int16 型的整数后，整个网络便完成了量化，然后就可以依据上面的流程做量化推理了。&lt;/p&gt;
&lt;h2 id=&#34;model-size&#34;&gt;Model Size&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;网络的大小取决于Parameter的数量，如果是float32，就是$\text{Parameters}\times 4$&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
