<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GAN on Blog de Preminstrel</title>
    <link>https://preminstrel.github.io/blog/tags/gan/</link>
    <description>Recent content in GAN on Blog de Preminstrel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>preminstrel@gmail.com (Hanshi Sun)</managingEditor>
    <webMaster>preminstrel@gmail.com (Hanshi Sun)</webMaster>
    <lastBuildDate>Sun, 09 Jan 2022 13:50:22 +0800</lastBuildDate><atom:link href="https://preminstrel.github.io/blog/tags/gan/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CycleGAN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/09/cyclegan/</link>
      <pubDate>Sun, 09 Jan 2022 13:50:22 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/09/cyclegan/</guid>
      
      <description>&lt;p&gt;在科研过程中，需要用到 CycleGAN 对眼底图象进行 CFP 和 FFA 的 translation。CycleGAN 的功能，通俗来讲就是在数据集 unpaired 的情况下风格迁移，原论文的链接如下。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10593&#34;&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://preminstrel.github.io/blog/img/20220109140203.png&#34; width=&#34;700px&#34;/&gt;
&lt;/div&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;图像到图像得转换是一类视觉图形问题，其目标是使用一组 pair 好的的图像对来学习输入图像和输出图像之间的映射。但是，对于一些难以获取配对训练数据的情况下，以往的方法将不可用。Zhu et al. 提出了一种在数据没有进行配对的情况下利用 CycleGAN 将图像从源域 $X$ 转换到目标域 $Y$ 的方法。&lt;/p&gt;
&lt;p&gt;其目标是：对于映射 $G: X \rightarrow Y$，使得 $G(X)$  和 $Y$ 使用对抗性损失(&lt;em&gt;&lt;strong&gt;adversarial loss&lt;/strong&gt;&lt;/em&gt;)进行衡量无法区分。因为这个映射是高度欠约束的，所以应该将它与逆映射 $F: Y \rightarrow X$ 结合起来，并引入循环一致性损失(&lt;em&gt;&lt;strong&gt;cycle consistency loss&lt;/strong&gt;&lt;/em&gt;)来推动 $F(G(X)) \approx X$（反之亦然）。&lt;/p&gt;
&lt;h1 id=&#34;theory&#34;&gt;Theory&lt;/h1&gt;
&lt;p&gt;想要做到这点，有两个比较重要的点，第一个就是双判别器。如图所示，两个分布 $X,Y$，生成器$G,F$ 分别是 $X$ 到 $Y$ 和 $Y$ 到 $X$ 的映射，两个判别器 $D_X,D_Y$ 可以对转换后的图片进行判别。&lt;/p&gt;
&lt;p&gt;第二个点就是 cycle consistency loss，用数据集中其他的图来检验生成器，这是防止 $G$ 和 $F$ 过拟合，比如想把一个小狗照片转化成梵高风格，如果没有 cycle consistency loss，生成器可能会生成一张梵高真实画作来骗过 $Dx$，而无视输入的小狗。&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://preminstrel.github.io/blog/img/20220109203103.png&#34; width=&#34;700px&#34;/&gt;
&lt;/div&gt;
&lt;p&gt;我们的目标是学习两个域 $X$ 和 $Y$ 给定的训练样本之间的映射函数。我们的模型包括两个映射$G: X\rightarrow Y$ 和 $F: Y\rightarrow X$. 此外，我们引入了两个对抗性鉴别器 $D_X$ 和 $D_Y$，其中 $D_X$ 旨在区分图像 $\{X\}$ 和翻译图像 $\{F(y)\} $；同样，$D_Y$ 的目的是区分 $\{y\}$ 和 $\{G(x)\}$。需要进行优化的目标包括两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Adversarial Loss&lt;/strong&gt;&lt;/em&gt;，用于将生成图像的分布与目标域中的数据分布相匹配；对于映射函数 $G: X\rightarrow Y$ 和其对应的 $D_Y$，我们要求优化 $\min_G\max_{D_Y}L_\text{GAN}(G,D_Y,X,Y)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$L_\text{GAN}(G,D_Y,X, Y) = \mathbb{E}_{y\sim p_\text{data}(y)}\left[\log D_Y(y)\right] + \mathbb{E}_{x\sim p_\text{data}(x)}\left[\log(1 −D_Y(G(x))\right]$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Cycle Consistency Loss&lt;/strong&gt;&lt;/em&gt;，以防止学习到的映射 $G$ 和 $F$ 相互矛盾。需要满足 $$x\rightarrow G(x)\rightarrow F(G(x))\approx x,\qquad y\rightarrow F(y)\rightarrow G(F(y))\approx y$$我们使用 Cycle Consistency Loss 来激励这种行为：
$$
\begin{aligned}
L_{\text {cyc }}(G, F) =\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[||F(G(x))-x||_{1}\right]+\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[||G(F(y))-y||_{1}\right]
\end{aligned}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由 Cycle Consistency Loss 引起的行为可以在图中观察到：重构图像 $F(G(x))$ 最终与输入图像 $x$ 紧密匹配。理论上，对抗训练可以学习映射输出 $G$ 和 $F$，它们分别作为目标域 $Y$ 和 $X$ 产生相同的分布。然而，具有足够大的容量，网络可以将相同的输入图像集合映射到目标域中的任何图像的随机排列。因此，单独的对抗性 loss 不能保证可以映射单个输入。需要另外来一个 loss，保证 $G$ 和 $F$ 不仅能满足各自的判别器，还能应用于其他图片。也就是说，$G$ 和 $F$ 可能合伙偷懒骗人，给 $G$ 一个图，$G$ 偷偷把小狗变成梵高自画像，$F$ 再把梵高自画像变成输入。Cycle Consistency loss 的到来制止了这种投机取巧的行为，他用梵高其他的画作测试 $FG$，用另外真实照片测试 $GF$，看看能否变回到原来的样子，这样保证了 $GF$ 在整个 $X，Y$ 分布区间的普适性。&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://preminstrel.github.io/blog/img/20220109213709.png&#34; width=&#34;400px&#34;/&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Full Objective&lt;/strong&gt;&lt;/em&gt;
$$
\begin{aligned}
L\left(G, F, D_{X}, D_{Y}\right) =L_{\mathrm{GAN}}\left(G, D_{Y}, X, Y\right)
+L_{\mathrm{GAN}}\left(F, D_{X}, Y, X\right)
+\lambda L_{\mathrm{cyc}}(G, F)
\end{aligned}
$$
其中 $\lambda$ 是控制两种目标的相对重要性的，我们要对下式进行优化：
$$
G^{*}, F^{*}=\arg \min _{G, F} \max _{D_{X}, D_{Y}} L\left(G, F, D_{X}, D_{Y}\right)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CycleGAN 可以看作是训练两个“自动编码器”：我们学习一个自动编码器 $F\circ G:X→ X$ 联合另一个 $G\circ F:Y\rightarrow Y$。 然而，这些自动编码器都有特殊的内部结构：它们通过中间表示将图像映射到自身，中间表示是将图像转换到另一个域中。这种设置也可以被视为“&lt;strong&gt;对抗性自动编码器(&lt;em&gt;adversarial autoencoders&lt;/em&gt;)&lt;/strong&gt;” 的特例，它使用对抗性丢失来训练自动编码器的瓶颈层，以匹配任意目标分布。&lt;/p&gt;
&lt;h1 id=&#34;implementation&#34;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;论文的作者给出了 &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34;&gt;Pytorch&lt;/a&gt; 的和 &lt;a href=&#34;https://github.com/junyanz/CycleGAN&#34;&gt;Torch&lt;/a&gt; 的具体实现步骤，在 GitHub 上可以找到很丰富的资源和 docs。需要注意的是，这个网络训练起来挺吃 GPU 的，用一张 Tesla P100-PCIE-16GB 来训练的话，取 trainA 和 trainB 均为1096 张 256*256 的照片，跑一个 epoch 要 5.5min，跑 200 个 epoch 大概要 18.3 个小时。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/37198143&#34;&gt;CycleGAN原理以及代码全解析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Introduction to GAN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/06/introduction-to-gan/</link>
      <pubDate>Thu, 06 Jan 2022 23:12:33 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/06/introduction-to-gan/</guid>
      
      <description>&lt;p&gt;最近科研要用到 CycleGAN，借此机会，便对 &lt;em&gt;&lt;strong&gt;GAN(Generative Adversarial Networks)&lt;/strong&gt;&lt;/em&gt; 进行一个学习。Ian Goodfellow 大牛的 Generative Adversarial Networks（arxiv：&lt;a href=&#34;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1406.2661&#34;&gt;https://arxiv.org/abs/1406.2661&lt;/a&gt;），是这个领域的开天辟地之作。就像 Yann LeCun 所说的，Adversarial training is the coolest thing since sliced bread.&lt;/p&gt;
&lt;h1 id=&#34;basic-theory&#34;&gt;Basic Theory&lt;/h1&gt;
&lt;p&gt;这里用生成图片为例对GAN的原理进行说明。假设我们有两个网络，&lt;em&gt;&lt;strong&gt;G(Generator)&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;D(Discriminator)&lt;/strong&gt;&lt;/em&gt;，它们的功能分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G$ 是一个生成图片的网络，它接收一个随机的噪声 $z$，通过这个噪声生成图片，记做 $G(z)$。其输出是一个 vector。&lt;/li&gt;
&lt;li&gt;$D$ 是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是 $x$，$x$ 代表一张图片，输出 $D(x)$ 代表 $x$ 为真实图片的概率，如果为 1，就代表 100% 是真实的图片，而输出为 0，就代表不可能是真实的图片。其输出是一个 scalar。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在训练过程中，&lt;strong&gt;生成网络 $G$ 的目标就是尽量生成真实的图片去欺骗判别网络 $D$。而 $D$ 的目标就是尽量把 $G$ 生成的图片和真实的图片分别开来。&lt;/strong&gt; 这样，$G$ 和 $D$ 构成了一个动态的“博弈过程”。最后博弈的结果是：&lt;strong&gt;在最理想的状态下，&lt;/strong&gt; $G$ 可以生成足以“以假乱真”的图片 $G(z)$。对于 $D$ 来说，它难以判定 $G$ 生成的图片究竟是不是真实的，因此 $D(G(z)) = 0.5$。这样，我们就成功地得到了一个生成式的模型 $G$，它可以用来生成图片。&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://preminstrel.github.io/blog/img/20220109125533.png&#34; width=&#34;500px&#34;/&gt;
&lt;/div&gt;
&lt;h1 id=&#34;formula&#34;&gt;Formula&lt;/h1&gt;
&lt;p&gt;以上只是大致说了一下 GAN 的核心原理，如何用数学语言描述呢？这里直接摘录论文里的公式：&lt;/p&gt;
&lt;p&gt;$$\min_{G} \max_{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]$$&lt;/p&gt;
&lt;p&gt;公式中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;整个式子由两项构成。$x$ 表示真实图片，$z$  表示输入 $G$ 网络的噪声，而 $G(z)$ 表示 $G$ 网络生成的图片。&lt;/li&gt;
&lt;li&gt;$D(x)$ 表示 $D$ 网络判断&lt;strong&gt;真实图片是否真实&lt;/strong&gt;的概率（因为 $x$ 就是真实的，所以对于 $D$ 来说，这个值越接近 1 越好）。而 $D(G(z))$ 是 &lt;strong&gt;$D$ 网络判断 $G$ 生成的图片的是否真实的概率。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$G$ 的目的：上面提到过，$D(G(z))$ 是&lt;strong&gt;$D$ 网络判断 $G$ 生成的图片是否真实的概率&lt;/strong&gt;，$G$ 应该希望自己生成的图片“越接近真实越好”。也就是说，$G$ 希望 $D(G(z))$ 尽可能得大，这时 $V(D, G)$ 会变小。因此我们看到式子的最前面的记号是 $\min_G$。&lt;/li&gt;
&lt;li&gt;$D$ 的目的：$D$ 的能力越强，$D(x)$ 应该越大，$D(G(x))$ 应该越小。这时 $V(D,G)$ 会变大。因此式子对于 $D$ 来说是求最大 $\max_D$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面这幅图片很好地描述了这个过程：&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://preminstrel.github.io/blog/img/20220106234555.png&#34; width=&#34;400px&#34;/&gt;
&lt;/div&gt;
&lt;h1 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Step 1&lt;/strong&gt;&lt;/em&gt;: Fix generator $G$, and update discriminator $D$. Discriminator learns to assign high scores to real objects and low scores to generated objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample $m$ examples $\{ x^1, x^2, &amp;hellip; , x^m \}$ from database&lt;/li&gt;
&lt;li&gt;Sample $m$ noise samples $\{z^1, z^2, &amp;hellip; , z^m\}$ from a distribution&lt;/li&gt;
&lt;li&gt;Obtaining generated data $\{\tilde{x}^1,\tilde{x}^2,&amp;hellip;,\tilde{x}^m\},\tilde{x}^i=G(z^i)$&lt;/li&gt;
&lt;li&gt;Update discriminator parameters $\theta_d$ to maximize $$\tilde{V}=\frac{1}{m}\sum_{i=1}^m\log D(x^i)+\frac{1}{m}\sum_{i=1}^m \log \left(1-D(\tilde{x}^i)\right)$$$$\theta_d\leftarrow \theta_d+\eta\nabla\tilde{V}(\theta_d)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Step 2&lt;/strong&gt;&lt;/em&gt;: Fix discriminator $D$, and update generator $G$. Generator learns to “fool” the discriminator&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample m noise samples $\{z^1, z^2, &amp;hellip; , z^m\}$ from a distribution&lt;/li&gt;
&lt;li&gt;Update generator parameters $\theta_g$ to maximize $$\tilde{V}=\frac{1}{m}\sum_{i=1}^m\log \left(D(G(z^i))\right)$$$$\theta_g\leftarrow \theta_g-\eta\nabla\tilde{V}(\theta_g)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24767059&#34;&gt;GAN学习指南：从原理入门到制作生成Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
