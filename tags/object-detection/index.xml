<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Object Detection on Blog de Preminstrel</title>
    <link>https://preminstrel.github.io/blog/tags/object-detection/</link>
    <description>Recent content in Object Detection on Blog de Preminstrel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>preminstrel@gmail.com (Hanshi Sun)</managingEditor>
    <webMaster>preminstrel@gmail.com (Hanshi Sun)</webMaster>
    <lastBuildDate>Fri, 14 Jan 2022 13:32:15 +0800</lastBuildDate><atom:link href="https://preminstrel.github.io/blog/tags/object-detection/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Faster R-CNN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/14/faster-r-cnn/</link>
      <pubDate>Fri, 14 Jan 2022 13:32:15 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/14/faster-r-cnn/</guid>
      
      <description>&lt;p&gt;Faster R-CNN 可以简单看成是&lt;strong&gt;区域生成网络&lt;/strong&gt; + Fast R-CNN 的模型，用区域生成网络(&lt;em&gt;&lt;strong&gt;Region Proposal Network, RPN&lt;/strong&gt;&lt;/em&gt;)来替代 Fast R-CNN 中的选择性搜索方法，结构如下：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/iEOGhpnroZqN19w&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/iEOGhpnroZqN19w.png&#34; width=&#34;400px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先向 CNN 网络(VGG-16)输入图片，Faster R-CNN 使用一组基础的 conv + relu + pooling 层提取 feature map。&lt;strong&gt;该 feature map 被共享用于后续 RPN 层和 fc 层。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RPN 网络用于生成 region proposals，Faster R-CNN 中称之为 &lt;em&gt;&lt;strong&gt;anchors&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;通过 softmax 判断 anchors 属于 foreground 或者 background&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;再利用 bounding box regression 修正 anchors 获得精确的 proposals，输出其 Top-N(默认 300)的区域给 Rol pooling&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成 anchors $\rightarrow$ softmax 分类器提取 fg anchors $\rightarrow$ bbox reg 回归 fg anchors $\rightarrow$ Proposal Layer 生成proposals&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后续就是 Fast R-CNN 操作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rpn&#34;&gt;RPN&lt;/h3&gt;
&lt;p&gt;RPN 网络的主要作用是得到比较准确的候选区域，整个过程分为两步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用 $n\times n$ (默认 $3\times 3$) 的大小窗口去扫描特征图，每个滑窗位置映射倒一个低维的向量(默认 256 维)，并为每个滑窗位置考虑 $k$ 种(在论文设计中 $k=9$)&lt;strong&gt;可能的参考窗口(论文中称为 anchors)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/6ropzGCEjRAw1Fc&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/6ropzGCEjRAw1Fc.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3 id=&#34;anchors&#34;&gt;Anchors&lt;/h3&gt;
&lt;p&gt;$3\times 3$ 卷积核的中心点对应原图上的位置，将该点作为 anchor 的中心点，在原图中框出多尺度、多种长宽比的 anchors，三种尺度 $\{128,256,512\}$，三种长宽比 $\{1:1,1:2,2:1\}$，每个特征图中的像素点有 9 个框。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;窗口输出 $[N，256]\rightarrow$ 分类：判断是否是背景&lt;/li&gt;
&lt;li&gt;回归位置：N 个候选框与自己对应目标值 GT 做回归，修正位置。得到更好的候选区域提供给 ROl pooling 使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;Faster R-CNN 的训练分为两部分，即两个网络的训练。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RPN 训练&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;从众多的候选区域中提取出 score 较高的，并且经过 regression 调整的候选区域&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast R-CNN部分的训练&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Fast R-CNN &lt;em&gt;&lt;strong&gt;classification(over classes, softmax)&lt;/strong&gt;&lt;/em&gt;︰所有类别分类 N+1，得到候选区域每个类别概率&lt;/li&gt;
&lt;li&gt;Fast R-CNN &lt;em&gt;&lt;strong&gt;regression(bbox regression, MAE)&lt;/strong&gt;&lt;/em&gt;：得到更好的位置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;样本准备：正负 anchors 样本比例为 $1:3$&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/VJW9yaSCjHXeu1E&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/VJW9yaSCjHXeu1E.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;metrics&lt;/th&gt;
&lt;th&gt;R-CNN&lt;/th&gt;
&lt;th&gt;Fast R-CNN&lt;/th&gt;
&lt;th&gt;Faster R-CNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Test time/image&lt;/td&gt;
&lt;td&gt;50.0s&lt;/td&gt;
&lt;td&gt;2.0s&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.2s&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mAP(VOC2007)&lt;/td&gt;
&lt;td&gt;66.0&lt;/td&gt;
&lt;td&gt;66.9&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;66.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;RPN&lt;/li&gt;
&lt;li&gt;End-to-End&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;训练参数太大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;改进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RPN(Region Proposal Networks) 改进对于小目标选择利用多尺度特征信息进行 RPN&lt;/li&gt;
&lt;li&gt;速度提升，如 YOLO 系列算法，删去RPN，直接对 proposal 进行分类回归，极大的提升了网络的速度&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Fast R-CNN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/13/fast-r-cnn/</link>
      <pubDate>Thu, 13 Jan 2022 22:31:08 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/13/fast-r-cnn/</guid>
      
      <description>&lt;p&gt;SPP-net 的性能已经得到很大的改善，但是由于网络之间不统一训练，造成很大的麻烦，所以 &lt;em&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt;&lt;/em&gt; 就是为了解决这样的问题。其改进的之处为：提出一个 &lt;em&gt;&lt;strong&gt;Rol pooling&lt;/strong&gt;&lt;/em&gt;，然后整合整个模型，把 CNN、Rol pooling、分类器、bbox 回归几个模块&lt;strong&gt;整个一起训练&lt;/strong&gt;。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/AWih7QImN5c9zYC&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/AWih7QImN5c9zYC.png&#34; widt=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先将整个图片输入到一个基础卷积网络，得到整张图的 feature map&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将选择性搜索算法的结果 region proposal (Rol）映射到 feature map 中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rol pooling layer 提取一个固定长度的特征向量，每个特征会输入到一系列全连接层，得到一个 Rol 特征向量 &lt;strong&gt;(此步骤是对每一个候选区域都会进行同样的操作)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中一个是传统 &lt;em&gt;&lt;strong&gt;softmax&lt;/strong&gt;&lt;/em&gt; 层进行分类，输出类别有 K 个类别加上”背景”类&lt;/li&gt;
&lt;li&gt;另一个是 &lt;em&gt;&lt;strong&gt;bounding box regressor&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rol-pooling&#34;&gt;Rol Pooling&lt;/h3&gt;
&lt;p&gt;首先 Rol pooling 只是一个简单版本的 SSP，目的是为了减少计算时间并得到固定长度的向量。Rol 池化层使用最大池化将任何有效的 Rol 区域内的特征转换为具有 $H\times W$ 的固定空间范围的小 feature map，其中 $H,W$ 是超参数，它们独立于任何特定的 Rol。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;single scale&lt;/strong&gt;&lt;/em&gt;：直接将 image 定为某种 scale，直接输入网络来训练即可。（Fast R-CNN）&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;multi scale&lt;/strong&gt;&lt;/em&gt;：生成一个金字塔，即 SPP-net&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后者比前者更加准确，没有突出很多，但是第一种节省了很多的时间，所以 Fast R-CNN 要比 SPP-net 快很多。&lt;/p&gt;
&lt;h3 id=&#34;end-to-end-model&#34;&gt;End-to-End Model&lt;/h3&gt;
&lt;p&gt;输出端可以直接进行完整的反向传播，整体优化目标函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征提取 CNN 和训练 SVM 分类器的训练在时间上是先后顺序，二者训练方式独立，因此 SVMs 的训练 Loss 无法更新之前的卷积层参数，于是去掉 SVM，便可以形成 End-to-End 模型。&lt;/li&gt;
&lt;li&gt;使用了 softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/SOFVNrZXLalG7fm&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/SOFVNrZXLalG7fm.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3 id=&#34;multi-task-loss&#34;&gt;Multi-task Loss&lt;/h3&gt;
&lt;p&gt;两个 loss，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于分类 loss，是一个 N+1 路的 softmax 输出，其中 N 是类别个数，1 是背景，使用&lt;strong&gt;交叉熵损失&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对于回归 loss，是一个 4N 路输出的 regressor，也就是说对于每个类别都会训练一个单独的 regressor 的意思，&lt;strong&gt;使用平均绝对误差(MAE)损失，即 L1 损失&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;fine-tuning 训练中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调整 CNN + Rol pooling + softmax&lt;/li&gt;
&lt;li&gt;调整 bbox regressor 回归当中的参数&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;metrics&lt;/th&gt;
&lt;th&gt;R-CNN&lt;/th&gt;
&lt;th&gt;SPP-net&lt;/th&gt;
&lt;th&gt;Fast R-CNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;training time(h)&lt;/td&gt;
&lt;td&gt;84&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;9.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;test time/picture(s)&lt;/td&gt;
&lt;td&gt;47.0&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mAP&lt;/td&gt;
&lt;td&gt;66.0&lt;/td&gt;
&lt;td&gt;63.1&lt;/td&gt;
&lt;td&gt;66.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;drawbacks&#34;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;使用 Selective Search 提取 Region Proposals，没有实现真正意义山东个端对端，操作十分耗时间。&lt;/li&gt;
&lt;li&gt;一个更高效的方法来求出候选框—— &lt;em&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1504.08083&#34;&gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>SPP-net</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/13/sppnet/</link>
      <pubDate>Thu, 13 Jan 2022 21:56:21 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/13/sppnet/</guid>
      
      <description>&lt;p&gt;SPP-net 的强大功能在目标检测中也很重要。使用 &lt;em&gt;&lt;strong&gt;SPP-net(Spatial Pyramid Pooling-net)&lt;/strong&gt;&lt;/em&gt;，我们只计算整个图像的特征图一次，然后将任意区域(子图像)中的特征池化以生成用于训练检测器的固定长度表示。这种方法&lt;strong&gt;避免了重复计算卷积特征&lt;/strong&gt;。在处理测试图像时，此方法比 R-CNN 方法快，同时在 Pascal VOC 2007 上实现了更好或相当的准确度。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/NfZ6v2S1tTEgmC5&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/NfZ6v2S1tTEgmC5.png&#34; &gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;r-cnn-vs-spp-net&#34;&gt;R-CNN v.s. SPP-net&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;R-CNN&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;R-CNN 是让每个候选区域经过 crop/wrap 等操作变换成固定大小的图像&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固定大小的图像塞给 CNN 传给后面的层做训练回归分类操作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;SPP-net&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SPP-net 把全图塞给 CNN 得到全图的 feature map&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;让 SS 得到&lt;strong&gt;候选区域直接映射特征向量中对应位置&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;映射过来的特征向量，&lt;strong&gt;经过 SPP 层(空间金字塔变换层)，S 输出固定大小的特征向量给 FC 层&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/Ngk5SU1qdteRwbT&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/Ngk5SU1qdteRwbT.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SPP-net 在 R-CNN 的基础上提出了改进，通过候选区域和 feature map 的映射，配合 SPP 层的使用，从而达到了CNN 层的共享计算，减少了运算时间，后面的 &lt;em&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt;&lt;/em&gt; 等也是受 SPPNet 的启发&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;drawbacks&#34;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;训练依然过慢、效率低，特征需要写入磁盘(因为 SVM 的存在)&lt;/li&gt;
&lt;li&gt;分阶段训练网络︰选取候选区域、训练 CNN、训练 SVM、训练 bbox 回归器，SPP-net 反向传播效率低&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.4729&#34;&gt;Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>R-CNN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/13/r-cnn/</link>
      <pubDate>Thu, 13 Jan 2022 12:28:23 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/13/r-cnn/</guid>
      
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;R-CNN(Regions with CNN features)&lt;/strong&gt;&lt;/em&gt;，是 R-CNN 系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将 DL 和传统的 CV 的知识相结合。比如 R-CNN pipeline 中的第二步和第四步其实就属于传统的 CV 技术。使用 selective search 提取 region proposals，使用 SVM 实现分类。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/UrKWeidxpXHAPZV&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/UrKWeidxpXHAPZV.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;p&gt;CNN 具有良好的特征提取和分类性能，采用 RegionProposal  方法实现目标检测问题。算法可以分为三步：&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;
graph LR
  id1(候选区域选择)---&gt;id2(CNN特征提取)---&gt;id3(分类与边界回归);
&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;利用 Selective Search 找出图片中可能存在目标的侯选区域(默认 2000 个)&lt;/li&gt;
&lt;li&gt;将候选区域调整为了适应 AlexNet 网络的输入图像的大小 $227\times 227$，通过 CNN 对候选区域提取特征向量，2000 个建议框的 CNN 特征组合成网络 AlexNet 最终输出：$2000\times 4096$ 维矩阵&lt;/li&gt;
&lt;li&gt;将 $2000\times 4096$ 维特征经过 SVM 分类器(20 种分类，SVM 是二分类器，则有 20 个 SVM)，获得 $2000\times 20$ 种类别矩阵&lt;/li&gt;
&lt;li&gt;分别对 $2000\times 20$ 维矩阵中进行&lt;strong&gt;非极大值抑制(NMS)剔除重叠建议框&lt;/strong&gt;，得到与目标物体最高的一些建议框&lt;/li&gt;
&lt;li&gt;修正 bbox，对 bbox 做&lt;strong&gt;回归&lt;/strong&gt;微调&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;选择一个 pre-trained 神经网络(AlexNet、VGG)&lt;/li&gt;
&lt;li&gt;使用需要检测的目标重新训练(re-train)最后全连接层(connected layer)&lt;/li&gt;
&lt;li&gt;提取 proposals 并计算 CNN 特征。利用选择性搜索(Selective Search)算法提取所有 proposals(大约 2000 幅 images)，调整(resize/warp)它们成固定大小，以满足 CNN 输入要求(因为全连接层的限制)，然后将 feature map 保存到本地磁盘&lt;/li&gt;
&lt;li&gt;利用 feature map 训练 SVM 来对目标和背景进行分类(每个类一个二进制 SVM)&lt;/li&gt;
&lt;li&gt;边界框回归(Bounding boxes Regression)：训练将输出一些校正因子的线性回归分类器&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在 Pascal VOC 2012 的数据集上，能够将目标检测的验证指标 mAP 提升到 53.3%,这相对于之前最好的结果提升了整整 30%.&lt;/li&gt;
&lt;li&gt;这篇论文证明了可以讲神经网络应用在自底向上的候选区域，这样就可以进行目标分类和目标定位。&lt;/li&gt;
&lt;li&gt;这篇论文也带来了一个观点，那就是当你缺乏大量的标注数据时，比较好的可行的手段是，进行神经网络的迁移学习，采用在其他大型数据集训练过后的神经网络，然后在小规模特定的数据集中进行 fine-tune 微调。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;drawbacks&#34;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;重复计算，每个 region proposal，都需要经过一个 AlexNet 特征提取，为所有的 RoI(region of interest) 提取特征大约花费 47 秒，占用空间&lt;/li&gt;
&lt;li&gt;selective search 方法生成 region proposal，对一帧图像，需要花费 2 秒&lt;/li&gt;
&lt;li&gt;三个模块(提取、分类、回归)是分别训练的，并且在训练时候，对于存储空间消耗较大&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1311.2524&#34;&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Introduction to Object Detection</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/12/introduction-to-object-detection/</link>
      <pubDate>Wed, 12 Jan 2022 22:06:06 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/12/introduction-to-object-detection/</guid>
      
      <description>&lt;p&gt;目标检测(&lt;em&gt;&lt;strong&gt;Object Detection&lt;/strong&gt;&lt;/em&gt;)的任务是找出图像中所有特定的目标，确定它们的类别和位置。由于各类物体有不同的外观和形状，加上成像时光照、遮挡等因素的干扰，目标检测一直是 CV 领域内具有挑战性的问题。&lt;/p&gt;
&lt;p&gt;计算机视觉中关于图像识别有四大类任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Classification&lt;/strong&gt;&lt;/em&gt;：给定一张图片或一段视频判断里面包含什么类别的目标。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Location&lt;/strong&gt;&lt;/em&gt;：定位出这个目标的的位置。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Detection&lt;/strong&gt;&lt;/em&gt;：定位出这个目标的位置并且知道目标物是什么。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;&lt;/em&gt;：分为实例的分割(&lt;em&gt;&lt;strong&gt;Instance-level&lt;/strong&gt;&lt;/em&gt;)和场景分割(&lt;em&gt;&lt;strong&gt;Scene-level&lt;/strong&gt;&lt;/em&gt;)，解决“每一个像素属于哪个目标物或场景”的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/UlCWgbAhcT6jIEd&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/UlCWgbAhcT6jIEd.png&#34; &gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;基于深度学习的目标检测算法主要分为两类：Two stage 和 One stage。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Tow Stage&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;先进行区域生成，该区域称之为 &lt;em&gt;&lt;strong&gt;region proposal&lt;/strong&gt;&lt;/em&gt; (简称 &lt;em&gt;&lt;strong&gt;RP&lt;/strong&gt;&lt;/em&gt;，一个有可能包含待检物体的预选框)，再通过卷积神经网络进行样本分类。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;mermaid&#34;&gt;
graph LR
  id1(特征提取)---&gt;id2(生成 RP)---&gt;id3(分类/定位回归);
&lt;/div&gt;
常见 tow stage 算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN 和 R-FCN等。
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;One Stage&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;不用 RP，直接在网络中提取特征来预测物体分类和位置。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;mermaid&#34;&gt;
graph LR
  id1(特征提取)---&gt;id3(分类/定位回归);
&lt;/div&gt;
常见的 one stage 算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD 和 RetinaNet 等。
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;目标检测分为两大系列—— &lt;em&gt;&lt;strong&gt;RCNN&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;YOLO&lt;/strong&gt;&lt;/em&gt;，RCNN 系列是基于&lt;strong&gt;区域检测&lt;/strong&gt;的代表性算法，YOLO 是基于&lt;strong&gt;区域提取&lt;/strong&gt;的代表性算法，另外还有著名的 &lt;em&gt;&lt;strong&gt;SSD&lt;/strong&gt;&lt;/em&gt; 是基于前两个系列的改进。&lt;/p&gt;
&lt;h3 id=&#34;bounding-boxes&#34;&gt;Bounding Boxes&lt;/h3&gt;
&lt;p&gt;很多目标检测技术都会涉及 &lt;strong&gt;候选框(&lt;em&gt;bounding boxes&lt;/em&gt;)&lt;/strong&gt; 的生成，物体候选框获取当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。目标识别与图像分割技术的发展进一步推动有效提取图像中信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Sliding Window&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用&lt;strong&gt;非极大值抑制&lt;/strong&gt;(&lt;em&gt;&lt;strong&gt;Non-Maximum Suppression, NMS&lt;/strong&gt;&lt;/em&gt;)的方法进行筛选。最终，经过 NMS 筛选后获得检测到的物体。滑窗法简单易于理解，但是&lt;strong&gt;不同窗口大小进行图像全局搜索导致效率低下，而且设计窗口大小时候还需要考虑物体的长宽比&lt;/strong&gt;。所以，对于&lt;strong&gt;实时性要求较高&lt;/strong&gt;的分类器，不推荐使用滑窗法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Selective Search(SS)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;滑窗法类似穷举进行图像子区域搜索，但是一般情况下图像中大部分子区域是没有物体的。学者们自然而然想到只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率。&lt;strong&gt;选择搜索方法是当下最为熟知的图像 bounding boxes 提取算法&lt;/strong&gt;，由 Koen E.A 于2011年提出。&lt;/p&gt;
&lt;p&gt;主要思想：图像中物体可能存在的区域应该是有某些&lt;strong&gt;相似性或者连续性&lt;/strong&gt;区域的。因此，选择搜索基于上面这一想法采用&lt;strong&gt;子区域合并&lt;/strong&gt;的方法进行提取 bounding boxes。首先，对输入图像进行分割算法产生许多小的子区域。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做 bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;step0&lt;/strong&gt;&lt;/em&gt;：生成区域集 $R$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;step1&lt;/strong&gt;&lt;/em&gt;：计算区域集 $R$ 里每个相邻区域的相似度 $S=\{s_1, s_2,…\}$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;step2&lt;/strong&gt;&lt;/em&gt;：找出相似度最高的两个区域，将其合并为新集，添加进 $R$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;step3&lt;/strong&gt;&lt;/em&gt;：从 $S$ 中移除所有与 &lt;em&gt;&lt;strong&gt;step2&lt;/strong&gt;&lt;/em&gt; 中有关的子集&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;step4&lt;/strong&gt;&lt;/em&gt;：计算新集与所有子集的相似度&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;step5&lt;/strong&gt;&lt;/em&gt;：跳至 &lt;em&gt;&lt;strong&gt;step2&lt;/strong&gt;&lt;/em&gt;，直至 $S$ 为空&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以把目标标签定义如下：&lt;/p&gt;
&lt;p&gt;$$
y=\left[
\begin{matrix}
\quad p_c\quad b_x\quad
b_y\quad
b_h\quad
b_w\quad
c_1\quad
c_2\quad
c_3\quad
\end{matrix}
\right]^{T}
$$&lt;/p&gt;
&lt;p&gt;它是一个向量，$p_c$ 表示是否含有对象，如果存在，则 $p_c=1$，如果是背景，则图片中没有要检测的对象，则 $p_c=0$。我们可这样理解 $p_c$，它表示被检测对象属于某一分类的概率，背景分类除外。如果检测到对象，就输出被检测对象的边界框参数 $b_x$、$b_y$、$b_h$ 和 $b_w$。最后，如果存在某对象，则让 $p_c=1$，同时输出属于某个类别的概率 $c_1$，$c_2$，$c_3$。&lt;/p&gt;
&lt;h3 id=&#34;iou&#34;&gt;IOU&lt;/h3&gt;
&lt;p&gt;使用 &lt;em&gt;&lt;strong&gt;IoU(Intersection over Union)&lt;/strong&gt;&lt;/em&gt; 来判断模型的好坏。
交并比函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域，而交集就是比较小的区域，那么交并比就是交集的大小，再除以并集面积。
$$\text{IoU}=\frac{\text{size of intersection}}{\text{size of union}}$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般约定，如果 $\text{IoU}\ge 0.5$，我们就可以说检测正确。但是 0.5 并不严格，如果想更加严格一些，可以把阀值从 0.5 改为 0.6 或者 0.7。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;nms&#34;&gt;NMS&lt;/h3&gt;
&lt;p&gt;预测结果中，可能多个预测结果间存在重叠部分，需要保留交并比最大的、去掉非最大的预测结果，这就是非极大值抑制 &lt;em&gt;&lt;strong&gt;(Non-Maximum Suppression,NMS)&lt;/strong&gt;&lt;/em&gt;。当有许多框框重合都表示他们中间存在检测结果时，我们必须要选择一个最好的。简单来说，就是选局部最大值。步骤如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discard all boxes with $p_c\le 0.6$&lt;/li&gt;
&lt;li&gt;While there are any remaining boxes:
&lt;ul&gt;
&lt;li&gt;Pick the box with the largest $p_c$ output that as a prediction.&lt;/li&gt;
&lt;li&gt;Discard any remaining  box with $\text{IoU}\ge 0.5$ with the box output in the previous step.(为了不影响到另外的检测结果）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;以上是算法检测单个对象的情况，如果尝试同时检测三个对象， 比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;anchor-boxes&#34;&gt;Anchor Boxes&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，可以使用 &lt;em&gt;&lt;strong&gt;anchor box&lt;/strong&gt;&lt;/em&gt; 这个概念。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假设有这样一张图片，我们继续使用 $3\times 3$ 网格，注意行人的中点和汽车的中点几乎在同一个地方，两者都落入到同一个格子中。而 anchor box 的思路是，这样，预先定义两个不同形状的 anchor box，或者 anchor box 形状，要做的是把预测结果和这两个 anchor box 关联起来。一般来说，可能会用更多的 anchor box，可能要 5 个甚至更多。&lt;/p&gt;
&lt;p&gt;定义类别标签，用的向量不再是之前的：
$$
y=\left[
\begin{matrix}
\quad p_c\quad b_x\quad
b_y\quad
b_h\quad
b_w\quad
c_1\quad
c_2\quad
c_3\quad
\end{matrix}
\right]^{T}
$$
而是：
$$
y=\left[\quad
p_c\quad
b_x\quad
b_y\quad
b_h\quad
b_w\quad
c_1\quad
c_2\quad
c_3\quad
p_c\quad
b_x\quad
b_y\quad
b_h\quad
b_w\quad
c_1\quad
c_2\quad
c_3\quad
\right]^{T}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comparing with the previous one&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Previous&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each object in training image is assigned to grid cell that contains that object’s midpoint.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;With two anchor boxes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each object in training image is assigned to grid cell that contains that object’s midpoint and anchor box for the grid cell with highest IoU.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果有两个 anchor box，但在同一个格子中有三个对象，这种情况算法处理不好，希望这种情况不会发生，但如果真的发生了，这个算法并没有很好的处理办法，对于这种情况，就引入一些打破僵局的默认手段。还有这种情况，两个对象都分配到一个格子中，而且它们的 anchor box 形状也一样，这是算法处理不好的另一种情况，需要引入一些打破僵局的默认手段，专门处理这种情况，希望的数据集里不会出现这种情况，其实出现的情况不多，所以对性能的影响应该不会很大。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;也许设立 anchor box 的好处在于能让学习算法能够更有针对性，特别是如果数据集有一些很高很瘦的对象，比如说行人，还有像汽车这样很宽的对象，这样算法就能更有针对性的处理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后，应该怎么选择 anchor box 呢？人们一般手工指定 anchor box 形状，可以选 5 到 10 个 anchor box 形状，覆盖到多种不同的形状，可以涵盖想要检测的对象的各种形状。&lt;/p&gt;
&lt;p&gt;还有一个更高级的版本，就是所谓的 &lt;em&gt;&lt;strong&gt;K-means&lt;/strong&gt;&lt;/em&gt;，可以将两类对象形状聚类，如果用它来选择一组 anchor box，选择最具有代表性的一组 anchor box，可以代表试图检测的十几个对象类别，但这其实是自动选择 anchor box 的高级方法。如果就人工选择一些形状，合理的考虑到所有对象的形状，预计会检测的很高很瘦或者很宽很胖的对象。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/yegeli/article/details/109861867&#34;&gt;目标检测（Object Detection）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
