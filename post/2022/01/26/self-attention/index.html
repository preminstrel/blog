<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  
  <meta name="author" content="Hanshi Sun">

  
  
  <meta name="description" content="Attention is all you need. æœ€è¿‘åˆšæ¥è§¦åˆ° Transformerï¼Œæ„Ÿè§‰å…¶æ¨¡å‹æ¯” CNNs è¦å¤æ‚äº†ä¸å°‘ï¼Œçœ‹äº†ä¸€äº›è®ºæ–‡ä¹Ÿä»…ä»…æ˜¯è‰è‰çœ‹è¿‡ï¼Œä¸ç†è§£å…¶åŸç†ï¼Œåœ¨ç½‘ä¸Šè¯»äº†ä¸€äº› blog">
  

  
  <link rel="icon" href="https://preminstrel.github.io/blog/favicon.ico">

  
  
  <meta name="keywords" content=" hugo  latex  theme ">
  

  
  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css"
  integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
  integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js"
  integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '\\[', right: '\\]', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false }
      ],
      ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code', 'option'],
      throwOnError: false
    });
  });
</script>


  

  
  <meta property="og:title" content="Self-Attention" />
<meta property="og:description" content="Attention is all you need. æœ€è¿‘åˆšæ¥è§¦åˆ° Transformerï¼Œæ„Ÿè§‰å…¶æ¨¡å‹æ¯” CNNs è¦å¤æ‚äº†ä¸å°‘ï¼Œçœ‹äº†ä¸€äº›è®ºæ–‡ä¹Ÿä»…ä»…æ˜¯è‰è‰çœ‹è¿‡ï¼Œä¸ç†è§£å…¶åŸç†ï¼Œåœ¨ç½‘ä¸Šè¯»äº†ä¸€äº› blog" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://preminstrel.github.io/blog/post/2022/01/26/self-attention/" />
<meta property="article:published_time" content="2022-01-26T16:36:51+08:00" />
<meta property="article:modified_time" content="2022-01-26T00:00:00+00:00" />


  
  <link rel="canonical" href="https://preminstrel.github.io/blog/post/2022/01/26/self-attention/">

  
  
  <meta itemprop="name" content="Self-Attention">
<meta itemprop="description" content="Attention is all you need. æœ€è¿‘åˆšæ¥è§¦åˆ° Transformerï¼Œæ„Ÿè§‰å…¶æ¨¡å‹æ¯” CNNs è¦å¤æ‚äº†ä¸å°‘ï¼Œçœ‹äº†ä¸€äº›è®ºæ–‡ä¹Ÿä»…ä»…æ˜¯è‰è‰çœ‹è¿‡ï¼Œä¸ç†è§£å…¶åŸç†ï¼Œåœ¨ç½‘ä¸Šè¯»äº†ä¸€äº› blog">
<meta itemprop="datePublished" content="2022-01-26T16:36:51&#43;08:00" />
<meta itemprop="dateModified" content="2022-01-26T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="896">



<meta itemprop="keywords" content="Deep Learning,Transformer," />

  
  <link media="screen" rel="stylesheet" href='https://preminstrel.github.io/blog/css/common.css'>
  <link media="screen" rel="stylesheet" href='https://preminstrel.github.io/blog/css/content.css'>

  
  
  <title>Self-Attention - Blog de Preminstrel</title>
  

  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Self-Attention"/>
<meta name="twitter:description" content="Attention is all you need. æœ€è¿‘åˆšæ¥è§¦åˆ° Transformerï¼Œæ„Ÿè§‰å…¶æ¨¡å‹æ¯” CNNs è¦å¤æ‚äº†ä¸å°‘ï¼Œçœ‹äº†ä¸€äº›è®ºæ–‡ä¹Ÿä»…ä»…æ˜¯è‰è‰çœ‹è¿‡ï¼Œä¸ç†è§£å…¶åŸç†ï¼Œåœ¨ç½‘ä¸Šè¯»äº†ä¸€äº› blog"/>


  
<link rel="stylesheet" href='https://preminstrel.github.io/blog/css/single.css'>

</head>

<body>
  <div id="wrapper">
    <header id="header">
  <h1>
    <a href="https://preminstrel.github.io/blog/">Blog de Preminstrel</a>
  </h1>

  <nav>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/">Post</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/post/">Archives</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/about/">About</a>
    </span>
    
  </nav>
</header>

    
<main id="main" class="post">
  
  
  <h1>Self-Attention</h1>
  
  <div>
    <b>Keywords: </b>
    
    <a class="link" href='https://preminstrel.github.io/blog/tags/deep-learning'>#Deep Learning</a>
    
    <a class="link" href='https://preminstrel.github.io/blog/tags/transformer'>#Transformer</a>
    
  </div>
  
  
  <article class="content">
    
    <blockquote>
<p>Attention is all you need.</p>
</blockquote>
<p>æœ€è¿‘åˆšæ¥è§¦åˆ° Transformerï¼Œæ„Ÿè§‰å…¶æ¨¡å‹æ¯” CNNs è¦å¤æ‚äº†ä¸å°‘ï¼Œçœ‹äº†ä¸€äº›è®ºæ–‡ä¹Ÿä»…ä»…æ˜¯è‰è‰çœ‹è¿‡ï¼Œä¸ç†è§£å…¶åŸç†ï¼Œåœ¨ç½‘ä¸Šè¯»äº†ä¸€äº› blogï¼Œæœ¬æ¬¡æ¥è¿›è¡Œä¸€æ¬¡æ€»ç»“ã€‚é¦–å…ˆä¾¿æ˜¯ Self-Attention çš„å…¬å¼</p>
<p>$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{D_k}})V$$</p>
<h1 id="terminology">Terminology</h1>
<p>å…¬å¼ç§å‡ºç°çš„ $Q,K,V$ åˆ†åˆ«æ˜¯ Queryã€Keyã€Valueçš„ç¼©å†™ï¼Œæˆ‘ä»¬çš„è¡¨è¾¾å¼å¦‚ä¸‹ï¼š</p>
<p>$$X W^Q=Q$$
$$XW^K=K$$
$$XW^V=V$$</p>
<p>æ–‡ç« ä¸­æ‰€è°“çš„ $Q,K,V$ çŸ©é˜µæ¥æºäº $X$ ä¸çŸ©é˜µçš„ä¹˜ç§¯ï¼Œæœ¬è´¨ä¸Šæ˜¯ $X$ çš„ä¸€ç³»åˆ—çš„çº¿æ€§å˜æ¢ã€‚åšçº¿æ€§å˜æ¢æ˜¯ä¸ºäº†æå‡æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼ŒçŸ©é˜µ $W$ éƒ½æ˜¯å¯ä»¥è®­ç»ƒçš„ï¼Œèµ·åˆ°ä¸€ä¸ªç¼“å†²çš„æ•ˆæœã€‚</p>
<p>æˆ‘ä»¬å‡è®¾ $Q,K$ ç§å…ƒç´ çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼Œ$A^T=Q^TK$ çš„å‡å€¼ä¸º 0ï¼Œæ–¹ç¨‹ä¸º $D$ã€‚å½“ $D$ å˜å¾—å¾ˆå¤§æ—¶ï¼Œ$A$ ä¸­çš„å…ƒç´ çš„æ–¹å·®ä¹Ÿä¼šå˜å¾—å¾ˆå¤§ï¼Œå¦‚æœ $A$ ä¸­çš„å…ƒç´ æ–¹å·®å¾ˆå¤§ï¼Œé‚£ä¹ˆ $A$ çš„åˆ†å¸ƒä¼šè¶‹äºé™¡å³­(åˆ†å¸ƒçš„æ–¹å·®å¤§ï¼Œåˆ†å¸ƒé›†ä¸­åœ¨ç»å¯¹å€¼å¤§çš„åŒºåŸŸ)ã€‚æˆ‘ä»¬å¯ä»¥å°†åˆ†å¸ƒâ€œé™¡å³­â€ç¨‹åº¦ä¸ $D$ è§£è€¦ï¼Œä»è€Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦å€¼ä¿æŒç¨³å®šã€‚</p>
<p>$$A\leftarrow \dfrac{A}{\sqrt{D_k}}$$</p>
<h1 id="theory">Theory</h1>
<p>å…¬å¼ä¸­çš„ $QK^T$ è¡¨ç¤ºçš„æ˜¯ $Q,K$ çš„å†…ç§¯ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯ä¸€æ–¹åœ¨å¦ä¸€æ–¹çš„<strong class=chinese>æŠ•å½±</strong>ï¼Œå…¶å¤§å°ä¹Ÿå¯ä»¥è¡¨ç¤ºå…¶<strong class=chinese>ç›¸å…³æ€§</strong>ã€‚Softmax æ˜¯ä¸ºäº†å°†ä¸€ç³»åˆ—çš„å€¼<strong class=chinese>å½’ä¸€åŒ–</strong>è€Œå­˜åœ¨çš„ã€‚</p>
<p>$$\text{softmax}(z_k)=\frac{e^{z_k}}{\sum_{i=1}^Ie^{z_i}}$$</p>
<p>è€Œéšåä¸ $V$ çš„ä¹˜ç§¯ï¼Œä»£è¡¨çš„æ˜¯<strong class=chinese>å‘é‡ç»è¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ æƒæ±‚å’Œä¹‹åçš„ç»“æœ</strong>ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œsoftmax ç®¡çš„æ˜¯ä¸€ä¸ªç›¸å…³åº¦æƒå€¼å¤§å°ï¼Œä¸åé¢çš„ $V$ ç›¸ä¹˜ï¼Œå¾—åˆ°çš„æ˜¯é€šè¿‡ç›¸å…³åº¦æƒå€¼æ ‡å‡†è€Œé‡æ–°è®¡ç®—å¾—åˆ°çš„é‡ã€‚</p>
<p>å¯¹ Self-Attention æ¥è¯´ï¼Œå®ƒè·Ÿæ¯ä¸€ä¸ªè¾“å…¥çš„å‘é‡éƒ½åš Attentionï¼Œæ‰€ä»¥æ²¡æœ‰è€ƒè™‘åˆ°è¾“å…¥çš„é¡ºåºã€‚æ›´é€šä¿—æ¥è®²ï¼Œå¤§å®¶å¯ä»¥å‘ç°æˆ‘ä»¬å‰æ–‡çš„è®¡ç®—æ¯ä¸€ä¸ªè¯å‘é‡éƒ½ä¸å…¶ä»–è¯å‘é‡è®¡ç®—å†…ç§¯ï¼Œå¾—åˆ°çš„ç»“æœä¸¢å¤±äº†æˆ‘ä»¬åŸæ¥æ–‡æœ¬çš„é¡ºåºä¿¡æ¯ã€‚å¯¹æ¯”æ¥è¯´ï¼ŒLSTM æ˜¯å¯¹äºæ–‡æœ¬é¡ºåºä¿¡æ¯çš„è§£é‡Šæ˜¯è¾“å‡ºè¯å‘é‡çš„å…ˆåé¡ºåºï¼Œè€Œæˆ‘ä»¬ä¸Šæ–‡çš„è®¡ç®—å¯¹ sequence çš„é¡ºåºè¿™ä¸€éƒ¨åˆ†åˆ™å®Œå…¨æ²¡æœ‰æåŠï¼Œä½ æ‰“ä¹±è¯å‘é‡çš„é¡ºåºï¼Œå¾—åˆ°çš„ç»“æœä»ç„¶æ˜¯ç›¸åŒçš„ï¼Œæ­¤å¤„ä¾¿å¯ä»¥å¼•å‡º Transformer çš„ä½ç½®ç¼–ç éƒ¨åˆ†ã€‚<strong>Query ä¸ Key ä½œç”¨å¾—åˆ° Attention çš„æƒå€¼ï¼Œä¹‹åè¿™ä¸ªæƒå€¼ä½œç”¨åœ¨ Value ä¸Šå¾—åˆ° Attentionå€¼ã€‚</strong></p>
<h1 id="code">Code</h1>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic"># Attention æœºåˆ¶çš„å®ç°</span>
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">math</span> <span style="color:#a2f;font-weight:bold">import</span> sqrt
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch.nn</span>


<span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">Self_Attention</span>(nn<span style="color:#666">.</span>Module):
    <span style="color:#080;font-style:italic"># input : batch_size * seq_len * input_dim</span>
    <span style="color:#080;font-style:italic"># q : batch_size * input_dim * dim_k</span>
    <span style="color:#080;font-style:italic"># k : batch_size * input_dim * dim_k</span>
    <span style="color:#080;font-style:italic"># v : batch_size * input_dim * dim_v</span>
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self,input_dim,dim_k,dim_v):
        <span style="color:#a2f">super</span>(Self_Attention,self)<span style="color:#666">.</span>__init__()
        self<span style="color:#666">.</span>q <span style="color:#666">=</span> nn<span style="color:#666">.</span>Linear(input_dim,dim_k)
        self<span style="color:#666">.</span>k <span style="color:#666">=</span> nn<span style="color:#666">.</span>Linear(input_dim,dim_k)
        self<span style="color:#666">.</span>v <span style="color:#666">=</span> nn<span style="color:#666">.</span>Linear(input_dim,dim_v)
        self<span style="color:#666">.</span>_norm_fact <span style="color:#666">=</span> <span style="color:#666">1</span> <span style="color:#666">/</span> sqrt(dim_k)
        
    
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self,x):
        Q <span style="color:#666">=</span> self<span style="color:#666">.</span>q(x) <span style="color:#080;font-style:italic"># Q: batch_size * seq_len * dim_k</span>
        K <span style="color:#666">=</span> self<span style="color:#666">.</span>k(x) <span style="color:#080;font-style:italic"># K: batch_size * seq_len * dim_k</span>
        V <span style="color:#666">=</span> self<span style="color:#666">.</span>v(x) <span style="color:#080;font-style:italic"># V: batch_size * seq_len * dim_v</span>
         
        atten <span style="color:#666">=</span> nn<span style="color:#666">.</span>Softmax(dim<span style="color:#666">=-</span><span style="color:#666">1</span>)(torch<span style="color:#666">.</span>bmm(Q,K<span style="color:#666">.</span>permute(<span style="color:#666">0</span>,<span style="color:#666">2</span>,<span style="color:#666">1</span>))) <span style="color:#666">*</span> self<span style="color:#666">.</span>_norm_fact <span style="color:#080;font-style:italic"># Q * K.T() # batch_size * seq_len * seq_len</span>
        
        output <span style="color:#666">=</span> torch<span style="color:#666">.</span>bmm(atten,V) <span style="color:#080;font-style:italic"># Q * K.T() * V # batch_size * seq_len * dim_v</span>
        
        <span style="color:#a2f;font-weight:bold">return</span> output
</code></pre></div><h1 id="reference">Reference</h1>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/410776234">è¶…è¯¦ç»†å›¾è§£Self-Attention</a></li>
</ul>

    
  </article>
  <div class="paginator">
    
    <a class="link" href="https://preminstrel.github.io/blog/post/2022/01/17/mmdetection-head/">â† prev</a>
    
    
    <a class="link" href="https://preminstrel.github.io/blog/post/2022/01/26/transformer/">next â†’</a>
    
  </div>
  <div class="comment">
    
    
    
    
    
    
  </div>
  
</main>

    <footer id="footer">
  <div>
    <span>Â© 2021</span> - <span>2022</span>
  </div>

  <div>
    <span>Powered by </span>
    <a class="link" href="https://gohugo.io/">Hugo</a>
    <span> ğŸ¦ Theme </span>
    <a class="link" href="https://github.com/queensferryme/hugo-theme-texify">TeXify</a>
  </div>

  <div class="footnote">
    <span>Follow me on <a class=link href=https://github.com/preminstrel>GitHub</a>,
<a class=link href=https://twitter.com/preminstrel>Twitter</a> or
<a class=link href=/index.xml>RSS</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel=noopener>CC BY-NC-SA 4.0</a>
</span>
  </div>
</footer>

  </div>

  
  

  
  

  
  

</body>

</html>
