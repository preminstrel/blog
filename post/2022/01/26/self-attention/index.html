<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  
  <meta name="author" content="Hanshi Sun">

  
  
  <meta name="description" content="Attention is all you need. 最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog">
  

  
  <link rel="icon" href="https://preminstrel.github.io/blog/favicon.ico">

  
  
  <meta name="keywords" content=" blog  preminstrel ">
  

  
  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css"
  integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
  integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js"
  integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '\\[', right: '\\]', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false }
      ],
      ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code', 'option'],
      throwOnError: false
    });
  });
</script>


  

  
  <meta property="og:title" content="Self-Attention" />
<meta property="og:description" content="Attention is all you need. 最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://preminstrel.github.io/blog/post/2022/01/26/self-attention/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-01-26T16:36:51+08:00" />
<meta property="article:modified_time" content="2022-01-26T00:00:00+00:00" />


  
  <link rel="canonical" href="https://preminstrel.github.io/blog/post/2022/01/26/self-attention/">

  
  
  <meta itemprop="name" content="Self-Attention">
<meta itemprop="description" content="Attention is all you need. 最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog"><meta itemprop="datePublished" content="2022-01-26T16:36:51+08:00" />
<meta itemprop="dateModified" content="2022-01-26T00:00:00+00:00" />
<meta itemprop="wordCount" content="896">
<meta itemprop="keywords" content="Deep Learning,Transformer," />

  
  <link media="screen" rel="stylesheet" href='https://preminstrel.github.io/blog/css/common.css'>
  <link media="screen" rel="stylesheet" href='https://preminstrel.github.io/blog/css/content.css'>

  
  
  <title>Self-Attention - Blog de Preminstrel</title>
  

  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Self-Attention"/>
<meta name="twitter:description" content="Attention is all you need. 最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog"/>


  
<link rel="stylesheet" href='https://preminstrel.github.io/blog/css/single.css'>

</head>

<body>
  <div id="wrapper">
    <header id="header">
  <h1>
    <a href="https://preminstrel.github.io/blog/">Blog de Preminstrel</a>
  </h1>

  <nav>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/">Post</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/post/">Archives</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/categories/">Categories</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="https://preminstrel.github.io">About</a>
    </span>
    
  </nav>
</header>

    
<main id="main" class="post">
  
  
  <h1>Self-Attention</h1>
  
  <div>
    <center>
    2022-01-26 | [<a class="link" href='https://preminstrel.github.io/blog/categories/transformer'>Transformer</a>]
    </center>
  </div>

  <div>
    <b>Keywords: </b>
    
    <a class="link" href='https://preminstrel.github.io/blog/tags/deep-learning'>#Deep Learning</a>
    
    <a class="link" href='https://preminstrel.github.io/blog/tags/transformer'>#Transformer</a>
    
  </div>
  
  
  
  <details>
    <summary>
      <b>Table of Contents</b>
    </summary>
    <div class="toc"><nav id="TableOfContents">
  <ul>
    <li><a href="#terminology">Terminology</a></li>
    <li><a href="#theory">Theory</a></li>
    <li><a href="#code">Code</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
  </details>
  
  
  <article class="content">
    
    <blockquote>
<p>Attention is all you need.</p>
</blockquote>
<p>最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog，本次来进行一次总结。首先便是 Self-Attention 的公式</p>
<p>$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{D_k}})V$$</p>
<h2 id="terminology">Terminology</h2>
<p>公式种出现的 $Q,K,V$ 分别是 Query、Key、Value的缩写，我们的表达式如下：</p>
<p>$$X W^Q=Q$$
$$XW^K=K$$
$$XW^V=V$$</p>
<p>文章中所谓的 $Q,K,V$ 矩阵来源于 $X$ 与矩阵的乘积，本质上是 $X$ 的一系列的线性变换。做线性变换是为了提升模型的拟合能力，矩阵 $W$ 都是可以训练的，起到一个缓冲的效果。</p>
<p>我们假设 $Q,K$ 种元素的均值为 0，方差为 1，$A^T=Q^TK$ 的均值为 0，方程为 $D$。当 $D$ 变得很大时，$A$ 中的元素的方差也会变得很大，如果 $A$ 中的元素方差很大，那么 $A$ 的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。我们可以将分布“陡峭”程度与 $D$ 解耦，从而使得训练过程中梯度值保持稳定。</p>
<p>$$A\leftarrow \dfrac{A}{\sqrt{D_k}}$$</p>
<h2 id="theory">Theory</h2>
<p>公式中的 $QK^T$ 表示的是 $Q,K$ 的内积，也可以说是一方在另一方的<strong class=chinese>投影</strong>，其大小也可以表示其<strong class=chinese>相关性</strong>。Softmax 是为了将一系列的值<strong class=chinese>归一化</strong>而存在的。</p>
<p>$$\text{softmax}(z_k)=\frac{e^{z_k}}{\sum_{i=1}^Ie^{z_i}}$$</p>
<p>而随后与 $V$ 的乘积，代表的是<strong class=chinese>向量经过注意力机制加权求和之后的结果</strong>。也就是说，softmax 管的是一个相关度权值大小，与后面的 $V$ 相乘，得到的是通过相关度权值标准而重新计算得到的量。</p>
<p>对 Self-Attention 来说，它跟每一个输入的向量都做 Attention，所以没有考虑到输入的顺序。更通俗来讲，大家可以发现我们前文的计算每一个词向量都与其他词向量计算内积，得到的结果丢失了我们原来文本的顺序信息。对比来说，LSTM 是对于文本顺序信息的解释是输出词向量的先后顺序，而我们上文的计算对 sequence 的顺序这一部分则完全没有提及，你打乱词向量的顺序，得到的结果仍然是相同的，此处便可以引出 Transformer 的位置编码部分。<strong>Query 与 Key 作用得到 Attention 的权值，之后这个权值作用在 Value 上得到 Attention值。</strong></p>
<h2 id="code">Code</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Attention 机制的实现</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">math</span> <span style="color:#a2f;font-weight:bold">import</span> sqrt
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch.nn</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">Self_Attention</span>(nn<span style="color:#666">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#080;font-style:italic"># input : batch_size * seq_len * input_dim</span>
</span></span><span style="display:flex;"><span>    <span style="color:#080;font-style:italic"># q : batch_size * input_dim * dim_k</span>
</span></span><span style="display:flex;"><span>    <span style="color:#080;font-style:italic"># k : batch_size * input_dim * dim_k</span>
</span></span><span style="display:flex;"><span>    <span style="color:#080;font-style:italic"># v : batch_size * input_dim * dim_v</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">def</span> __init__(self,input_dim,dim_k,dim_v):
</span></span><span style="display:flex;"><span>        <span style="color:#a2f">super</span>(Self_Attention,self)<span style="color:#666">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#666">.</span>q <span style="color:#666">=</span> nn<span style="color:#666">.</span>Linear(input_dim,dim_k)
</span></span><span style="display:flex;"><span>        self<span style="color:#666">.</span>k <span style="color:#666">=</span> nn<span style="color:#666">.</span>Linear(input_dim,dim_k)
</span></span><span style="display:flex;"><span>        self<span style="color:#666">.</span>v <span style="color:#666">=</span> nn<span style="color:#666">.</span>Linear(input_dim,dim_v)
</span></span><span style="display:flex;"><span>        self<span style="color:#666">.</span>_norm_fact <span style="color:#666">=</span> <span style="color:#666">1</span> <span style="color:#666">/</span> sqrt(dim_k)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self,x):
</span></span><span style="display:flex;"><span>        Q <span style="color:#666">=</span> self<span style="color:#666">.</span>q(x) <span style="color:#080;font-style:italic"># Q: batch_size * seq_len * dim_k</span>
</span></span><span style="display:flex;"><span>        K <span style="color:#666">=</span> self<span style="color:#666">.</span>k(x) <span style="color:#080;font-style:italic"># K: batch_size * seq_len * dim_k</span>
</span></span><span style="display:flex;"><span>        V <span style="color:#666">=</span> self<span style="color:#666">.</span>v(x) <span style="color:#080;font-style:italic"># V: batch_size * seq_len * dim_v</span>
</span></span><span style="display:flex;"><span>         
</span></span><span style="display:flex;"><span>        atten <span style="color:#666">=</span> nn<span style="color:#666">.</span>Softmax(dim<span style="color:#666">=-</span><span style="color:#666">1</span>)(torch<span style="color:#666">.</span>bmm(Q,K<span style="color:#666">.</span>permute(<span style="color:#666">0</span>,<span style="color:#666">2</span>,<span style="color:#666">1</span>))) <span style="color:#666">*</span> self<span style="color:#666">.</span>_norm_fact <span style="color:#080;font-style:italic"># Q * K.T() # batch_size * seq_len * seq_len</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        output <span style="color:#666">=</span> torch<span style="color:#666">.</span>bmm(atten,V) <span style="color:#080;font-style:italic"># Q * K.T() * V # batch_size * seq_len * dim_v</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#a2f;font-weight:bold">return</span> output
</span></span></code></pre></div><h2 id="reference">Reference</h2>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/410776234">超详细图解Self-Attention</a></li>
</ul>

    
  </article>
  <div class="paginator">
    
    <a class="link" href="https://preminstrel.github.io/blog/post/2021/10/17/arrhythmia-classifier-using-cnn-with-alq/">← prev</a>
    
    
    <a class="link" href="https://preminstrel.github.io/blog/post/2022/01/26/transformer/">next →</a>
    
  </div>
  <div class="comment">
    
    
    
    
    
    
  </div>
  
</main>

    <footer id="footer">
  <div>
    <span>© 2021</span> - <span>2023</span>
  </div>
  <div class="footnote">
    <span>Follow me on <a class=link href=https://github.com/preminstrel>GitHub</a>,
<a class=link href=https://twitter.com/preminstrel>Twitter</a> or
<a class=link href=/blog/index.xml>RSS</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel=noopener>CC BY-NC-SA 4.0</a>
</span>
  </div>
</footer>

  </div>

  
  

  
  

  
  

</body>

</html>
