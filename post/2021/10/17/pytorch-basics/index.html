<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  
  <meta name="author" content="Hanshi Sun">

  
  
  <meta name="description" content="01. Overview DL &amp; ML 维度诅咒：维度越高，需要的训练集越大 深度学习和机器学习的区别：多了一层用来提取特征的层。传统的机器学习中（无标签），Feature">
  

  
  <link rel="icon" href="https://preminstrel.github.io/blog/favicon.ico">

  
  
  <meta name="keywords" content=" hugo  latex  theme ">
  

  
  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css"
  integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
  integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js"
  integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '\\[', right: '\\]', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false }
      ],
      ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code', 'option'],
      throwOnError: false
    });
  });
</script>


  

  
  <meta property="og:title" content="Pytorch Basics" />
<meta property="og:description" content="01. Overview DL &amp; ML 维度诅咒：维度越高，需要的训练集越大 深度学习和机器学习的区别：多了一层用来提取特征的层。传统的机器学习中（无标签），Feature" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://preminstrel.github.io/blog/post/2021/10/17/pytorch-basics/" />
<meta property="article:published_time" content="2021-10-17T00:25:41+08:00" />
<meta property="article:modified_time" content="2021-10-17T00:25:41+08:00" />


  
  <link rel="canonical" href="https://preminstrel.github.io/blog/post/2021/10/17/pytorch-basics/">

  
  
  <meta itemprop="name" content="Pytorch Basics">
<meta itemprop="description" content="01. Overview DL &amp; ML 维度诅咒：维度越高，需要的训练集越大 深度学习和机器学习的区别：多了一层用来提取特征的层。传统的机器学习中（无标签），Feature">
<meta itemprop="datePublished" content="2021-10-17T00:25:41&#43;08:00" />
<meta itemprop="dateModified" content="2021-10-17T00:25:41&#43;08:00" />
<meta itemprop="wordCount" content="2462">



<meta itemprop="keywords" content="Pytorch,Deep Learning," />

  
  <link media="screen" rel="stylesheet" href='https://preminstrel.github.io/blog/css/common.css'>
  <link media="screen" rel="stylesheet" href='https://preminstrel.github.io/blog/css/content.css'>

  
  
  <title>Pytorch Basics - Blog de Preminstrel</title>
  

  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Pytorch Basics"/>
<meta name="twitter:description" content="01. Overview DL &amp; ML 维度诅咒：维度越高，需要的训练集越大 深度学习和机器学习的区别：多了一层用来提取特征的层。传统的机器学习中（无标签），Feature"/>


  
<link rel="stylesheet" href='https://preminstrel.github.io/blog/css/single.css'>

</head>

<body>
  <div id="wrapper">
    <header id="header">
  <h1>
    <a href="https://preminstrel.github.io/blog/">Blog de Preminstrel</a>
  </h1>

  <nav>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/">Post</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/post/">Archives</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/blog/about/">About</a>
    </span>
    
  </nav>
</header>

    
<main id="main" class="post">
  
  
  <h1>Pytorch Basics</h1>
  
  <div>
    <b>Keywords: </b>
    
    <a class="link" href='https://preminstrel.github.io/blog/tags/pytorch'>#Pytorch</a>
    
    <a class="link" href='https://preminstrel.github.io/blog/tags/deep-learning'>#Deep Learning</a>
    
  </div>
  
  
  <article class="content">
    
    <h2 id="01-overview">01. Overview</h2>
<h3 id="dl--ml">DL &amp; ML</h3>
<p><img src="/img/20210913192607.png" alt=""></p>
<ul>
<li>维度诅咒：维度越高，需要的训练集越大</li>
<li>深度学习和机器学习的区别：多了一层用来提取特征的层。传统的机器学习中（无标签），Feature是单独做训练的，后面的Mapping from features和它是分开的。但是在深度学习中，这个是统一的。所以深度学习也称为 <strong>End to End</strong> 的学习方式。</li>
</ul>
<h3 id="route">Route</h3>
<p><img src="/img/20210913193312.png" alt=""></p>
<h3 id="gradient-descent">Gradient Descent</h3>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x_data <span style="color:#666">=</span> [<span style="color:#666">1</span>,<span style="color:#666">2</span>,<span style="color:#666">3</span>]
y_data <span style="color:#666">=</span> [<span style="color:#666">2</span>,<span style="color:#666">4</span>,<span style="color:#666">6</span>]
w<span style="color:#666">=</span><span style="color:#666">1</span>

<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(x):
    <span style="color:#a2f;font-weight:bold">return</span> x<span style="color:#666">*</span>w

<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">loss</span>(xs,ys):
    loss <span style="color:#666">=</span> <span style="color:#666">0</span>
    <span style="color:#a2f;font-weight:bold">for</span> x,y <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">zip</span>(xs,ys):
        y_pred <span style="color:#666">=</span> forward(x)
        loss <span style="color:#666">+=</span> (y_pred<span style="color:#666">-</span>y)<span style="color:#666">**</span><span style="color:#666">2</span>
    <span style="color:#a2f;font-weight:bold">return</span> loss<span style="color:#666">/</span><span style="color:#a2f">len</span>(xs)

<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">gradient</span>(xs,ys):
    grad<span style="color:#666">=</span><span style="color:#666">0</span>
    <span style="color:#a2f;font-weight:bold">for</span> x,y <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">zip</span>(xs,ys):
        grad <span style="color:#666">+=</span> <span style="color:#666">2</span><span style="color:#666">*</span>x<span style="color:#666">*</span>(x<span style="color:#666">*</span>w<span style="color:#666">-</span>y)
    <span style="color:#a2f;font-weight:bold">return</span> grad<span style="color:#666">/</span><span style="color:#a2f">len</span>(xs)

<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;Predict(before training)&#39;</span>,<span style="color:#666">4</span>, forward(<span style="color:#666">4</span>))
<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    loss_val <span style="color:#666">=</span> loss(x_data, y_data)
    grad_val <span style="color:#666">=</span> gradient(x_data, y_data)
    w <span style="color:#666">-=</span> <span style="color:#666">0.01</span><span style="color:#666">*</span>grad_val
    <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;Epoch:&#39;</span>, epoch, <span style="color:#b44">&#39;w=&#39;</span>,w,<span style="color:#b44">&#39;loss=&#39;</span>,loss_val)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;Predict(After training)&#39;</span>,<span style="color:#666">4</span>, forward(<span style="color:#666">4</span>))
</code></pre></div><p><img src="/img/20210913203403.png" alt=""></p>
<ul>
<li>随机梯度下降的计算无法并行，计算中是有依赖的</li>
<li>随机梯度下降的性能更好，但是无法并行，所以速度比较慢</li>
<li>所以我们用<strong>mini-batch</strong>，批量随机梯度下降，这是一种折中的方法</li>
</ul>
<h3 id="前向传播和反向传播">前向传播和反向传播</h3>
<p><img src="/img/20210913220248.png" alt=""></p>
<h2 id="02-linear-model">02. Linear Model</h2>
<h3 id="tensor--linear-model">Tensor &amp; Linear Model</h3>
<p>In PyTorch, Tensor is the important component in <strong>constructing dynamic
computational graph</strong>. It contains <strong>data and grad</strong>, which storage the value of node and gradient w.r.t loss respectively.
如下代码是在构造计算图，在Pytorch中我们需要这样看待它。</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
x_data <span style="color:#666">=</span> [<span style="color:#666">1</span>,<span style="color:#666">2</span>,<span style="color:#666">3</span>]
y_data <span style="color:#666">=</span> [<span style="color:#666">2</span>,<span style="color:#666">4</span>,<span style="color:#666">6</span>]
w <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([<span style="color:#666">1.0</span>])
w<span style="color:#666">.</span>requires_grad <span style="color:#666">=</span> True

<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(x):
    <span style="color:#a2f;font-weight:bold">return</span> x <span style="color:#666">*</span> w
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">loss</span>(x,y):
    y_pred <span style="color:#666">=</span> forward(x)
    <span style="color:#a2f;font-weight:bold">return</span> (y_pred<span style="color:#666">-</span>y) <span style="color:#666">**</span> <span style="color:#666">2</span>
</code></pre></div><p>训练过程</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f">sum</span> <span style="color:#666">=</span> <span style="color:#666">0</span>
<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    <span style="color:#a2f;font-weight:bold">for</span> x, y <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">zip</span>(x_data, y_data): <span style="color:#080;font-style:italic"># 抽取样本</span>
        l <span style="color:#666">=</span> loss(x, y)     <span style="color:#080;font-style:italic"># 前馈：计算loss，计算完释放</span>
        l<span style="color:#666">.</span>backward()                 <span style="color:#080;font-style:italic"># 反馈</span>
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b44">grad:&#39;</span>, x, y, w<span style="color:#666">.</span>grad<span style="color:#666">.</span>item())  <span style="color:#080;font-style:italic">#item是一个标量</span>
        w<span style="color:#666">.</span>data <span style="color:#666">=</span> w<span style="color:#666">.</span>data <span style="color:#666">-</span> <span style="color:#666">0.01</span> <span style="color:#666">*</span> w<span style="color:#666">.</span>grad<span style="color:#666">.</span>data  <span style="color:#080;font-style:italic"># 注意此时grad要取到data</span>
        <span style="color:#a2f">sum</span> <span style="color:#666">+=</span> l<span style="color:#666">.</span>item()              <span style="color:#080;font-style:italic"># 计算损失和</span>
        w<span style="color:#666">.</span>grad<span style="color:#666">.</span>data<span style="color:#666">.</span>zero_()             <span style="color:#080;font-style:italic"># 清零梯度</span>
        
    <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;progress:&#34;</span>, epoch, l<span style="color:#666">.</span>item())
</code></pre></div><p><strong>NOTICE</strong>:
The grad computed by <code>.backward()</code> will be accumulated. So after update, <strong>remember set the grad to ZERO!!</strong></p>
<ul>
<li><code>.data</code>返回的是一个Tensor，而<code>.item()</code>返回的是具体的数值（非Tensor数据类型）。</li>
</ul>
<h3 id="framework">Framework</h3>
<p>4个步骤：</p>
<p><strong>Prepare dataset</strong> : we shall talk about this later</p>
<p><strong>Design model using Class</strong>: inherit from nn.Module</p>
<p><strong>Construct loss and optimizer</strong>: using PyTorch API</p>
<p><strong>Training cycle</strong>: forward, backward, update</p>
<h4 id="prepare-dataset">Prepare dataset</h4>
<p>In PyTorch, the computational graph is in mini-batch fashion, so X and Y are $3 \times 1$ Tensors.</p>
<p>$$\hat{y}=w\cdot x+b$$
$$\left[\begin{aligned}&amp;y_{pred}^{(1)}\\ &amp;y_{pred}^{(2)} \\ &amp;y_{pred}^{(3)} \end{aligned}\right]=w\cdot \left[\begin{aligned}&amp;x^{(1)}\\ &amp;x^{(2)} \\ &amp;x^{(3)}\end{aligned}\right]+b$$</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
x_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([[<span style="color:#666">1.0</span>], [<span style="color:#666">2.0</span>], [<span style="color:#666">3.0</span>]])
y_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([[<span style="color:#666">2.0</span>], [<span style="color:#666">4.0</span>], [<span style="color:#666">6.0</span>]])
</code></pre></div><h4 id="design-model">Design Model</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">LinearModel</span>(torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Module):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self):     <span style="color:#080;font-style:italic"># 构造函数</span>
        <span style="color:#a2f">super</span>(LinearModel, self)<span style="color:#666">.</span>__init__()  <span style="color:#080;font-style:italic"># 调用父类的构造</span>
        self<span style="color:#666">.</span>linear <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">1</span>,<span style="color:#666">1</span>)     <span style="color:#080;font-style:italic"># 构造对象</span>
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self, x):    <span style="color:#080;font-style:italic"># 必须有forward传播函数的定义</span>
        y_pred <span style="color:#666">=</span> self<span style="color:#666">.</span>linear(x)
        <span style="color:#a2f;font-weight:bold">return</span> y_pred

model <span style="color:#666">=</span> LinearModel()
</code></pre></div><ul>
<li><code>nn.Linear</code> contain two member Tensors: <strong>weight</strong> and <strong>bias</strong></li>
</ul>
<blockquote>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">torch</span><span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(in_features, out_features, bias <span style="color:#666">=</span> True)
</code></pre></div><p>Applies a linear transformation $y=Ax+b$
Parameters:</p>
<ul>
<li>in_features: the size of each input sample</li>
<li>out_features: the size of each output sample</li>
<li>bias: Default: True</li>
</ul>
</blockquote>
<p>实际上，在运算过程中是<strong class=chinese>做转置的</strong>
$$y=x\cdot w+b$$
$$y = w^{T}\cdot x+b$$</p>
<h4 id="construct-loss-and-optimizer">Construct Loss and Optimizer</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">criterion <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>MSELoss(size_average<span style="color:#666">=</span>False)
optimizer <span style="color:#666">=</span> torch<span style="color:#666">.</span>optim<span style="color:#666">.</span>SGD(model<span style="color:#666">.</span>parameters(), lr<span style="color:#666">=</span><span style="color:#666">0.01</span>)
</code></pre></div><blockquote>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">torch</span><span style="color:#666">.</span>nn<span style="color:#666">.</span>MSELoss(size_average, <span style="color:#a2f">reduce</span> <span style="color:#666">=</span> True)
</code></pre></div><p>求均方根值 $l(x,y)=L={l_1,\cdots,l_N}^{T},\quad l_n=(x_n-y_n)^2$
其中，N是batch size。</p>
</blockquote>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">torch</span><span style="color:#666">.</span>optim<span style="color:#666">.</span>SGD(params, lr<span style="color:#666">=&lt;</span><span style="color:#a2f">object</span> <span style="color:#a2f">object</span><span style="color:#666">&gt;</span>, momentum <span style="color:#666">=</span> <span style="color:#666">0</span>, dampening <span style="color:#666">=</span> <span style="color:#666">0</span>, weight_decay <span style="color:#666">=</span> <span style="color:#666">0</span>, nesterov <span style="color:#666">=</span> False)
</code></pre></div><p>Implements stochastic gradient descent (optionally with momentum)</p>
<h4 id="training-cycle">Training Cycle</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    y_pred <span style="color:#666">=</span> model(x_data)
    loss <span style="color:#666">=</span> criterion(y_pred, y_data)
    <span style="color:#a2f;font-weight:bold">print</span>(epoch, loss)
    
    optimizer<span style="color:#666">.</span>zero_grad()  <span style="color:#080;font-style:italic"># 注意梯度清零</span>
    loss<span style="color:#666">.</span>backward()        <span style="color:#080;font-style:italic"># 反向传播</span>
    optimizer<span style="color:#666">.</span>step()       <span style="color:#080;font-style:italic">#Update</span>
</code></pre></div><h4 id="test-model">Test Model</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic"># Output weight and bias</span>
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;w=&#39;</span>, model<span style="color:#666">.</span>linear<span style="color:#666">.</span>weight<span style="color:#666">.</span>item())
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;b=&#39;</span>, model<span style="color:#666">.</span>linear<span style="color:#666">.</span>bias<span style="color:#666">.</span>item())

<span style="color:#080;font-style:italic"># Test Model</span>
x_test <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([<span style="color:#666">4.0</span>])
y_test <span style="color:#666">=</span> model(x_test)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;y_pred=&#39;</span>, y_test<span style="color:#666">.</span>data)
</code></pre></div><h2 id="03-logistic-regression">03. Logistic Regression</h2>
<h3 id="one-dimension">One Dimension</h3>
<p><img src="/img/20210914162303.png" alt="">
Logistic function can guarantee that the output is in $[0,1]$, and loss function is $$loss=-(y\log \hat{y}+(1-y)\log (1-\hat{y}))$$
此时误差计算的是分布的差异 <strong>(BCE)</strong>，而不是几何上的距离。</p>
<blockquote>
<p>Mini-Batch Loss Function for Binary Classification
$$loss=-\frac{1}{N}\sum^N_{n=1}(y_n\log \hat{y}_n+(1-y_n)\log (1-\hat{y}_n))$$</p>
</blockquote>
<p>流程和之前的差不多，代码如下：</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic">#---------------------Prepare dataset--------------------------#</span>
x_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([[<span style="color:#666">1.0</span>], [<span style="color:#666">2.0</span>], [<span style="color:#666">3.0</span>]])
y_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([[<span style="color:#666">0</span>], [<span style="color:#666">0</span>], [<span style="color:#666">1</span>]])

<span style="color:#080;font-style:italic">#-----------------Design model using Class---------------------#</span>
<span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">LogisticRegressionModel</span>(torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Module):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self):
        <span style="color:#a2f">super</span>(LogisticRegressionModel, self)<span style="color:#666">.</span>__init__()
        self<span style="color:#666">.</span>linear <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">1</span>, <span style="color:#666">1</span>)
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self, x):
        y_pred <span style="color:#666">=</span> F<span style="color:#666">.</span>sigmoid(self<span style="color:#666">.</span>linear(x))
        <span style="color:#a2f;font-weight:bold">return</span> y_pred
model <span style="color:#666">=</span> LogisticRegressionModel()

<span style="color:#080;font-style:italic">#----------------Construct loss and optimizer------------------#</span>
criterion <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>BCELoss(size_average<span style="color:#666">=</span>False)
optimizer <span style="color:#666">=</span> torch<span style="color:#666">.</span>optim<span style="color:#666">.</span>SGD(model<span style="color:#666">.</span>parameters(), lr<span style="color:#666">=</span><span style="color:#666">0.01</span>)

<span style="color:#080;font-style:italic">#----------------------Training cycle--------------------------#</span>
<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">1000</span>):
    y_pred <span style="color:#666">=</span> model(x_data)
    loss <span style="color:#666">=</span> criterion(y_pred, y_data)
    <span style="color:#a2f;font-weight:bold">print</span>(epoch, loss<span style="color:#666">.</span>item())
    optimizer<span style="color:#666">.</span>zero_grad()
    loss<span style="color:#666">.</span>backward()
    optimizer<span style="color:#666">.</span>step()
</code></pre></div><h3 id="multi-dimension">Multi Dimension</h3>
<p>二维数据中，一般来说列对应的是feature，行对应的是sample，一行为一条record</p>
<h4 id="mini-batchn-samples">Mini-Batch（N samples）</h4>
<p><img src="/img/20210914165040.png" alt=""></p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">Model</span>(torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Module):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self):
        <span style="color:#a2f">super</span>(Model, self)<span style="color:#666">.</span>__init__()
        self<span style="color:#666">.</span>linear <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">8</span>, <span style="color:#666">1</span>)
        self<span style="color:#666">.</span>sigmoid <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Sigmoid()
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self, x):
        x <span style="color:#666">=</span> self<span style="color:#666">.</span>sigmoid(self<span style="color:#666">.</span>linear(x))
        <span style="color:#a2f;font-weight:bold">return</span> x

model <span style="color:#666">=</span> Model()
</code></pre></div><h4 id="prepare-dataset-1">Prepare Dataset</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">numpy</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">np</span>
xy <span style="color:#666">=</span> np<span style="color:#666">.</span>loadtxt(<span style="color:#b44">&#39;diabetes.csv.gz&#39;</span>, delimiter<span style="color:#666">=</span><span style="color:#b44">&#39;,&#39;</span>, dtype<span style="color:#666">=</span>np<span style="color:#666">.</span>float32)
x_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>from_numpy(xy[:,:<span style="color:#666">-</span><span style="color:#666">1</span>]) <span style="color:#080;font-style:italic"># 除了最后一行都要</span>
y_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>from_numpy(xy[:, [<span style="color:#666">-</span><span style="color:#666">1</span>]])
</code></pre></div><h4 id="define-model">Define Model</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
<span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">Model</span>(torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Module):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self):
        <span style="color:#a2f">super</span>(Model, self)<span style="color:#666">.</span>__init__()
        self<span style="color:#666">.</span>linear1 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">8</span>, <span style="color:#666">6</span>)
        self<span style="color:#666">.</span>linear2 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">6</span>, <span style="color:#666">4</span>)
        self<span style="color:#666">.</span>linear3 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">4</span>, <span style="color:#666">1</span>)
        self<span style="color:#666">.</span>sigmoid <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Sigmoid()
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self, x):
        x <span style="color:#666">=</span> self<span style="color:#666">.</span>sigmoid(self<span style="color:#666">.</span>linear1(x))
        x <span style="color:#666">=</span> self<span style="color:#666">.</span>sigmoid(self<span style="color:#666">.</span>linear2(x))
        x <span style="color:#666">=</span> self<span style="color:#666">.</span>sigmoid(self<span style="color:#666">.</span>linear3(x))
        <span style="color:#a2f;font-weight:bold">return</span> x
    
model <span style="color:#666">=</span> Model()

</code></pre></div><h4 id="construct-loss-and-optimizer-1">Construct Loss and Optimizer</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">criterion <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>BCELoss(size_average<span style="color:#666">=</span>True)
optimizer <span style="color:#666">=</span> torch<span style="color:#666">.</span>optim<span style="color:#666">.</span>SGD(model<span style="color:#666">.</span>parameters(), lr<span style="color:#666">=</span><span style="color:#666">0.1</span>)
</code></pre></div><h4 id="training-cycle-1">Training Cycle</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    <span style="color:#080;font-style:italic"># Forward</span>
    y_pred <span style="color:#666">=</span> model(x_data)
    loss <span style="color:#666">=</span> criterion(y_pred, y_data)
    <span style="color:#a2f;font-weight:bold">print</span>(epoch, loss<span style="color:#666">.</span>item())

    <span style="color:#080;font-style:italic"># Backward</span>
    optimizer<span style="color:#666">.</span>zero_grad()
    loss<span style="color:#666">.</span>backward()

    <span style="color:#080;font-style:italic"># Update</span>
    optimizer<span style="color:#666">.</span>step()
</code></pre></div><h2 id="04-dataset-and-dataloader">04. Dataset and DataLoader</h2>
<h3 id="use-all-of-the-data">Use all of the data</h3>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">xy <span style="color:#666">=</span> np<span style="color:#666">.</span>loadtxt(<span style="">‘</span>diabetes<span style="color:#666">.</span>csv<span style="color:#666">.</span>gz<span style="">’</span> , delimiter<span style="color:#666">=</span><span style="">‘</span>,<span style="">’</span> , dtype<span style="color:#666">=</span>np<span style="color:#666">.</span>float32)
x_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>from_numpy(xy[:,:<span style="color:#666">-</span><span style="color:#666">1</span>])
y_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>from_numpy(xy[:, [<span style="color:#666">-</span><span style="color:#666">1</span>]])
<span style="">……</span>
<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    <span style="color:#080;font-style:italic"># 1. Forward</span>
    y_pred <span style="color:#666">=</span> model(x_data)
    loss <span style="color:#666">=</span> criterion(y_pred, y_data)
    <span style="color:#a2f;font-weight:bold">print</span>(epoch, loss<span style="color:#666">.</span>item())
    <span style="color:#080;font-style:italic"># 2. Backward</span>
    optimizer<span style="color:#666">.</span>zero_grad()
    loss<span style="color:#666">.</span>backward()
    <span style="color:#080;font-style:italic"># 3. Update</span>
    optimizer<span style="color:#666">.</span>step()
</code></pre></div><h3 id="terminology">Terminology</h3>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic"># Training cycle</span>
<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(training_epochs):
    <span style="color:#080;font-style:italic"># Loop over all batches</span>
    <span style="color:#a2f;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(total_batch):
</code></pre></div><ul>
<li><strong>Epoch</strong>
One forward pass and one backward pass of <strong>all the training examples</strong>.</li>
<li><strong>Batch-Size</strong>
The <strong>number of training examples</strong> in one forward backward pass.</li>
<li><strong>Iteration</strong>
Number of passes, each pass using <strong>batch size</strong> number of examples.</li>
</ul>
<p>比如，1000个样本，我们分成10个Batch，Batch-Size=100，Iteration=10</p>
<h3 id="dataloader">DataLoader</h3>
<p><img src="/img/20210914182044.png" alt=""></p>
<p><strong>Define Dataset</strong></p>
<ul>
<li>Dataset 是一个抽象类，不可以实例化；而 DataLoader 是可以实例化的</li>
<li>number workers 指的是多线程并行的个数，好像 Windows 上必须是0，建议在 Linux 使用</li>
<li>getitem 是当数据比较大，需要通过读取文件名来 get 相应的 item 进行处理</li>
</ul>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">torch.utils.data</span> <span style="color:#a2f;font-weight:bold">import</span> Dataset
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">torch.utils.data</span> <span style="color:#a2f;font-weight:bold">import</span> DataLoader
<span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">DiabetesDataset</span>(Dataset):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self):
        <span style="color:#a2f;font-weight:bold">pass</span>
    <span style="color:#a2f;font-weight:bold">def</span> __getitem__(self, index):
        <span style="color:#a2f;font-weight:bold">pass</span>
    <span style="color:#a2f;font-weight:bold">def</span> __len__(self):
        <span style="color:#a2f;font-weight:bold">pass</span>
dataset <span style="color:#666">=</span> DiabetesDataset()
train_loader <span style="color:#666">=</span> DataLoader(dataset<span style="color:#666">=</span>dataset,
                          batch_size<span style="color:#666">=</span><span style="color:#666">32</span>,
                          shuffle<span style="color:#666">=</span>True,
                          num_workers<span style="color:#666">=</span><span style="color:#666">2</span>)
<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    <span style="color:#a2f;font-weight:bold">for</span> i, data <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">enumerate</span>(train_loader, <span style="color:#666">0</span>):
        <span style="">……</span>
</code></pre></div><h4 id="example-diabetes-dataset">Example: Diabetes Dataset</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">DiabetesDataset</span>(Dataset):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self, filepath):
        xy <span style="color:#666">=</span> np<span style="color:#666">.</span>loadtxt(filepath, delimiter<span style="color:#666">=</span><span style="color:#b44">&#39;,&#39;</span>, dtype<span style="color:#666">=</span>np<span style="color:#666">.</span>float32)
        self<span style="color:#666">.</span>len <span style="color:#666">=</span> xy<span style="color:#666">.</span>shape[<span style="color:#666">0</span>] <span style="color:#080;font-style:italic"># 取第0元素：长度</span>
        self<span style="color:#666">.</span>x_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>from_numpy(xy[:, :<span style="color:#666">-</span><span style="color:#666">1</span>])
        self<span style="color:#666">.</span>y_data <span style="color:#666">=</span> torch<span style="color:#666">.</span>from_numpy(xy[:, [<span style="color:#666">-</span><span style="color:#666">1</span>]])
    <span style="color:#a2f;font-weight:bold">def</span> __getitem__(self, index):
        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>x_data[index], self<span style="color:#666">.</span>y_data[index] <span style="color:#080;font-style:italic"># 返回对应样本即可</span>
    <span style="color:#a2f;font-weight:bold">def</span> __len__(self):
        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>len

dataset <span style="color:#666">=</span> DiabetesDataset(<span style="color:#b44">&#39;diabetes.csv.gz&#39;</span>)
train_loader <span style="color:#666">=</span> DataLoader(dataset<span style="color:#666">=</span>dataset,
                          batch_size<span style="color:#666">=</span><span style="color:#666">32</span>,
                          shuffle<span style="color:#666">=</span>True,
                          num_workers<span style="color:#666">=</span><span style="color:#666">2</span>)

<span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">100</span>):
    <span style="color:#a2f;font-weight:bold">for</span> i, data <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">enumerate</span>(train_loader, <span style="color:#666">0</span>):
        <span style="color:#080;font-style:italic"># 1. Prepare data</span>
        inputs, labels <span style="color:#666">=</span> data
        <span style="color:#080;font-style:italic"># 2. Forward</span>
        y_pred <span style="color:#666">=</span> model(inputs)
        loss <span style="color:#666">=</span> criterion(y_pred, labels)
        <span style="color:#a2f;font-weight:bold">print</span>(epoch, i, loss<span style="color:#666">.</span>item())
        <span style="color:#080;font-style:italic"># 3. Backward</span>
        optimizer<span style="color:#666">.</span>zero_grad()
        loss<span style="color:#666">.</span>backward()
        <span style="color:#080;font-style:italic"># 4. Update</span>
        optimizer<span style="color:#666">.</span>step()
</code></pre></div><ul>
<li>i代表的是第几组数据，表示每次拿的是 $x[i],y[i]$</li>
<li>train_loader 拿出来的是一个元组 $(X,Y)$，是 getitem 传过来的</li>
<li>0代表是从第0个开始枚举</li>
<li>inputs代表 x，labels代表y。或者我们可以将data改为 (input,labels)</li>
</ul>
<h2 id="05-softmax">05. Softmax</h2>
<h3 id="softmax概念">Softmax概念</h3>
<p>对于多输出，我们要对输出进行进一步的<strong class=chinese>规格化</strong>
$$P(y=i)\ge 0\qquad \sum_{i=0}^9 P(y=i)=1$$
Suppose $𝑍^𝑙 ∈ ℝ^𝐾$ is the output of the last linear layer, the* Softmax function*:
$$P(y=i)=\frac{e^{z_i}}{\sum_{j=0}^{K-1} e^{z_j}},\quad i\in {0,\cdots ,K-1}$$</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
criterion <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>CrossEntropyLoss()
Y <span style="color:#666">=</span> torch<span style="color:#666">.</span>LongTensor([<span style="color:#666">2</span>, <span style="color:#666">0</span>, <span style="color:#666">1</span>])

Y_pred1 <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([[<span style="color:#666">0.1</span>, <span style="color:#666">0.2</span>, <span style="color:#666">0.9</span>],
                        [<span style="color:#666">1.1</span>, <span style="color:#666">0.1</span>, <span style="color:#666">0.2</span>],
                        [<span style="color:#666">0.2</span>, <span style="color:#666">2.1</span>, <span style="color:#666">0.1</span>]])
Y_pred2 <span style="color:#666">=</span> torch<span style="color:#666">.</span>Tensor([[<span style="color:#666">0.8</span>, <span style="color:#666">0.2</span>, <span style="color:#666">0.3</span>],
                        [<span style="color:#666">0.2</span>, <span style="color:#666">0.3</span>, <span style="color:#666">0.5</span>],
                        [<span style="color:#666">0.2</span>, <span style="color:#666">0.2</span>, <span style="color:#666">0.5</span>]])

l1 <span style="color:#666">=</span> criterion(Y_pred1, Y)
l2 <span style="color:#666">=</span> criterion(Y_pred2, Y)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;Batch Loss1 = &#34;</span>, l1<span style="color:#666">.</span>data, <span style="color:#b44">&#34;</span><span style="color:#b62;font-weight:bold">\n</span><span style="color:#b44">Batch Loss2=&#34;</span>, l2<span style="color:#666">.</span>data)
</code></pre></div><h3 id="crossentropyloss-vs-nulloss">CrossEntropyLoss vs NULLoss</h3>
<h3 id="implementation">Implementation</h3>
<h4 id="package">Package</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">torchvision</span> <span style="color:#a2f;font-weight:bold">import</span> transforms
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">torchvision</span> <span style="color:#a2f;font-weight:bold">import</span> datasets
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">torch.utils.data</span> <span style="color:#a2f;font-weight:bold">import</span> DataLoader
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch.nn.functional</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">F</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch.optim</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">optim</span>
</code></pre></div><ul>
<li><code>transform</code>: 对数据集进行处理</li>
<li><code>F</code>: For using function relu()</li>
<li><code>optim</code>: For constructing Optimizer</li>
</ul>
<h4 id="prepare-dataset-2">Prepare Dataset</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">batch_size <span style="color:#666">=</span> <span style="color:#666">64</span>
transform <span style="color:#666">=</span> transforms<span style="color:#666">.</span>Compose([
    transforms<span style="color:#666">.</span>ToTensor(),
    transforms<span style="color:#666">.</span>Normalize((<span style="color:#666">0.1307</span>, ), (<span style="color:#666">0.3081</span>, ))
])

train_dataset <span style="color:#666">=</span> datasets<span style="color:#666">.</span>MNIST(root<span style="color:#666">=</span><span style="color:#b44">&#39;../dataset/mnist/&#39;</span>,
                               train<span style="color:#666">=</span>True,
                               download<span style="color:#666">=</span>True,
                               transform<span style="color:#666">=</span>transform)
train_loader <span style="color:#666">=</span> DataLoader(train_dataset,
                          shuffle<span style="color:#666">=</span>True,
                          batch_size<span style="color:#666">=</span>batch_size)

test_dataset <span style="color:#666">=</span> datasets<span style="color:#666">.</span>MNIST(root<span style="color:#666">=</span><span style="color:#b44">&#39;../dataset/mnist/&#39;</span>,
                              train<span style="color:#666">=</span>False,
                              download<span style="color:#666">=</span>True,
                              transform<span style="color:#666">=</span>transform)
test_loader <span style="color:#666">=</span> DataLoader(test_dataset,
                         shuffle<span style="color:#666">=</span>False,
                         batch_size<span style="color:#666">=</span>batch_size)
</code></pre></div><ul>
<li>transform是为了Normalization数据，0.1307是均值，0.3081是标准差。这是MNIST数据集经过计算所得到的均值和标准差。</li>
</ul>
<h4 id="design-model-1">Design Model</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">Net</span>(torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Module):
    <span style="color:#a2f;font-weight:bold">def</span> __init__(self):
        <span style="color:#a2f">super</span>(Net, self)<span style="color:#666">.</span>__init__()
        self<span style="color:#666">.</span>l1 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">784</span>, <span style="color:#666">512</span>)
        self<span style="color:#666">.</span>l2 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">512</span>, <span style="color:#666">256</span>)
        self<span style="color:#666">.</span>l3 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">256</span>, <span style="color:#666">128</span>)
        self<span style="color:#666">.</span>l4 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">128</span>, <span style="color:#666">64</span>)
        self<span style="color:#666">.</span>l5 <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Linear(<span style="color:#666">64</span>, <span style="color:#666">10</span>)
    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">forward</span>(self, x):
        x <span style="color:#666">=</span> x<span style="color:#666">.</span>view(<span style="color:#666">-</span><span style="color:#666">1</span>, <span style="color:#666">784</span>)
        x <span style="color:#666">=</span> F<span style="color:#666">.</span>relu(self<span style="color:#666">.</span>l1(x))
        x <span style="color:#666">=</span> F<span style="color:#666">.</span>relu(self<span style="color:#666">.</span>l2(x))
        x <span style="color:#666">=</span> F<span style="color:#666">.</span>relu(self<span style="color:#666">.</span>l3(x))
        x <span style="color:#666">=</span> F<span style="color:#666">.</span>relu(self<span style="color:#666">.</span>l4(x))
        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>l5(x)

model <span style="color:#666">=</span> Net()
</code></pre></div><h4 id="construct-loss-and-optimizer-2">Construct Loss and Optimizer</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">criterion <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>CrossEntropyLoss()
optimizer <span style="color:#666">=</span> optim<span style="color:#666">.</span>SGD(model<span style="color:#666">.</span>parameters(), lr<span style="color:#666">=</span><span style="color:#666">0.01</span>, momentum<span style="color:#666">=</span><span style="color:#666">0.5</span>)
</code></pre></div><h4 id="train-and-test">Train and Test</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">train</span>(epoch):
    running_loss <span style="color:#666">=</span> <span style="color:#666">0.0</span>
    <span style="color:#a2f;font-weight:bold">for</span> batch_idx, data <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">enumerate</span>(train_loader, <span style="color:#666">0</span>):
        inputs, target <span style="color:#666">=</span> data
        optimizer<span style="color:#666">.</span>zero_grad()
        <span style="color:#080;font-style:italic"># forward + backward + update</span>
        outputs <span style="color:#666">=</span> model(inputs)
        loss <span style="color:#666">=</span> criterion(outputs, target)
        loss<span style="color:#666">.</span>backward()
        optimizer<span style="color:#666">.</span>step()
        running_loss <span style="color:#666">+=</span> loss<span style="color:#666">.</span>item()
        <span style="color:#a2f;font-weight:bold">if</span> batch_idx <span style="color:#666">%</span> <span style="color:#666">300</span> <span style="color:#666">==</span> <span style="color:#666">299</span>:
            <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;[</span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44">, </span><span style="color:#b68;font-weight:bold">%5d</span><span style="color:#b44">] loss: </span><span style="color:#b68;font-weight:bold">%.3f</span><span style="color:#b44">&#39;</span> <span style="color:#666">%</span> (epoch <span style="color:#666">+</span> <span style="color:#666">1</span>, batch_idx <span style="color:#666">+</span> <span style="color:#666">1</span>, running_loss <span style="color:#666">/</span> <span style="color:#666">300</span>))
            running_loss <span style="color:#666">=</span> <span style="color:#666">0.0</span>
</code></pre></div><div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">test</span>():
    correct <span style="color:#666">=</span> <span style="color:#666">0</span>
    total <span style="color:#666">=</span> <span style="color:#666">0</span>
    <span style="color:#a2f;font-weight:bold">with</span> torch<span style="color:#666">.</span>no_grad():
        <span style="color:#a2f;font-weight:bold">for</span> data <span style="color:#a2f;font-weight:bold">in</span> test_loader:
            images, labels <span style="color:#666">=</span> data
            outputs <span style="color:#666">=</span> model(images)
            _, predicted <span style="color:#666">=</span> torch<span style="color:#666">.</span>max(outputs<span style="color:#666">.</span>data, dim<span style="color:#666">=</span><span style="color:#666">1</span>)
            total <span style="color:#666">+=</span> labels<span style="color:#666">.</span>size(<span style="color:#666">0</span>)
            correct <span style="color:#666">+=</span> (predicted <span style="color:#666">==</span> labels)<span style="color:#666">.</span>sum()<span style="color:#666">.</span>item()
    <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;Accuracy on test set: </span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44"> </span><span style="color:#b68;font-weight:bold">%%</span><span style="color:#b44">&#39;</span> <span style="color:#666">%</span> (<span style="color:#666">100</span> <span style="color:#666">*</span> correct <span style="color:#666">/</span> total))
</code></pre></div><div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">for</span> epoch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">10</span>):
    train(epoch)
    test()
</code></pre></div><h4 id="mention">MENTION</h4>
<p>我们在打标签的时候需要从0开始，否则是有问题的。</p>
<h2 id="06-cnn">06. CNN</h2>
<p>卷积神经网络可以做到保留原始的空间信息，而全连接层是展开成一个，无法保留空间信息。</p>
<p><img src="/img/20210915121409.png" alt=""></p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
in_channels, out_channels<span style="color:#666">=</span> <span style="color:#666">5</span>, <span style="color:#666">10</span>
width, height <span style="color:#666">=</span> <span style="color:#666">100</span>, <span style="color:#666">100</span>
kernel_size <span style="color:#666">=</span> <span style="color:#666">3</span>
batch_size <span style="color:#666">=</span> <span style="color:#666">1</span>

<span style="color:#a2f">input</span> <span style="color:#666">=</span> torch<span style="color:#666">.</span>randn(batch_size,
                    in_channels,
                    width,
                    height)

conv_layer <span style="color:#666">=</span> torch<span style="color:#666">.</span>nn<span style="color:#666">.</span>Conv2d(in_channels,
                             out_channels,
                             kernel_size<span style="color:#666">=</span>kernel_size)

output <span style="color:#666">=</span> conv_layer(<span style="color:#a2f">input</span>)

<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#a2f">input</span><span style="color:#666">.</span>shape)
<span style="color:#a2f;font-weight:bold">print</span>(output<span style="color:#666">.</span>shape)
<span style="color:#a2f;font-weight:bold">print</span>(conv_layer<span style="color:#666">.</span>weight<span style="color:#666">.</span>shape)
</code></pre></div><p>输出为：</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">torch<span style="color:#666">.</span>Size([<span style="color:#666">1</span>, <span style="color:#666">5</span>, <span style="color:#666">100</span>, <span style="color:#666">100</span>])
torch<span style="color:#666">.</span>Size([<span style="color:#666">1</span>, <span style="color:#666">10</span>, <span style="color:#666">98</span>, <span style="color:#666">98</span>])
torch<span style="color:#666">.</span>Size([<span style="color:#666">10</span>, <span style="color:#666">5</span>, <span style="color:#666">3</span>, <span style="color:#666">3</span>])
</code></pre></div><h3 id="mnist-example">MNIST Example</h3>
<p><img src="/img/20210915122114.png" alt=""></p>
<h4 id="move-model-to-gpu">Move Model to GPU</h4>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">device <span style="color:#666">=</span> torch<span style="color:#666">.</span>device(<span style="color:#b44">&#34;cuda:0&#34;</span> <span style="color:#a2f;font-weight:bold">if</span> torch<span style="color:#666">.</span>cuda<span style="color:#666">.</span>is_available() <span style="color:#a2f;font-weight:bold">else</span> <span style="color:#b44">&#34;cpu&#34;</span>)
model<span style="color:#666">.</span>to(device)

<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">train</span>(epoch):
    running_loss <span style="color:#666">=</span> <span style="color:#666">0.0</span>
    <span style="color:#a2f;font-weight:bold">for</span> batch_idx, data <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">enumerate</span>(train_loader, <span style="color:#666">0</span>):
        inputs, target <span style="color:#666">=</span> data
        inputs, target <span style="color:#666">=</span> inputs<span style="color:#666">.</span>to(device), target<span style="color:#666">.</span>to(device)
        optimizer<span style="color:#666">.</span>zero_grad()
        
        <span style="color:#080;font-style:italic"># forward + backward + update</span>
        outputs <span style="color:#666">=</span> model(inputs)
        loss <span style="color:#666">=</span> criterion(outputs, target)
        loss<span style="color:#666">.</span>backward()
        optimizer<span style="color:#666">.</span>step()
        running_loss <span style="color:#666">+=</span> loss<span style="color:#666">.</span>item()
        <span style="color:#a2f;font-weight:bold">if</span> batch_idx <span style="color:#666">%</span> <span style="color:#666">300</span> <span style="color:#666">==</span> <span style="color:#666">299</span>:
            <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;[</span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44">, </span><span style="color:#b68;font-weight:bold">%5d</span><span style="color:#b44">] loss: </span><span style="color:#b68;font-weight:bold">%.3f</span><span style="color:#b44">&#39;</span> <span style="color:#666">%</span> (epoch <span style="color:#666">+</span> <span style="color:#666">1</span>, batch_idx <span style="color:#666">+</span> <span style="color:#666">1</span>, running_loss <span style="color:#666">/</span> <span style="color:#666">2000</span>))
            running_loss <span style="color:#666">=</span> <span style="color:#666">0.0</span>
            
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">test</span>():
    correct <span style="color:#666">=</span> <span style="color:#666">0</span>
    total <span style="color:#666">=</span> <span style="color:#666">0</span>
    <span style="color:#a2f;font-weight:bold">with</span> torch<span style="color:#666">.</span>no_grad():
        <span style="color:#a2f;font-weight:bold">for</span> data <span style="color:#a2f;font-weight:bold">in</span> test_loader:
            inputs, target <span style="color:#666">=</span> data
            inputs, target <span style="color:#666">=</span> inputs<span style="color:#666">.</span>to(device), target<span style="color:#666">.</span>to(device)
            outputs <span style="color:#666">=</span> model(inputs)
            _, predicted <span style="color:#666">=</span> torch<span style="color:#666">.</span>max(outputs<span style="color:#666">.</span>data, dim<span style="color:#666">=</span><span style="color:#666">1</span>)
            total <span style="color:#666">+=</span> target<span style="color:#666">.</span>size(<span style="color:#666">0</span>)
            correct <span style="color:#666">+=</span> (predicted <span style="color:#666">==</span> target)<span style="color:#666">.</span>sum()<span style="color:#666">.</span>item()
    <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;Accuracy on test set: </span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44"> </span><span style="color:#b68;font-weight:bold">%%</span><span style="color:#b44"> [</span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44">/</span><span style="color:#b68;font-weight:bold">%d</span><span style="color:#b44">]&#39;</span> <span style="color:#666">%</span> (<span style="color:#666">100</span> <span style="color:#666">*</span> correct <span style="color:#666">/</span> total, correct, total))

</code></pre></div><h3 id="inception-module">Inception Module</h3>
<p><img src="/img/20210915173840.png" alt=""></p>
<h4 id="1x1-convolution">1x1 convolution</h4>
<p>可以<strong class=chinese>改变通道数量</strong>，可以跨越不同通道相同像素的值，做到了信息融合的目的。
卷积核的数量就代表了通道的数量。
作用：降低运算量</p>
<p><img src="/img/20210915175019.png" alt=""></p>

    
  </article>
  <div class="paginator">
    
    <a class="link" href="https://preminstrel.github.io/blog/post/2021/10/17/vim-basics/">← prev</a>
    
    
    <a class="link" href="https://preminstrel.github.io/blog/post/2021/10/17/quantization-compression/">next →</a>
    
  </div>
  <div class="comment">
    
    
    
    
    
    
  </div>
  
</main>

    <footer id="footer">
  <div>
    <span>© 2021</span> - <span>2022</span>
  </div>

  <div>
    <span>Powered by </span>
    <a class="link" href="https://gohugo.io/">Hugo</a>
    <span> 🍦 Theme </span>
    <a class="link" href="https://github.com/queensferryme/hugo-theme-texify">TeXify</a>
  </div>

  <div class="footnote">
    <span>Follow me on <a class=link href=https://github.com/preminstrel>GitHub</a>,
<a class=link href=https://twitter.com/preminstrel>Twitter</a> or
<a class=link href=/index.xml>RSS</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank rel=noopener>CC BY-NC-SA 4.0</a>
</span>
  </div>
</footer>

  </div>

  
  

  
  

  
  

</body>

</html>
