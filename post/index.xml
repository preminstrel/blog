<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Blog de Preminstrel</title>
    <link>https://preminstrel.github.io/blog/post/</link>
    <description>Recent content in Posts on Blog de Preminstrel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>preminstrel@gmail.com (Hanshi Sun)</managingEditor>
    <webMaster>preminstrel@gmail.com (Hanshi Sun)</webMaster>
    <lastBuildDate>Mon, 08 Aug 2022 22:17:34 -0600</lastBuildDate><atom:link href="https://preminstrel.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MacBook Pro 14-inch</title>
      <link>https://preminstrel.github.io/blog/post/2022/08/08/mbp14_2021/</link>
      <pubDate>Mon, 08 Aug 2022 22:17:34 -0600</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/08/08/mbp14_2021/</guid>
      
      <description>&lt;p&gt;In Canada, I bought a MacBook Pro 14-inch whose chip is Apple M1 Pro, with 32 GB Memory and 1TB Storage. The impressive Built-in Liquid Retina XDR Display make it&amp;rsquo;s a pleasure to watch the screen. Moreover, the&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Banff Trip</title>
      <link>https://preminstrel.github.io/blog/post/2022/07/20/banff_trip/</link>
      <pubDate>Wed, 20 Jul 2022 12:58:09 -0600</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/07/20/banff_trip/</guid>
      
      <description>&lt;p&gt;When I arrived Edmonton, I have signed up a trip to Banff. After I signed the waiver, I was on the bus to Banff on July 15th. I lived with two Indians and Ruiqi, who is from HUST, majoring in Mechanical Engineering. One of the Indians is called Simha, the name of Lion King, which sounds very ambitious. We have a night chat, and I got to know that universities in India employ the origin English textbooks and teach them in English, which is really a shock to me. Simha said that his major is computer science and he has some papers accepcted by top conference like CVPR and AAAI. His intern toppic is related to ECG diagnosis, extremely resembling my first topic. Just when we are sharing our viewpoint upon the projects, Ruiqi had taken a lot of photos of the sky with twinkling stars.&lt;/p&gt;
&lt;p&gt;On July 16th, we waked up pretty early and got a ride to Lake Louise, which I think is the best sencery during this trip. We took enormous photos and chose a very well-taken one as our avater in all social networking accounts. The most impressive thing is that I got the new avatar with Lake Louise.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/preminstrel/preminstrel.github.io/raw/master/assets/img/bio.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;After that, we had a boat floting tour around the moutains on the limpid water. To be frank, it is very cozy for me to enjoy the comfortable wind with tiny sway. We had a lunch at Banff Town and we selected a Chinese resturant for just some appetizers because the main dishes are so expensive that we cannot afford it.&lt;/p&gt;
&lt;p&gt;In the afternoon, we made the acquaintance of Kinh and Toya, who are from Vitnam and Japan respectly, and now we are Asian Five.&lt;/p&gt;
&lt;p&gt;I think the 17th trip is not as interesting as 16th, which is just like a dessert after main dishes. Therefore, I will demonstrate it in this blog.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Canada Entry</title>
      <link>https://preminstrel.github.io/blog/post/2022/07/09/canada-entry/</link>
      <pubDate>Sat, 09 Jul 2022 16:04:30 -0600</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/07/09/canada-entry/</guid>
      
      <description>&lt;p&gt;今年 7 月 6 号的时候，坐上了上海浦东 PVG 出发的 AC026 航班，前往加拿大温哥华 YVR 后转机至 YEG。在温哥华办理了入境手续，先取了行李，随后去 Immigration Office 办理了 Visitor Record，给我签到了 2022/10/31。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Canada Visa</title>
      <link>https://preminstrel.github.io/blog/post/2022/05/09/canada-visa/</link>
      <pubDate>Mon, 09 May 2022 16:15:25 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/05/09/canada-visa/</guid>
      
      <description>&lt;h2 id=&#34;application---march&#34;&gt;Application - March&lt;/h2&gt;
&lt;p&gt;参与 Mitacs 项目需要申请加拿大签证，这里进行一个申请的记录。这个项目让我们申请的是 TRV (Temporary Resident Visa)，通过 GSS (Global Skill Strategy) 在入境时申请 Work Exemption，也就是说，我申请的是 Visitor Visa V-1。但是实际上，目前很多人下的签都是 WX-1，是工签，有人是 MULTIPLE，也有人是 ONE。由于我还没贴签，所以不太清楚自己下的是什么。&lt;/p&gt;
&lt;p&gt;申请的时候，有两个通道，分别是 GCKey 和 IRCC Portal，前者是老通道，后者是疫情之后才开放的新通道。老通道是填表，同时要上传很多材料；新通道是类似于做问卷一样，填入一些信息，eye-friendly。我选择了 Portal 进行申报，在三月一号的时候完成了签证的申请。在缴费 $185 的时候，我发现用 Mastercard 付款失败了，但是用银联的信用卡反而成功了，群里也有人反应说借记卡也可以付款成功。指纹采集信当天就下来了，我预约了三月四号中午去上海的签证中心录指纹。&lt;/p&gt;
&lt;h2 id=&#34;biometrics---march&#34;&gt;Biometrics - March&lt;/h2&gt;
&lt;p&gt;三月四日，我坐动车去上海的加拿大签证中心进行生物信息采集 (Biometrics)，本想去一趟斌斌舅舅家玩，但是由于他有事还是算了。录指纹的时候要携带：护照、护照复印件、同意书、指纹采集信；在采集完之后会给你一个热敏打印一样的小贴纸，这个不能扔，要好好保管住了，在贴签的时候以及出入境都要查看。指纹有效期是十年，所以十年之内再申请加签是不用再录指纹的。录完指纹后，我在南京路逛了逛，中午吃了西餐，下午去了一趟外文书店看了看。随后改签了动车，早早就回无锡了。&lt;/p&gt;
&lt;h2 id=&#34;webform---april&#34;&gt;Webform - April&lt;/h2&gt;
&lt;p&gt;签证迟迟不下，我便利用 Webform 进行了催签，上海和北京和加拿大的都催了一下，收到了北京签证中心的回复，但是回复令人心寒，通篇的基调是 negative 的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Email from &lt;code&gt;beijing-immigration@international.gc.ca&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;Please be advised that your application is currently undergoing standard background checks and the processing time will be extended. Unfortunately, we are unable to advice at this time when a final decision might be made.&lt;/p&gt;
&lt;p&gt;Please note that all applications are considered on their own merits and there is no guarantee that a visa will be issued.  You will be advised if any further documentation or information is required.&lt;/p&gt;
&lt;p&gt;Should you wish to withdraw your application, please send signed a written request advising this office that you wish to withdraw your application to &lt;a href=&#34;mailto:beijing-immigration@international.gc.ca&#34;&gt;beijing-immigration@international.gc.ca&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;加拿大签证中心给的回复稍微中性一些，但是都是套话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Good day Hanshi Sun,&lt;/p&gt;
&lt;p&gt;Thank you for contacting Immigration, Refugees and Citizenship Canada (IRCC).&lt;/p&gt;
&lt;p&gt;We verified the information you provided and can confirm that your application is still in process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;re-application---april&#34;&gt;Re-application - April&lt;/h2&gt;
&lt;p&gt;由于害怕自己被拒签，所以又在 GCKey 上申请了一次，多花了 $100，还申请了调档，花了 ￥100。GCKey申请后指纹自动同步了，这次没过几天就开始了 review。过了仅仅五个工作日，在 2022-04-29 的凌晨 02:00，我的邮箱先后收到了 IRCC Portal 的 Withdraw 和 GCKey 的 Original Passport Request，重新递交的效果显著！&lt;/p&gt;
&lt;p&gt;GCKey Timeline:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Subject&lt;/th&gt;
&lt;th&gt;Date sent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Original Passport Request&lt;/td&gt;
&lt;td&gt;April 28, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Correspondence Letter&lt;/td&gt;
&lt;td&gt;April 28, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Submission Confirmation&lt;/td&gt;
&lt;td&gt;April 21, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Confirmation of Online Application Transmission&lt;/td&gt;
&lt;td&gt;April 21, 2022&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;申请材料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application for Visitor Visa (Temporary Resident Visa) Made Outside of Canada (IMM5257): &lt;code&gt;imm5257e.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Offer of Employment: &lt;code&gt;invitation-letter + award-letter&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proof of Work Permit Exemption: &lt;code&gt;award-letter.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Family Information Form (IMM5707): &lt;code&gt;imm5707e.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Travel History: &lt;code&gt;travel.pdf&lt;/code&gt; (概述去过的境外国家经历+目的+签证)&lt;/li&gt;
&lt;li&gt;Passport: &lt;code&gt;passport.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;General Education and Employment Form: &lt;code&gt;imm0104e.pdf&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Proof of Means of Financial Support: &lt;code&gt;invitation-letter + award-letter + 芝麻信用英文报告&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Digital photo: &lt;code&gt;photo.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Purpose of Travel - Other: 描述本次去的目的和行程，写一个文档，转成 PDF 上传&lt;/li&gt;
&lt;li&gt;Proof that you Meet the Requirements of the Job Being Offered： 成绩单 + invitation-letter + award-letter&lt;/li&gt;
&lt;li&gt;Schedule 1 - Application for a Temporary Resident Visa Made Outside Canada (IMM 5257): &lt;code&gt;imm5257b_1.pdf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当天将护照和相关材料用顺丰寄出，寄到北京加拿大签证申请中心，静候贴签的 tracking number。&lt;/p&gt;
&lt;h2 id=&#34;vfs---may&#34;&gt;VFS - May&lt;/h2&gt;
&lt;p&gt;五月四号，五一节之后的第一天，收到 VFS 邮件，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - Your application has been dispatched from the Canada Visa Application Centre to the IRCC Office on Wed May 04 2022 for processing.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第二封：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - Your application has been received and is under process at the IRCC Office on Thu May 05 2022.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第三封（五月六日）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - The decision envelope for your application has been dispatched from the IRCC Office to the Canada Visa Application Centre.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第四封（五月六日）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tracking ID No. - 20220504CNBJPKT72344 - The decision envelope for your application has been received at the Canada Visa Application Centre on Fri May 06 2022 from the IRCC Office, and is ready for collection or further delivery by courier as per your option upon the submission of the application. For details of the VAC locations, please refer to our website at: &lt;a href=&#34;https://visa.vfsglobal.com/chn/en/can/attend-centre;&#34;&gt;https://visa.vfsglobal.com/chn/en/can/attend-centre;&lt;/a&gt; for the business hours of operation under the COVID impact, please refer to our website at: &lt;a href=&#34;https://visa.vfsglobal.com/chn/en/can/news/covid-update&#34;&gt;https://visa.vfsglobal.com/chn/en/can/news/covid-update&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第五封（五月六日）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The decision envelope for your application, tracking ID No. 20220504CNBJPKT72344 has been couriered from the Canada Visa Application Centre, Canada Visa Application Centre, Beijing on Fri May 06 2022 via courier partner. Please use Tracking id. or the AWB number provided to track the shipment on the courier partners website after 24hrs.&lt;/p&gt;
&lt;p&gt;Regards,&lt;/p&gt;
&lt;p&gt;VFS Global&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;五月九日，收到了北京发来的 EMS，贴签结束。查看签证后，发现是 WX-1，有效期到 2029-07-08，还算满意了。但是申请签证的过程，堪称折磨。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Tales of Arise</title>
      <link>https://preminstrel.github.io/blog/post/2022/05/05/tales-of-arise/</link>
      <pubDate>Thu, 05 May 2022 22:18:01 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/05/05/tales-of-arise/</guid>
      
      <description>&lt;blockquote&gt;
&lt;p&gt;Keep the sentence, until everything is settled. &amp;mdash;&amp;mdash; Shionne&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tales of Arise is the first video game of Tales Series I played. I am impressed by its remarkable landscape and typical characters. However, I think it should be a gap between it and Xenoblade Chronicles 2. The plot of TOA (Tales of Arise) is a little conservative that I can even predict the end. Moreover, the music exactly does not match the plot or scenes. Somehow, I regard it as pale.&lt;/p&gt;
&lt;h2 id=&#34;characters-and-plots&#34;&gt;Characters and Plots&lt;/h2&gt;
&lt;p&gt;The male protagonist is the typical JRPG role. And the emotional or love details of the male and female protagonists are adequately described, which is commendable. The heroine is born to be untouchable and the hero can touch her thanks to his sealed nervous system. Therefore, typically, it is a &amp;ldquo;boy meets girl&amp;rdquo; story. Immense quantity of little dialogues falshcards effectively contribute to the constructing the characters and its personalities. Maybe because of the shortage of budget or time limit, the last phase of the plot seems a little hash and does not provide enough time for me to enjoy and digest. Thanks to the kindness of &lt;a href=&#34;https://www.bandainamcostudios.com/en/&#34;&gt;BANDAI NAMCO Studios Inc.&lt;/a&gt;, the end is a satisfactory for me and most players. But, you know, cause the XB2 (Xenoblade Chronicles 2) made a great impact to me, such plots or characters are not that attractive.&lt;/p&gt;
&lt;h2 id=&#34;scenes&#34;&gt;Scenes&lt;/h2&gt;
&lt;p&gt;The scenes of TOA is really a big deal, which has made a milestone for JRPG area, proving that the scene of JRPG can be that impressvie. It could be the new game egnie or rendering method that makes such wonderful world and characters with exactly pragmatic consumption of GPU and CPU memory. It is a really advancement for Tales Series and JRPG! Congratulations!&lt;/p&gt;
&lt;h2 id=&#34;music&#34;&gt;Music&lt;/h2&gt;
&lt;p&gt;To be frank, expect for the two OP and ED, the music is terrible, especially the background music. I hope the BANDAI could be careful dealing with DLC sale strategy because I heard that the music can be improved by the DCL purchase. The OP and animation perfomance is really remarkable, but I have no more words for overall music. After all, I like the OP2: &lt;code&gt;Hello,Again～昔からある場所～&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;combat-system&#34;&gt;Combat System&lt;/h2&gt;
&lt;p&gt;I truly appreciate the clever artificial intelligence operation, which even is better than me. Thanks to the beatiful and impressive combat aniamtion, the system can be better than I think. The strategy can be set in advance, which is friendly. However, the quantity of boring side quests and repetitive monster battles is not that satisfying.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Although I don&amp;rsquo;t play a lot of JRPGs, I think a story can be very simple or very old-fashioned, but as long as the characters who accompany him on this journey for dozens of hours can continue to live happily in this game, and my own life is also empowered because of this, so all I can say is, it&amp;rsquo;s a good game.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plots: ★★★&lt;/li&gt;
&lt;li&gt;Characters:  ★★★&lt;/li&gt;
&lt;li&gt;Scenes: ★★★★★&lt;/li&gt;
&lt;li&gt;Music: ★★&lt;/li&gt;
&lt;li&gt;System: ★★★&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Mitacs Globalink Research Interns</title>
      <link>https://preminstrel.github.io/blog/post/2022/04/15/mitacs/</link>
      <pubDate>Fri, 15 Apr 2022 13:47:07 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/04/15/mitacs/</guid>
      
      <description>&lt;p&gt;去年八月份的时候，学校教务处官网发了申报 Mitacs 项目的相关申报流程。简单来说，这个项目是加拿大 Mitasc 和 CSC 的合作项目，每年选派 200 名本科生在大三暑假去加拿大各高校去进行 Research。这个项目和自己套的暑研比，优缺点也是比较明显的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;: 每个月国家提供经费 $1800，国家承担来回机票，报销签证费用；Mitacs 免费帮买保险，也有理由可以去进行课内的暑期短学期的学分替换，签证申请比较方便等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cons&lt;/strong&gt;: 套到的项目和学校都不如自己套的好，项目匹配机制比较迷，周期很长，需要填很多材料，申请流程比较繁琐。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我最后是录到了 University of Alberta 的 ECE department 的一个华人老师 &lt;a href=&#34;https://www.ece.ualberta.ca/~xingyu/index.html&#34;&gt;Xingyu Li&lt;/a&gt; 的 Weak/self supervision for abnormal detection in medical image analysis 项目，下面我来简单谈谈申请流程。&lt;/p&gt;
&lt;h2 id=&#34;personal-background&#34;&gt;Personal Background&lt;/h2&gt;
&lt;p&gt;首先提供我申请时的背景。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本科 SEU，CGPA: 3.98/4, 93.8/100&lt;/li&gt;
&lt;li&gt;无语言成绩，无竞赛，有国奖等零碎的奖学金&lt;/li&gt;
&lt;li&gt;有一段相关科研和毫不相关的科研经历，有一篇 EI 水会并未在 CV 提及&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;2021-09-04&lt;/em&gt;   完成 Mitacs 网申，收到自动回复的邮件说 Application Complete&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-09-26&lt;/em&gt;   收到邮件说申请 portal 已关闭：2022 Globalink Research Internship applications now closed&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-07&lt;/em&gt;   陆续有人收到拒信&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-10&lt;/em&gt;   官网更新消息，CUC (Candidate under consideration)，也就是通过初审，进入项目匹配阶段。这一阶段大家可能会收到一些老师的面试和相关信息&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-18&lt;/em&gt;   收到第一个面试邮件，是我 Rank 7 UBC 的一位女老师。提出了以下面试细节：
&lt;ul&gt;
&lt;li&gt;5-10 min interview&lt;/li&gt;
&lt;li&gt;Zoom is required, with video if possible&lt;/li&gt;
&lt;li&gt;time restricted, on Friday Nov 19th, noon PDT (Vancouver time)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-11-20&lt;/em&gt;   凌晨四点，进入 Zoom 会议，面试的人有七个，我排在倒数第二个，我以为大概五点多到我，但是四点半就到我了。因为这个老师面试极快，先 30s 介绍了自己的项目，然后就问了我一个问题——介绍你以前的project，我讲之前，她问我你能不能在两分钟之内结束。我打开自己做的 Slides 开始讲，成功在两分钟之内讲完了。说好的 5-10 min 呢？我很快讲完之后，她直接说：OK，have a good day, you can leave the zoom room. 面完就感觉她挺急的，这个 rank 7 的项目估计是寄了，不过也不咋心疼&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-12-10&lt;/em&gt;   Portal 出 offer 了 是 Alberta 无面试录了。下午一点收到了 offer 邮件，说匹配成功，Congratulations&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2021-12-11&lt;/em&gt;   可以在 Portal 上 accept 了，接受之后会有一封感谢信发到邮箱&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-02-17&lt;/em&gt;   收到下一步的邮件，填写一些 agreement 和个人信息。关于疫情是否 on site，Mitacs 觉得是大概率 on site 的。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-03-01&lt;/em&gt;   在 IRCC Portal 申请了签证&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-03-04&lt;/em&gt;   去上海签证中心录生物信息，进入漫长的 Visa 审核流程&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-03-22&lt;/em&gt;   CSC 系统更新，发贺信，进行签约和一些银行卡开卡手续&lt;/li&gt;
&lt;li&gt;&lt;em&gt;2022-04-15&lt;/em&gt;   最近上海爆发疫情，CSC 银行卡的寄送受到了影响，我们也封校了，签证还在审理中&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;application-detail&#34;&gt;Application Detail&lt;/h2&gt;
&lt;p&gt;这里说一说我认为的对成功申请和匹配的一些建议和细节。首先，匹配分两个阶段。第一阶段初审过后，会分为三类：CUC、Waitlist、Rej。CUC 代表经过初审，可以去和教授进行面试和 rerank 来匹配。匹配结束后，CUC 有可能没匹配上，进入 Waitlist。然后是第二轮匹配，这个时候第一轮的 Waitlist 有概率会被录取，但是可能性比较小。&lt;/p&gt;
&lt;h3 id=&#34;初审-rightarrow-cuc&#34;&gt;初审 $\rightarrow$ CUC&lt;/h3&gt;
&lt;p&gt;想要通过初审，那么 GPA 和 Resume 是最重要的。也可以看到，我没有任何竞赛和语言成绩，依旧通过了初审。所以 transcript 和 Resume 至关重要。GPA 这个也不好准备，大家都知道，基本上低于 85 分就没啥希望了。主要努力方向还是 Resume。Mitacs 官网会提供 Resume Template，但是我不建议用。个人写自己的 Resume 是要去突出自己的优势，稍微掩盖自己的劣势的。同时，Mitacs 给的模板比较粗糙，不好看。所以我建议用 LaTeX 进行写作，Overleaf 上也可以找到一些好看的模板，不会装 LaTeX 的可以用 Overleaf 在线编译下载 PDF（最好还是学一下 LaTeX，这个不会就去做 Research 也挺离谱的）。&lt;/p&gt;
&lt;h3 id=&#34;interview-rightarrow-offer&#34;&gt;Interview $\rightarrow$ Offer&lt;/h3&gt;
&lt;p&gt;这个阶段基本是最难熬的，可能会很焦虑。尤其是当你发现，自己一封邮件没收到，群里小伙伴有的都面完五六个老师的时候，心里会很不是滋味。这里我想说的是，无面试录取的可能性很大，尤其是 University of Alberta，好多都是没面试直接拿 Offer 的。如果你有幸被面试，那么一定要提前做好展示的 Slides，然后配置好网络环境（科学上网），最后面试完给老师发一封感谢信。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;这个项目总体来说，由于 Mitacs 的谜一般的匹配机制，所以也是比较看运气的。所以，如果没录不用觉得自己咋样，自己套往往是更好的。祝大家都能拿到 offer。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Transformer</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/26/transformer/</link>
      <pubDate>Wed, 26 Jan 2022 17:47:40 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/26/transformer/</guid>
      
      <description>&lt;p&gt;《Attention Is All You Need》是 Google 团队在 2017 年提出的一篇论文。该论文以“attention”为核心，提出了 Transformer 模型。Transformer 基于 Encoder-Decoder，摒弃了 CNNs，完全由 Attention mechanism 实现。&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;p&gt;传统 seq2seq 最大的问题在于将 Encoder 端的所有信息&lt;strong&gt;压缩到一个固定长度的向量&lt;/strong&gt;中，并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，而且这样一股脑的把该固定向量送入 Decoder 端，Decoder 端不能够关注到其想要关注的信息。并且模型计算不可并行，计算隐层状态 $h_t$ 依赖于 $h_{t-1}$ 以及状态 $t$ 时刻的输入，因此需要耗费大量时间。&lt;/p&gt;
&lt;p&gt;Transformer 完全依赖于 Attention Mechanism，解决了输入输出的长期依赖问题，并且拥有并行计算的能力，大大减少了计算资源的消耗。Self-Attention模块，让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力。Muti-Head Attention 模块使得 Encoder 端拥有并行计算的能力。&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;
&lt;p&gt;Transformer 采用 Encoder-Decoder 架构，如下图所示。Encoder 层和 Decoder 层分别由 6 个相同的 Encoder 和decoder堆叠而成，模型架构更加复杂。其中，Encoder 层引入了 &lt;em&gt;&lt;strong&gt;Multi-Head&lt;/strong&gt;&lt;/em&gt; 机制，可以并行计算，Decoder 层仍旧需要串行计算。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/Ml7Wiqra8TdAZv2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/Ml7Wiqra8TdAZv2.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Encoder 层和 Decoder 层内部结构如下图所示。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/rCmxoUspEFbhfSd&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/rCmxoUspEFbhfSd.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Encoder 具有两层结构，&lt;strong&gt;Self-Attention 和前馈神经网络&lt;/strong&gt;。Self-Attention 计算句子中的每个词都和其他词的关联，从而帮助模型更好地理解上下文语义，引入 Muti-Head Attention 后，每个头关注句子的不同位置，增强了Attention 机制关注句子内部单词之间作用的表达能力。前馈神经网络为 Encoder 引入非线性变换，增强了模型的拟合能力。&lt;/li&gt;
&lt;li&gt;Decoder 接受 output 输入的&lt;strong&gt;同时接受 Encoder 的输入&lt;/strong&gt;，帮助当前节点获取到需要重点关注的内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-head-attention&#34;&gt;Multi-Head Attention&lt;/h3&gt;
&lt;p&gt;Multi-Head Attention 计算过程如下图，在讲解Multi-Head Attention 之前，我们需要了解Self-Attention。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/vuW2BzLpKig3lrV&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/vuW2BzLpKig3lrV.png&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Query 与 Key 作用得到 Attention 的权值，之后这个权值作用在 Value 上得到 Attention值。&lt;/strong&gt; 这种通过 Query 和 Key 的相似性程度来确定 value 的权重分布的方法被称为 &lt;em&gt;&lt;strong&gt;scaled dot-product attention&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{D_k}})V$$&lt;/p&gt;
&lt;p&gt;这里给出我在知乎上看到的一个很不错的帖子里面的图片解释 scaled dot-product attention：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/1VaBDNAm4S2Yex9&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/1VaBDNAm4S2Yex9.jpg&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;但是，在 Transformer 模型中，作者使用了 Muti-Head 机制代替了 single self-attention。&lt;/p&gt;
&lt;p&gt;$$
\text{MultiHead}(Q,K,V) =\text{Concat}\left(\text {head}_1, \ldots, \text{head}_h \right) W^{O}
$$&lt;/p&gt;
&lt;p&gt;$$
\text{where head}_{\mathrm{i}} =\operatorname{Attention}\left(QW_i^Q, KW_i^K, VW_i^V \right)
$$&lt;/p&gt;
&lt;p&gt;Where the projections are parameter matrices $W_{i}^{Q} \in \mathbb{R}^{d_{model} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{model} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{model} \times d_{v}}$ and $W^{O} \in \mathbb{R}^{h d_{v} \times d_{model}}$.&lt;/p&gt;
&lt;p&gt;论文中采用 8 个头，$h=8,d_{k}=d_{v}=d_{model} / h=64$。通过权重矩阵 $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ 将 $Q,K,V$ 分割，每个头分别计算 single self-attention，因为权重矩阵 $W_{i}^{Q},W_{i}^{K},W_{i}^{V}$ 不相同，$QW_i^Q,KW_i^K,VW_i^V$ 的结果各不相同，因此我们说每个头的关注点各有侧重。最后，将每个头计算出的 single self-attention 进行 concat，通过总的权重矩阵 $W^O$ 决定对每个头的关注程度，从而能够做到在不同语境下对相同句子进行不同理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention 是将 Query 和 Key 映射到同一高维空间中去计算相似度，而对应的 Multi-head Attention 把 Query 和 Key 映射到高维空间 $\alpha$ 的不同子空间 $(\alpha_1,\alpha_2,\dots, \alpha_h)$ 中去计算相似度。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;position-wise-feed-forward&#34;&gt;Position-wise Feed Forward&lt;/h3&gt;
&lt;p&gt;$$\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2$$&lt;/p&gt;
&lt;p&gt;每一层经过 Attention 之后，还会有一个 FFN，这个 FFN 的作用就是&lt;strong&gt;空间变换&lt;/strong&gt;。FFN 包含了 2 层 Linear Transformation 层，中间的激活函数是 ReLU。&lt;/p&gt;
&lt;p&gt;Attention 层的 output 最后会和 $W^O$ 相乘，为什么这里又要增加一个 2 层的 FFN 网络？其实，FFN 的加入&lt;strong&gt;引入了非线性(ReLu激活函数)，变换了 Attention Output 的空间, 从而增加了模型的表现能力&lt;/strong&gt;。把 FFN 去掉模型也是可以用的，但是效果差了很多。&lt;/p&gt;
&lt;h3 id=&#34;layer-normalization&#34;&gt;Layer Normalization&lt;/h3&gt;
&lt;p&gt;在每个 block 中，最后出现的是 Layer Normalization，其作用是规范优化空间，加速收敛。&lt;/p&gt;
&lt;p&gt;$$\text{LN}(x_i)=\alpha\frac{x_i-\mu_i}{\sqrt{\sigma^2+\xi}}+\beta$$&lt;/p&gt;
&lt;p&gt;当我们使用梯度下降算法做优化时，我们可能会对输入数据进行归一化，但是经过网络层作用后，我们的数据已经不是归一化的了。随着网络层数的增加，数据分布不断发生变化，偏差越来越大，导致我们不得不使用&lt;strong&gt;更小的学习率&lt;/strong&gt;来稳定梯度。Layer Normalization 的作用就是&lt;strong&gt;保证数据特征分布的稳定性&lt;/strong&gt;，将数据标准化到 ReLU 激活函数的作用区域，可以使得激活函数更好的发挥作用&lt;/p&gt;
&lt;h3 id=&#34;positional-encoding&#34;&gt;Positional Encoding&lt;/h3&gt;
&lt;p&gt;位置信息编码位于 Encoder 和 Decoder 的 Embedding 之后，每个 block 之前。它非常重要，没有这部分模型就无法运行。Positional Encoding 是 Transformer 的特有机制，弥补了 Attention 机制无法捕捉 sequence 中 token 位置信息的缺点。&lt;/p&gt;
&lt;p&gt;$$
PE_{(pos, 2i)}=\sin\left(pos/10000^{2i/d_{\text{model}}}\right)
$$&lt;/p&gt;
&lt;p&gt;$$
PE_{(pos,2i+1)}=\cos\left(pos/10000^{2i/d_{\text{model}}}\right)
$$&lt;/p&gt;
&lt;p&gt;Positional Embedding 的成分直接叠加于 Embedding 之上，使得每个 token 的&lt;strong&gt;位置信息&lt;/strong&gt;和它的&lt;strong&gt;语义信息&lt;/strong&gt;(embedding)充分融合，并被传递到后续所有经过复杂变换的序列表达中去。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transformer 中，模型输入 Encoder 的每个 token 向量由两部分加和而成：Position Encoding + Input Embedding。Transformer 的特性使得输入 Encoder 的向量之间完全平等（不存在 RNN 的 recurrent 结构），token 的实际位置于位置信息编码唯一绑定。Positional Encoding 的引入使得模型能够充分利用 token 在 sequence 中的位置信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;论文中使用的 Positional Encoding(PE) 是正余弦函数，位置(pos)越小，波长越长，每一个位置对应的 PE 都是唯一的。同时作者也提到，之所以选用正余弦函数作为 PE，是因为这可以使得模型学习到 token 之间的相对位置关系：因为对于任意的偏移量 $k$，$PE_{pos+k}$ 可以由 $PE_{pos}$ 的线性表示，也就是 $PE_{pos}$ 乘上某个线性变换矩阵就得到了 $PE_{pos+k}$。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
P E_{(p o s+k, 2 i)}=\sin \left((p o s+k) / 10000^{2 i / d_{\text {model }}}\right)
$$$$
P E_{(p o s+k, 2 i+1)}=\cos \left((p o s+k) / 10000^{2 i / d_{\text {model }}}\right)
$$&lt;/p&gt;
&lt;h3 id=&#34;mask&#34;&gt;Mask&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Mask&lt;/strong&gt;&lt;/em&gt; 表示掩码，它&lt;strong&gt;对某些值进行掩盖，使其在参数更新时不产生效果&lt;/strong&gt;。Transformer 模型里面涉及两种 Mask，分别是 Padding Mask 和 Sequence Mask。其中，Padding Mask 在所有的 scaled dot-product attention 里面都需要用到，而 Sequence Mask 只有在 Decoder 的 Self-Attention 里面用到。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Padding Mask&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;什么是 Padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的 Attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。&lt;/p&gt;
&lt;p&gt;具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！&lt;/p&gt;
&lt;p&gt;而我们的 Padding mask 实际上是一个张量，每个值都是一个Boolean，值为 False 的地方就是我们要进行处理的地方。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Sequence mask&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Sequence Mask 是为了使得 Decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。&lt;/p&gt;
&lt;p&gt;具体办法是：&lt;strong&gt;产生一个上三角矩阵，上三角的值全为 0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于 Decoder 的 Self-Attention，里面使用到的 scaled dot-product attention，同时需要 Padding Mask 和 Sequence mask 作为 attn_mask，具体实现就是两个 Mask 相加作为 attn_mask。其他情况，attn_mask 一律等于 Padding mask。&lt;/p&gt;
&lt;h3 id=&#34;linear--softmax&#34;&gt;Linear &amp;amp; Softmax&lt;/h3&gt;
&lt;p&gt;Decoder 最后是一个线性变换和 Softmax 层。解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是 Softmax 层。&lt;/p&gt;
&lt;p&gt;线性变换层是一个简单的全连接神经网络，它可以&lt;strong&gt;把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里&lt;/strong&gt;。不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数（&lt;strong&gt;相当于做 vocaburary_size 大小的分类&lt;/strong&gt;）。接下来的 Softmax 层便会把那些分数变成概率（都为正数、上限 1.0）。&lt;strong&gt;概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;整体运行效果图如下：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/7wBRdlvJnzeVUL9&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/7wBRdlvJnzeVUL9.gif&#34; &gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/311156298&#34;&gt;Transformer - Attention is all you need&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Self-Attention</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/26/self-attention/</link>
      <pubDate>Wed, 26 Jan 2022 16:36:51 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/26/self-attention/</guid>
      
      <description>&lt;blockquote&gt;
&lt;p&gt;Attention is all you need.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最近刚接触到 Transformer，感觉其模型比 CNNs 要复杂了不少，看了一些论文也仅仅是草草看过，不理解其原理，在网上读了一些 blog，本次来进行一次总结。首先便是 Self-Attention 的公式&lt;/p&gt;
&lt;p&gt;$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{D_k}})V$$&lt;/p&gt;
&lt;h2 id=&#34;terminology&#34;&gt;Terminology&lt;/h2&gt;
&lt;p&gt;公式种出现的 $Q,K,V$ 分别是 Query、Key、Value的缩写，我们的表达式如下：&lt;/p&gt;
&lt;p&gt;$$X W^Q=Q$$
$$XW^K=K$$
$$XW^V=V$$&lt;/p&gt;
&lt;p&gt;文章中所谓的 $Q,K,V$ 矩阵来源于 $X$ 与矩阵的乘积，本质上是 $X$ 的一系列的线性变换。做线性变换是为了提升模型的拟合能力，矩阵 $W$ 都是可以训练的，起到一个缓冲的效果。&lt;/p&gt;
&lt;p&gt;我们假设 $Q,K$ 种元素的均值为 0，方差为 1，$A^T=Q^TK$ 的均值为 0，方程为 $D$。当 $D$ 变得很大时，$A$ 中的元素的方差也会变得很大，如果 $A$ 中的元素方差很大，那么 $A$ 的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。我们可以将分布“陡峭”程度与 $D$ 解耦，从而使得训练过程中梯度值保持稳定。&lt;/p&gt;
&lt;p&gt;$$A\leftarrow \dfrac{A}{\sqrt{D_k}}$$&lt;/p&gt;
&lt;h2 id=&#34;theory&#34;&gt;Theory&lt;/h2&gt;
&lt;p&gt;公式中的 $QK^T$ 表示的是 $Q,K$ 的内积，也可以说是一方在另一方的&lt;strong&gt;投影&lt;/strong&gt;，其大小也可以表示其&lt;strong&gt;相关性&lt;/strong&gt;。Softmax 是为了将一系列的值&lt;strong&gt;归一化&lt;/strong&gt;而存在的。&lt;/p&gt;
&lt;p&gt;$$\text{softmax}(z_k)=\frac{e^{z_k}}{\sum_{i=1}^Ie^{z_i}}$$&lt;/p&gt;
&lt;p&gt;而随后与 $V$ 的乘积，代表的是&lt;strong&gt;向量经过注意力机制加权求和之后的结果&lt;/strong&gt;。也就是说，softmax 管的是一个相关度权值大小，与后面的 $V$ 相乘，得到的是通过相关度权值标准而重新计算得到的量。&lt;/p&gt;
&lt;p&gt;对 Self-Attention 来说，它跟每一个输入的向量都做 Attention，所以没有考虑到输入的顺序。更通俗来讲，大家可以发现我们前文的计算每一个词向量都与其他词向量计算内积，得到的结果丢失了我们原来文本的顺序信息。对比来说，LSTM 是对于文本顺序信息的解释是输出词向量的先后顺序，而我们上文的计算对 sequence 的顺序这一部分则完全没有提及，你打乱词向量的顺序，得到的结果仍然是相同的，此处便可以引出 Transformer 的位置编码部分。&lt;strong&gt;Query 与 Key 作用得到 Attention 的权值，之后这个权值作用在 Value 上得到 Attention值。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Attention 机制的实现&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;math&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; sqrt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;torch.nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#00f&#34;&gt;Self_Attention&lt;/span&gt;(nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Module):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# input : batch_size * seq_len * input_dim&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# q : batch_size * input_dim * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# k : batch_size * input_dim * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# v : batch_size * input_dim * dim_v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;def&lt;/span&gt; __init__(self,input_dim,dim_k,dim_v):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a2f&#34;&gt;super&lt;/span&gt;(Self_Attention,self)&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;__init__()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;q &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Linear(input_dim,dim_k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;k &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Linear(input_dim,dim_k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;v &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Linear(input_dim,dim_v)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;_norm_fact &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; sqrt(dim_k)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;forward&lt;/span&gt;(self,x):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        Q &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;q(x) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Q: batch_size * seq_len * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        K &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;k(x) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# K: batch_size * seq_len * dim_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        V &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;v(x) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# V: batch_size * seq_len * dim_v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        atten &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;Softmax(dim&lt;span style=&#34;color:#666&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;)(torch&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;bmm(Q,K&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;))) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;_norm_fact &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Q * K.T() # batch_size * seq_len * seq_len&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;bmm(atten,V) &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Q * K.T() * V # batch_size * seq_len * dim_v&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;return&lt;/span&gt; output
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/410776234&#34;&gt;超详细图解Self-Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/14/faster-r-cnn/</link>
      <pubDate>Fri, 14 Jan 2022 13:32:15 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/14/faster-r-cnn/</guid>
      
      <description>&lt;p&gt;Faster R-CNN 可以简单看成是&lt;strong&gt;区域生成网络&lt;/strong&gt; + Fast R-CNN 的模型，用区域生成网络(&lt;em&gt;&lt;strong&gt;Region Proposal Network, RPN&lt;/strong&gt;&lt;/em&gt;)来替代 Fast R-CNN 中的选择性搜索方法，结构如下：&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/iEOGhpnroZqN19w&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/iEOGhpnroZqN19w.png&#34; width=&#34;400px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先向 CNN 网络(VGG-16)输入图片，Faster R-CNN 使用一组基础的 conv + relu + pooling 层提取 feature map。&lt;strong&gt;该 feature map 被共享用于后续 RPN 层和 fc 层。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RPN 网络用于生成 region proposals，Faster R-CNN 中称之为 &lt;em&gt;&lt;strong&gt;anchors&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;通过 softmax 判断 anchors 属于 foreground 或者 background&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;再利用 bounding box regression 修正 anchors 获得精确的 proposals，输出其 Top-N(默认 300)的区域给 Rol pooling&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成 anchors $\rightarrow$ softmax 分类器提取 fg anchors $\rightarrow$ bbox reg 回归 fg anchors $\rightarrow$ Proposal Layer 生成proposals&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后续就是 Fast R-CNN 操作&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rpn&#34;&gt;RPN&lt;/h3&gt;
&lt;p&gt;RPN 网络的主要作用是得到比较准确的候选区域，整个过程分为两步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用 $n\times n$ (默认 $3\times 3$) 的大小窗口去扫描特征图，每个滑窗位置映射倒一个低维的向量(默认 256 维)，并为每个滑窗位置考虑 $k$ 种(在论文设计中 $k=9$)&lt;strong&gt;可能的参考窗口(论文中称为 anchors)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/6ropzGCEjRAw1Fc&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/6ropzGCEjRAw1Fc.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3 id=&#34;anchors&#34;&gt;Anchors&lt;/h3&gt;
&lt;p&gt;$3\times 3$ 卷积核的中心点对应原图上的位置，将该点作为 anchor 的中心点，在原图中框出多尺度、多种长宽比的 anchors，三种尺度 $\{128,256,512\}$，三种长宽比 $\{1:1,1:2,2:1\}$，每个特征图中的像素点有 9 个框。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;窗口输出 $[N，256]\rightarrow$ 分类：判断是否是背景&lt;/li&gt;
&lt;li&gt;回归位置：N 个候选框与自己对应目标值 GT 做回归，修正位置。得到更好的候选区域提供给 ROl pooling 使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;Faster R-CNN 的训练分为两部分，即两个网络的训练。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RPN 训练&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;从众多的候选区域中提取出 score 较高的，并且经过 regression 调整的候选区域&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast R-CNN部分的训练&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Fast R-CNN &lt;em&gt;&lt;strong&gt;classification(over classes, softmax)&lt;/strong&gt;&lt;/em&gt;︰所有类别分类 N+1，得到候选区域每个类别概率&lt;/li&gt;
&lt;li&gt;Fast R-CNN &lt;em&gt;&lt;strong&gt;regression(bbox regression, MAE)&lt;/strong&gt;&lt;/em&gt;：得到更好的位置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;样本准备：正负 anchors 样本比例为 $1:3$&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/VJW9yaSCjHXeu1E&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/VJW9yaSCjHXeu1E.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;metrics&lt;/th&gt;
&lt;th&gt;R-CNN&lt;/th&gt;
&lt;th&gt;Fast R-CNN&lt;/th&gt;
&lt;th&gt;Faster R-CNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Test time/image&lt;/td&gt;
&lt;td&gt;50.0s&lt;/td&gt;
&lt;td&gt;2.0s&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.2s&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mAP(VOC2007)&lt;/td&gt;
&lt;td&gt;66.0&lt;/td&gt;
&lt;td&gt;66.9&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;66.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;RPN&lt;/li&gt;
&lt;li&gt;End-to-End&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;训练参数太大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;改进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RPN(Region Proposal Networks) 改进对于小目标选择利用多尺度特征信息进行 RPN&lt;/li&gt;
&lt;li&gt;速度提升，如 YOLO 系列算法，删去RPN，直接对 proposal 进行分类回归，极大的提升了网络的速度&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Fast R-CNN</title>
      <link>https://preminstrel.github.io/blog/post/2022/01/13/fast-r-cnn/</link>
      <pubDate>Thu, 13 Jan 2022 22:31:08 +0800</pubDate>
      <author>preminstrel@gmail.com (Hanshi Sun)</author>
      <guid>https://preminstrel.github.io/blog/post/2022/01/13/fast-r-cnn/</guid>
      
      <description>&lt;p&gt;SPP-net 的性能已经得到很大的改善，但是由于网络之间不统一训练，造成很大的麻烦，所以 &lt;em&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt;&lt;/em&gt; 就是为了解决这样的问题。其改进的之处为：提出一个 &lt;em&gt;&lt;strong&gt;Rol pooling&lt;/strong&gt;&lt;/em&gt;，然后整合整个模型，把 CNN、Rol pooling、分类器、bbox 回归几个模块&lt;strong&gt;整个一起训练&lt;/strong&gt;。&lt;/p&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/AWih7QImN5c9zYC&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/AWih7QImN5c9zYC.png&#34; widt=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先将整个图片输入到一个基础卷积网络，得到整张图的 feature map&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将选择性搜索算法的结果 region proposal (Rol）映射到 feature map 中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rol pooling layer 提取一个固定长度的特征向量，每个特征会输入到一系列全连接层，得到一个 Rol 特征向量 &lt;strong&gt;(此步骤是对每一个候选区域都会进行同样的操作)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中一个是传统 &lt;em&gt;&lt;strong&gt;softmax&lt;/strong&gt;&lt;/em&gt; 层进行分类，输出类别有 K 个类别加上”背景”类&lt;/li&gt;
&lt;li&gt;另一个是 &lt;em&gt;&lt;strong&gt;bounding box regressor&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rol-pooling&#34;&gt;Rol Pooling&lt;/h3&gt;
&lt;p&gt;首先 Rol pooling 只是一个简单版本的 SSP，目的是为了减少计算时间并得到固定长度的向量。Rol 池化层使用最大池化将任何有效的 Rol 区域内的特征转换为具有 $H\times W$ 的固定空间范围的小 feature map，其中 $H,W$ 是超参数，它们独立于任何特定的 Rol。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;single scale&lt;/strong&gt;&lt;/em&gt;：直接将 image 定为某种 scale，直接输入网络来训练即可。（Fast R-CNN）&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;multi scale&lt;/strong&gt;&lt;/em&gt;：生成一个金字塔，即 SPP-net&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后者比前者更加准确，没有突出很多，但是第一种节省了很多的时间，所以 Fast R-CNN 要比 SPP-net 快很多。&lt;/p&gt;
&lt;h3 id=&#34;end-to-end-model&#34;&gt;End-to-End Model&lt;/h3&gt;
&lt;p&gt;输出端可以直接进行完整的反向传播，整体优化目标函数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征提取 CNN 和训练 SVM 分类器的训练在时间上是先后顺序，二者训练方式独立，因此 SVMs 的训练 Loss 无法更新之前的卷积层参数，于是去掉 SVM，便可以形成 End-to-End 模型。&lt;/li&gt;
&lt;li&gt;使用了 softmax&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=center&gt;
&lt;a href=&#34;https://sm.ms/image/SOFVNrZXLalG7fm&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/04/14/SOFVNrZXLalG7fm.png&#34; width=&#34;600px&#34;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3 id=&#34;multi-task-loss&#34;&gt;Multi-task Loss&lt;/h3&gt;
&lt;p&gt;两个 loss，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于分类 loss，是一个 N+1 路的 softmax 输出，其中 N 是类别个数，1 是背景，使用&lt;strong&gt;交叉熵损失&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对于回归 loss，是一个 4N 路输出的 regressor，也就是说对于每个类别都会训练一个单独的 regressor 的意思，&lt;strong&gt;使用平均绝对误差(MAE)损失，即 L1 损失&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;fine-tuning 训练中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调整 CNN + Rol pooling + softmax&lt;/li&gt;
&lt;li&gt;调整 bbox regressor 回归当中的参数&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;metrics&lt;/th&gt;
&lt;th&gt;R-CNN&lt;/th&gt;
&lt;th&gt;SPP-net&lt;/th&gt;
&lt;th&gt;Fast R-CNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;training time(h)&lt;/td&gt;
&lt;td&gt;84&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;9.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;test time/picture(s)&lt;/td&gt;
&lt;td&gt;47.0&lt;/td&gt;
&lt;td&gt;2.3&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mAP&lt;/td&gt;
&lt;td&gt;66.0&lt;/td&gt;
&lt;td&gt;63.1&lt;/td&gt;
&lt;td&gt;66.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;drawbacks&#34;&gt;Drawbacks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;使用 Selective Search 提取 Region Proposals，没有实现真正意义山东个端对端，操作十分耗时间。&lt;/li&gt;
&lt;li&gt;一个更高效的方法来求出候选框—— &lt;em&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1504.08083&#34;&gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
